1
00:00:06,899 --> 00:00:13,199

all right today today we're going to

2
00:00:13,199 --> 00:00:13,209
all right today today we're going to
 

3
00:00:13,209 --> 00:00:17,760
all right today today we're going to
talk about spark spark say essentially a

4
00:00:17,760 --> 00:00:17,770
talk about spark spark say essentially a
 

5
00:00:17,770 --> 00:00:21,360
talk about spark spark say essentially a
successor to MapReduce you can think of

6
00:00:21,360 --> 00:00:21,370
successor to MapReduce you can think of
 

7
00:00:21,370 --> 00:00:24,390
successor to MapReduce you can think of
it as a kind of evolutionary step in

8
00:00:24,390 --> 00:00:24,400
it as a kind of evolutionary step in
 

9
00:00:24,400 --> 00:00:28,590
it as a kind of evolutionary step in
MapReduce and one reason we're looking

10
00:00:28,590 --> 00:00:28,600
MapReduce and one reason we're looking
 

11
00:00:28,600 --> 00:00:31,200
MapReduce and one reason we're looking
at it is that it's widely used today for

12
00:00:31,200 --> 00:00:31,210
at it is that it's widely used today for
 

13
00:00:31,210 --> 00:00:34,139
at it is that it's widely used today for
data center computations that's turned

14
00:00:34,139 --> 00:00:34,149
data center computations that's turned
 

15
00:00:34,149 --> 00:00:37,250
data center computations that's turned
out to be very popular and very useful

16
00:00:37,250 --> 00:00:37,260
out to be very popular and very useful
 

17
00:00:37,260 --> 00:00:40,349
out to be very popular and very useful
one interesting thing it does which will

18
00:00:40,349 --> 00:00:40,359
one interesting thing it does which will
 

19
00:00:40,359 --> 00:00:41,579
one interesting thing it does which will
pay attention to is that it it

20
00:00:41,579 --> 00:00:41,589
pay attention to is that it it
 

21
00:00:41,589 --> 00:00:43,770
pay attention to is that it it
generalizes the kind of two stages of

22
00:00:43,770 --> 00:00:43,780
generalizes the kind of two stages of
 

23
00:00:43,780 --> 00:00:47,540
generalizes the kind of two stages of
MapReduce the map introduced into a

24
00:00:47,540 --> 00:00:47,550
MapReduce the map introduced into a
 

25
00:00:47,550 --> 00:00:51,329
MapReduce the map introduced into a
complete notion of multi-step data flow

26
00:00:51,329 --> 00:00:51,339
complete notion of multi-step data flow
 

27
00:00:51,339 --> 00:00:57,569
complete notion of multi-step data flow
graphs that and this is both helpful for

28
00:00:57,569 --> 00:00:57,579
graphs that and this is both helpful for
 

29
00:00:57,579 --> 00:00:59,669
graphs that and this is both helpful for
flexibility for the programmer it's more

30
00:00:59,669 --> 00:00:59,679
flexibility for the programmer it's more
 

31
00:00:59,679 --> 00:01:02,129
flexibility for the programmer it's more
expressive and it also gives the system

32
00:01:02,129 --> 00:01:02,139
expressive and it also gives the system
 

33
00:01:02,139 --> 00:01:04,219
expressive and it also gives the system
the SPARC system a lot more to chew on

34
00:01:04,219 --> 00:01:04,229
the SPARC system a lot more to chew on
 

35
00:01:04,229 --> 00:01:07,520
the SPARC system a lot more to chew on
when it comes to optimization and

36
00:01:07,520 --> 00:01:07,530
when it comes to optimization and
 

37
00:01:07,530 --> 00:01:09,480
when it comes to optimization and
dealing with faults dealing with

38
00:01:09,480 --> 00:01:09,490
dealing with faults dealing with
 

39
00:01:09,490 --> 00:01:12,749
dealing with faults dealing with
failures and also for the from the

40
00:01:12,749 --> 00:01:12,759
failures and also for the from the
 

41
00:01:12,759 --> 00:01:14,100
failures and also for the from the
programmers point of view it supports

42
00:01:14,100 --> 00:01:14,110
programmers point of view it supports
 

43
00:01:14,110 --> 00:01:16,529
programmers point of view it supports
iterative applications application said

44
00:01:16,529 --> 00:01:16,539
iterative applications application said
 

45
00:01:16,539 --> 00:01:19,130
iterative applications application said
you know loop over the data effectively

46
00:01:19,130 --> 00:01:19,140
you know loop over the data effectively
 

47
00:01:19,140 --> 00:01:21,899
you know loop over the data effectively
much better than that produced us you

48
00:01:21,899 --> 00:01:21,909
much better than that produced us you
 

49
00:01:21,909 --> 00:01:24,170
much better than that produced us you
can cobble together a lot of stuff with

50
00:01:24,170 --> 00:01:24,180
can cobble together a lot of stuff with
 

51
00:01:24,180 --> 00:01:27,569
can cobble together a lot of stuff with
multiple MapReduce applications running

52
00:01:27,569 --> 00:01:27,579
multiple MapReduce applications running
 

53
00:01:27,579 --> 00:01:30,090
multiple MapReduce applications running
one after another but it's all a lot

54
00:01:30,090 --> 00:01:30,100
one after another but it's all a lot
 

55
00:01:30,100 --> 00:01:36,139
one after another but it's all a lot
more convenient in and SPARC okay so I

56
00:01:36,139 --> 00:01:36,149
more convenient in and SPARC okay so I
 

57
00:01:36,149 --> 00:01:38,340
more convenient in and SPARC okay so I
think I'm just gonna start right off

58
00:01:38,340 --> 00:01:38,350
think I'm just gonna start right off
 

59
00:01:38,350 --> 00:01:41,899
think I'm just gonna start right off
with an example application this is the

60
00:01:41,899 --> 00:01:41,909
with an example application this is the
 

61
00:01:41,909 --> 00:01:47,429
with an example application this is the
code for PageRank and I'll just copy

62
00:01:47,429 --> 00:01:47,439
code for PageRank and I'll just copy
 

63
00:01:47,439 --> 00:01:52,830
code for PageRank and I'll just copy
this code with a few a few changes from

64
00:01:52,830 --> 00:01:52,840
this code with a few a few changes from
 

65
00:01:52,840 --> 00:01:57,510
this code with a few a few changes from
some sample source code in the

66
00:01:57,510 --> 00:01:57,520

 

67
00:01:57,520 --> 00:02:01,500

in the spark source I guess it's

68
00:02:01,500 --> 00:02:01,510
in the spark source I guess it's
 

69
00:02:01,510 --> 00:02:02,670
in the spark source I guess it's
actually a little bit hard to read let

70
00:02:02,670 --> 00:02:02,680
actually a little bit hard to read let
 

71
00:02:02,680 --> 00:02:04,230
actually a little bit hard to read let
me just give me a second law try to make

72
00:02:04,230 --> 00:02:04,240
me just give me a second law try to make
 

73
00:02:04,240 --> 00:02:14,850
me just give me a second law try to make
it bigger

74
00:02:14,850 --> 00:02:14,860

 

75
00:02:14,860 --> 00:02:18,130

all right okay so if this is if this is

76
00:02:18,130 --> 00:02:18,140
all right okay so if this is if this is
 

77
00:02:18,140 --> 00:02:20,110
all right okay so if this is if this is
too hard to read is there's a copy of it

78
00:02:20,110 --> 00:02:20,120
too hard to read is there's a copy of it
 

79
00:02:20,120 --> 00:02:22,930
too hard to read is there's a copy of it
in the notes and it's an expansion of

80
00:02:22,930 --> 00:02:22,940
in the notes and it's an expansion of
 

81
00:02:22,940 --> 00:02:26,560
in the notes and it's an expansion of
the code and section 3 to 2 in the paper

82
00:02:26,560 --> 00:02:26,570
the code and section 3 to 2 in the paper
 

83
00:02:26,570 --> 00:02:31,030
the code and section 3 to 2 in the paper
a page rank which is a algorithm that

84
00:02:31,030 --> 00:02:31,040
a page rank which is a algorithm that
 

85
00:02:31,040 --> 00:02:33,490
a page rank which is a algorithm that
Google uses pretty famous algorithm for

86
00:02:33,490 --> 00:02:33,500
Google uses pretty famous algorithm for
 

87
00:02:33,500 --> 00:02:38,790
Google uses pretty famous algorithm for
calculating how important different web

88
00:02:38,790 --> 00:02:38,800
calculating how important different web
 

89
00:02:38,800 --> 00:02:42,370
calculating how important different web
search results are what PageRank is

90
00:02:42,370 --> 00:02:42,380
search results are what PageRank is
 

91
00:02:42,380 --> 00:02:43,390
search results are what PageRank is
trying to do

92
00:02:43,390 --> 00:02:43,400
trying to do
 

93
00:02:43,400 --> 00:02:46,690
trying to do
well actually PageRank is sort of widely

94
00:02:46,690 --> 00:02:46,700
well actually PageRank is sort of widely
 

95
00:02:46,700 --> 00:02:49,170
well actually PageRank is sort of widely
used as an example of something that

96
00:02:49,170 --> 00:02:49,180
used as an example of something that
 

97
00:02:49,180 --> 00:02:51,340
used as an example of something that
doesn't actually work that well and

98
00:02:51,340 --> 00:02:51,350
doesn't actually work that well and
 

99
00:02:51,350 --> 00:02:53,670
doesn't actually work that well and
MapReduce and the reason is that

100
00:02:53,670 --> 00:02:53,680
MapReduce and the reason is that
 

101
00:02:53,680 --> 00:02:56,500
MapReduce and the reason is that
PageRank involves a bunch of sort of

102
00:02:56,500 --> 00:02:56,510
PageRank involves a bunch of sort of
 

103
00:02:56,510 --> 00:02:58,630
PageRank involves a bunch of sort of
distinct steps and worse PageRank

104
00:02:58,630 --> 00:02:58,640
distinct steps and worse PageRank
 

105
00:02:58,640 --> 00:03:01,120
distinct steps and worse PageRank
involves iteration there's a loop in it

106
00:03:01,120 --> 00:03:01,130
involves iteration there's a loop in it
 

107
00:03:01,130 --> 00:03:03,540
involves iteration there's a loop in it
that's got to be run many times and

108
00:03:03,540 --> 00:03:03,550
that's got to be run many times and
 

109
00:03:03,550 --> 00:03:06,120
that's got to be run many times and
MapReduce just has nothing to say about

110
00:03:06,120 --> 00:03:06,130
MapReduce just has nothing to say about
 

111
00:03:06,130 --> 00:03:12,960
MapReduce just has nothing to say about
about iteration the input the PageRank

112
00:03:12,960 --> 00:03:12,970
about iteration the input the PageRank
 

113
00:03:12,970 --> 00:03:15,850
about iteration the input the PageRank
for this version of PageRank is just a

114
00:03:15,850 --> 00:03:15,860
for this version of PageRank is just a
 

115
00:03:15,860 --> 00:03:20,949
for this version of PageRank is just a
giant collection of lines one per link

116
00:03:20,949 --> 00:03:20,959
giant collection of lines one per link
 

117
00:03:20,959 --> 00:03:23,350
giant collection of lines one per link
in the web and each line then has two

118
00:03:23,350 --> 00:03:23,360
in the web and each line then has two
 

119
00:03:23,360 --> 00:03:26,110
in the web and each line then has two
URLs the URL of the page containing a

120
00:03:26,110 --> 00:03:26,120
URLs the URL of the page containing a
 

121
00:03:26,120 --> 00:03:28,540
URLs the URL of the page containing a
link and the URL of the link that that

122
00:03:28,540 --> 00:03:28,550
link and the URL of the link that that
 

123
00:03:28,550 --> 00:03:31,720
link and the URL of the link that that
page points to and you know if the

124
00:03:31,720 --> 00:03:31,730
page points to and you know if the
 

125
00:03:31,730 --> 00:03:33,910
page points to and you know if the
intent is that you get this file from by

126
00:03:33,910 --> 00:03:33,920
intent is that you get this file from by
 

127
00:03:33,920 --> 00:03:36,040
intent is that you get this file from by
crawling the web and looking at all the

128
00:03:36,040 --> 00:03:36,050
crawling the web and looking at all the
 

129
00:03:36,050 --> 00:03:38,380
crawling the web and looking at all the
all collecting together all the links in

130
00:03:38,380 --> 00:03:38,390
all collecting together all the links in
 

131
00:03:38,390 --> 00:03:40,360
all collecting together all the links in
the web's the input is absolutely

132
00:03:40,360 --> 00:03:40,370
the web's the input is absolutely
 

133
00:03:40,370 --> 00:03:46,780
the web's the input is absolutely
enormous and as just a sort of silly

134
00:03:46,780 --> 00:03:46,790
enormous and as just a sort of silly
 

135
00:03:46,790 --> 00:03:49,600
enormous and as just a sort of silly
little example for us from when I

136
00:03:49,600 --> 00:03:49,610
little example for us from when I
 

137
00:03:49,610 --> 00:03:53,170
little example for us from when I
actually run this code I've given some

138
00:03:53,170 --> 00:03:53,180
actually run this code I've given some
 

139
00:03:53,180 --> 00:03:55,390
actually run this code I've given some
example input here and this is the way

140
00:03:55,390 --> 00:03:55,400
example input here and this is the way
 

141
00:03:55,400 --> 00:03:56,949
example input here and this is the way
the impro would really look it's just

142
00:03:56,949 --> 00:03:56,959
the impro would really look it's just
 

143
00:03:56,959 --> 00:03:59,380
the impro would really look it's just
lines each line with two URLs and I'm

144
00:03:59,380 --> 00:03:59,390
lines each line with two URLs and I'm
 

145
00:03:59,390 --> 00:04:03,280
lines each line with two URLs and I'm
using u1 that's the URL of a page and u3

146
00:04:03,280 --> 00:04:03,290
using u1 that's the URL of a page and u3
 

147
00:04:03,290 --> 00:04:07,509
using u1 that's the URL of a page and u3
for example as the URL of a link that

148
00:04:07,509 --> 00:04:07,519
for example as the URL of a link that
 

149
00:04:07,519 --> 00:04:09,479
for example as the URL of a link that
that page points to just for convenience

150
00:04:09,479 --> 00:04:09,489
that page points to just for convenience
 

151
00:04:09,489 --> 00:04:12,820
that page points to just for convenience
and so the web graph that this input

152
00:04:12,820 --> 00:04:12,830
and so the web graph that this input
 

153
00:04:12,830 --> 00:04:15,220
and so the web graph that this input
file represents there's only three pages

154
00:04:15,220 --> 00:04:15,230
file represents there's only three pages
 

155
00:04:15,230 --> 00:04:21,310
file represents there's only three pages
in it one two three I could just

156
00:04:21,310 --> 00:04:21,320
in it one two three I could just
 

157
00:04:21,320 --> 00:04:22,600
in it one two three I could just
interpret the links there's a link from

158
00:04:22,600 --> 00:04:22,610
interpret the links there's a link from
 

159
00:04:22,610 --> 00:04:24,420
interpret the links there's a link from
one two three

160
00:04:24,420 --> 00:04:24,430
one two three
 

161
00:04:24,430 --> 00:04:27,409
one two three
there's a link from one back to itself

162
00:04:27,409 --> 00:04:27,419
there's a link from one back to itself
 

163
00:04:27,419 --> 00:04:30,230
there's a link from one back to itself
there's a web link from two to three

164
00:04:30,230 --> 00:04:30,240
there's a web link from two to three
 

165
00:04:30,240 --> 00:04:32,700
there's a web link from two to three
there's a web link from two back to

166
00:04:32,700 --> 00:04:32,710
there's a web link from two back to
 

167
00:04:32,710 --> 00:04:35,490
there's a web link from two back to
itself and there's a web link from three

168
00:04:35,490 --> 00:04:35,500
itself and there's a web link from three
 

169
00:04:35,500 --> 00:04:39,180
itself and there's a web link from three
to one just like a very simple graph

170
00:04:39,180 --> 00:04:39,190
to one just like a very simple graph
 

171
00:04:39,190 --> 00:04:42,810
to one just like a very simple graph
structure what PageRank is trying to do

172
00:04:42,810 --> 00:04:42,820
structure what PageRank is trying to do
 

173
00:04:42,820 --> 00:04:45,090
structure what PageRank is trying to do
it's you know estimating the importance

174
00:04:45,090 --> 00:04:45,100
it's you know estimating the importance
 

175
00:04:45,100 --> 00:04:47,790
it's you know estimating the importance
of each page what that really means is

176
00:04:47,790 --> 00:04:47,800
of each page what that really means is
 

177
00:04:47,800 --> 00:04:50,610
of each page what that really means is
that it's estimating the importance

178
00:04:50,610 --> 00:04:50,620
that it's estimating the importance
 

179
00:04:50,620 --> 00:04:53,490
that it's estimating the importance
based on whether other important pages

180
00:04:53,490 --> 00:04:53,500
based on whether other important pages
 

181
00:04:53,500 --> 00:04:56,969
based on whether other important pages
have links to a given page and what's

182
00:04:56,969 --> 00:04:56,979
have links to a given page and what's
 

183
00:04:56,979 --> 00:04:58,200
have links to a given page and what's
really going on here is this kind of

184
00:04:58,200 --> 00:04:58,210
really going on here is this kind of
 

185
00:04:58,210 --> 00:05:01,140
really going on here is this kind of
modeling the estimated probability that

186
00:05:01,140 --> 00:05:01,150
modeling the estimated probability that
 

187
00:05:01,150 --> 00:05:05,040
modeling the estimated probability that
a user who clicks on links will end on

188
00:05:05,040 --> 00:05:05,050
a user who clicks on links will end on
 

189
00:05:05,050 --> 00:05:08,189
a user who clicks on links will end on
each given page so it has this user

190
00:05:08,189 --> 00:05:08,199
each given page so it has this user
 

191
00:05:08,199 --> 00:05:11,700
each given page so it has this user
model in which the user has a 85 percent

192
00:05:11,700 --> 00:05:11,710
model in which the user has a 85 percent
 

193
00:05:11,710 --> 00:05:14,279
model in which the user has a 85 percent
chance of following a link from the

194
00:05:14,279 --> 00:05:14,289
chance of following a link from the
 

195
00:05:14,289 --> 00:05:17,189
chance of following a link from the
users current page following a randomly

196
00:05:17,189 --> 00:05:17,199
users current page following a randomly
 

197
00:05:17,199 --> 00:05:19,140
users current page following a randomly
selected link from the users current

198
00:05:19,140 --> 00:05:19,150
selected link from the users current
 

199
00:05:19,150 --> 00:05:21,800
selected link from the users current
page to wherever that link leads and a

200
00:05:21,800 --> 00:05:21,810
page to wherever that link leads and a
 

201
00:05:21,810 --> 00:05:25,890
page to wherever that link leads and a
15% chance of simply switching to some

202
00:05:25,890 --> 00:05:25,900
15% chance of simply switching to some
 

203
00:05:25,900 --> 00:05:27,210
15% chance of simply switching to some
other page even though there's not a

204
00:05:27,210 --> 00:05:27,220
other page even though there's not a
 

205
00:05:27,220 --> 00:05:29,070
other page even though there's not a
link to it as you would if you you know

206
00:05:29,070 --> 00:05:29,080
link to it as you would if you you know
 

207
00:05:29,080 --> 00:05:33,320
link to it as you would if you you know
entered a URL directly into the browser

208
00:05:33,320 --> 00:05:33,330
entered a URL directly into the browser
 

209
00:05:33,330 --> 00:05:38,939
entered a URL directly into the browser
and the idea is that the he drank

210
00:05:38,939 --> 00:05:38,949
and the idea is that the he drank
 

211
00:05:38,949 --> 00:05:43,560
and the idea is that the he drank
algorithm kind of runs this repeatedly

212
00:05:43,560 --> 00:05:43,570
algorithm kind of runs this repeatedly
 

213
00:05:43,570 --> 00:05:45,390
algorithm kind of runs this repeatedly
it sort of simulates the user looking at

214
00:05:45,390 --> 00:05:45,400
it sort of simulates the user looking at
 

215
00:05:45,400 --> 00:05:48,390
it sort of simulates the user looking at
a page and then following a link and

216
00:05:48,390 --> 00:05:48,400
a page and then following a link and
 

217
00:05:48,400 --> 00:05:51,600
a page and then following a link and
kind of adds the from pages importance

218
00:05:51,600 --> 00:05:51,610
kind of adds the from pages importance
 

219
00:05:51,610 --> 00:05:53,850
kind of adds the from pages importance
to the target pages importance and then

220
00:05:53,850 --> 00:05:53,860
to the target pages importance and then
 

221
00:05:53,860 --> 00:05:55,710
to the target pages importance and then
sort of runs this again and it's going

222
00:05:55,710 --> 00:05:55,720
sort of runs this again and it's going
 

223
00:05:55,720 --> 00:06:00,899
sort of runs this again and it's going
to end up in the system like page rank

224
00:06:00,899 --> 00:06:00,909
to end up in the system like page rank
 

225
00:06:00,909 --> 00:06:02,879
to end up in the system like page rank
on SPARC it's going to kind of run this

226
00:06:02,879 --> 00:06:02,889
on SPARC it's going to kind of run this
 

227
00:06:02,889 --> 00:06:06,270
on SPARC it's going to kind of run this
simulation for all pages in parallel it

228
00:06:06,270 --> 00:06:06,280
simulation for all pages in parallel it
 

229
00:06:06,280 --> 00:06:09,890
simulation for all pages in parallel it
or literately

230
00:06:09,890 --> 00:06:09,900

 

231
00:06:09,900 --> 00:06:13,020

the and the idea is that it's going to

232
00:06:13,020 --> 00:06:13,030
the and the idea is that it's going to
 

233
00:06:13,030 --> 00:06:14,670
the and the idea is that it's going to
keep track the algorithms gonna keep

234
00:06:14,670 --> 00:06:14,680
keep track the algorithms gonna keep
 

235
00:06:14,680 --> 00:06:16,770
keep track the algorithms gonna keep
track of the page rank of every single

236
00:06:16,770 --> 00:06:16,780
track of the page rank of every single
 

237
00:06:16,780 --> 00:06:19,550
track of the page rank of every single
page or every single URL and update it

238
00:06:19,550 --> 00:06:19,560
page or every single URL and update it
 

239
00:06:19,560 --> 00:06:22,290
page or every single URL and update it
as it sort of simulates random user

240
00:06:22,290 --> 00:06:22,300
as it sort of simulates random user
 

241
00:06:22,300 --> 00:06:24,600
as it sort of simulates random user
clicks I mean that eventually that those

242
00:06:24,600 --> 00:06:24,610
clicks I mean that eventually that those
 

243
00:06:24,610 --> 00:06:27,860
clicks I mean that eventually that those
ranks will converge on kind of the true

244
00:06:27,860 --> 00:06:27,870
ranks will converge on kind of the true
 

245
00:06:27,870 --> 00:06:31,519
ranks will converge on kind of the true
final values now

246
00:06:31,519 --> 00:06:31,529
final values now
 

247
00:06:31,529 --> 00:06:34,729
final values now
because it's iterative although you can

248
00:06:34,729 --> 00:06:34,739
because it's iterative although you can
 

249
00:06:34,739 --> 00:06:37,249
because it's iterative although you can
code this up in rapid MapReduce it's a

250
00:06:37,249 --> 00:06:37,259
code this up in rapid MapReduce it's a
 

251
00:06:37,259 --> 00:06:39,319
code this up in rapid MapReduce it's a
pain it can't be just a single MapReduce

252
00:06:39,319 --> 00:06:39,329
pain it can't be just a single MapReduce
 

253
00:06:39,329 --> 00:06:45,429
pain it can't be just a single MapReduce
program it has to be multiple you know

254
00:06:45,429 --> 00:06:45,439
program it has to be multiple you know
 

255
00:06:45,439 --> 00:06:48,589
program it has to be multiple you know
multiple calls to a MapReduce

256
00:06:48,589 --> 00:06:48,599
multiple calls to a MapReduce
 

257
00:06:48,599 --> 00:06:51,349
multiple calls to a MapReduce
application where each call sort of

258
00:06:51,349 --> 00:06:51,359
application where each call sort of
 

259
00:06:51,359 --> 00:06:53,869
application where each call sort of
simulates one step in the iteration so

260
00:06:53,869 --> 00:06:53,879
simulates one step in the iteration so
 

261
00:06:53,879 --> 00:06:55,729
simulates one step in the iteration so
you can do in a MapReduce but it's a

262
00:06:55,729 --> 00:06:55,739
you can do in a MapReduce but it's a
 

263
00:06:55,739 --> 00:06:58,039
you can do in a MapReduce but it's a
pain and it's also kind of slope because

264
00:06:58,039 --> 00:06:58,049
pain and it's also kind of slope because
 

265
00:06:58,049 --> 00:07:00,469
pain and it's also kind of slope because
MapReduce it's only thinking about one

266
00:07:00,469 --> 00:07:00,479
MapReduce it's only thinking about one
 

267
00:07:00,479 --> 00:07:02,419
MapReduce it's only thinking about one
map and one reduce and it's always

268
00:07:02,419 --> 00:07:02,429
map and one reduce and it's always
 

269
00:07:02,429 --> 00:07:05,269
map and one reduce and it's always
reading its input from the GFS from disk

270
00:07:05,269 --> 00:07:05,279
reading its input from the GFS from disk
 

271
00:07:05,279 --> 00:07:07,129
reading its input from the GFS from disk
and the GFS filesystem and always

272
00:07:07,129 --> 00:07:07,139
and the GFS filesystem and always
 

273
00:07:07,139 --> 00:07:09,079
and the GFS filesystem and always
writing its output which would be this

274
00:07:09,079 --> 00:07:09,089
writing its output which would be this
 

275
00:07:09,089 --> 00:07:12,979
writing its output which would be this
sort of updated per page ranks every

276
00:07:12,979 --> 00:07:12,989
sort of updated per page ranks every
 

277
00:07:12,989 --> 00:07:17,059
sort of updated per page ranks every
stage also writes those updated per page

278
00:07:17,059 --> 00:07:17,069
stage also writes those updated per page
 

279
00:07:17,069 --> 00:07:19,819
stage also writes those updated per page
ranks to files in GFS also so there's a

280
00:07:19,819 --> 00:07:19,829
ranks to files in GFS also so there's a
 

281
00:07:19,829 --> 00:07:22,999
ranks to files in GFS also so there's a
lot of file i/o if you run this as sort

282
00:07:22,999 --> 00:07:23,009
lot of file i/o if you run this as sort
 

283
00:07:23,009 --> 00:07:26,829
lot of file i/o if you run this as sort
of a sequence of MapReduce applications

284
00:07:26,829 --> 00:07:26,839
of a sequence of MapReduce applications
 

285
00:07:26,839 --> 00:07:31,269
of a sequence of MapReduce applications
all right so we have here this sum

286
00:07:31,269 --> 00:07:31,279
all right so we have here this sum
 

287
00:07:31,279 --> 00:07:32,769
all right so we have here this sum
there's an a PageRank code that came

288
00:07:32,769 --> 00:07:32,779
there's an a PageRank code that came
 

289
00:07:32,779 --> 00:07:35,859
there's an a PageRank code that came
with um came a spark I'm actually gonna

290
00:07:35,859 --> 00:07:35,869
with um came a spark I'm actually gonna
 

291
00:07:35,869 --> 00:07:38,619
with um came a spark I'm actually gonna
run it for you I'm gonna run the whole

292
00:07:38,619 --> 00:07:38,629
run it for you I'm gonna run the whole
 

293
00:07:38,629 --> 00:07:39,999
run it for you I'm gonna run the whole
thing for you

294
00:07:39,999 --> 00:07:40,009
thing for you
 

295
00:07:40,009 --> 00:07:42,489
thing for you
this code shown here on the input that

296
00:07:42,489 --> 00:07:42,499
this code shown here on the input that
 

297
00:07:42,499 --> 00:07:44,349
this code shown here on the input that
I've shown just to see what the final

298
00:07:44,349 --> 00:07:44,359
I've shown just to see what the final
 

299
00:07:44,359 --> 00:07:46,989
I've shown just to see what the final
output is and then I'll look through and

300
00:07:46,989 --> 00:07:46,999
output is and then I'll look through and
 

301
00:07:46,999 --> 00:07:52,669
output is and then I'll look through and
we're going to step by step and

302
00:07:52,669 --> 00:07:52,679

 

303
00:07:52,679 --> 00:07:56,519

show how it executes alright so here's

304
00:07:56,519 --> 00:07:56,529
show how it executes alright so here's
 

305
00:07:56,529 --> 00:08:02,609
show how it executes alright so here's
the you should see a screen share now at

306
00:08:02,609 --> 00:08:02,619
the you should see a screen share now at
 

307
00:08:02,619 --> 00:08:05,729
the you should see a screen share now at
a terminal window and I'm showing you

308
00:08:05,729 --> 00:08:05,739
a terminal window and I'm showing you
 

309
00:08:05,739 --> 00:08:10,189
a terminal window and I'm showing you
the input file then I got a hand to this

310
00:08:10,189 --> 00:08:10,199
the input file then I got a hand to this
 

311
00:08:10,199 --> 00:08:14,339
the input file then I got a hand to this
PageRank program and now here's how I

312
00:08:14,339 --> 00:08:14,349
PageRank program and now here's how I
 

313
00:08:14,349 --> 00:08:17,009
PageRank program and now here's how I
read it I've you know I've downloaded a

314
00:08:17,009 --> 00:08:17,019
read it I've you know I've downloaded a
 

315
00:08:17,019 --> 00:08:19,799
read it I've you know I've downloaded a
copy of SPARC to my laptop it turns out

316
00:08:19,799 --> 00:08:19,809
copy of SPARC to my laptop it turns out
 

317
00:08:19,809 --> 00:08:23,519
copy of SPARC to my laptop it turns out
to be pretty easy and if it's a pre

318
00:08:23,519 --> 00:08:23,529
to be pretty easy and if it's a pre
 

319
00:08:23,529 --> 00:08:27,119
to be pretty easy and if it's a pre
compiled version of it I can just run it

320
00:08:27,119 --> 00:08:27,129
compiled version of it I can just run it
 

321
00:08:27,129 --> 00:08:29,219
compiled version of it I can just run it
just runs in the Java Virtual Machine I

322
00:08:29,219 --> 00:08:29,229
just runs in the Java Virtual Machine I
 

323
00:08:29,229 --> 00:08:30,839
just runs in the Java Virtual Machine I
can run it very easily so it's actually

324
00:08:30,839 --> 00:08:30,849
can run it very easily so it's actually
 

325
00:08:30,849 --> 00:08:33,779
can run it very easily so it's actually
doing downloading SPARC and running

326
00:08:33,779 --> 00:08:33,789
doing downloading SPARC and running
 

327
00:08:33,789 --> 00:08:35,759
doing downloading SPARC and running
simple stuff turns out to be pretty

328
00:08:35,759 --> 00:08:35,769
simple stuff turns out to be pretty
 

329
00:08:35,769 --> 00:08:37,550
simple stuff turns out to be pretty
straightforward so I'm gonna run the

330
00:08:37,550 --> 00:08:37,560
straightforward so I'm gonna run the
 

331
00:08:37,560 --> 00:08:40,139
straightforward so I'm gonna run the
code that I show with the input that I

332
00:08:40,139 --> 00:08:40,149
code that I show with the input that I
 

333
00:08:40,149 --> 00:08:43,409
code that I show with the input that I
show and we're gonna see a lot of sort

334
00:08:43,409 --> 00:08:43,419
show and we're gonna see a lot of sort
 

335
00:08:43,419 --> 00:08:48,150
show and we're gonna see a lot of sort
of junk error messages go by but in the

336
00:08:48,150 --> 00:08:48,160
of junk error messages go by but in the
 

337
00:08:48,160 --> 00:08:52,079
of junk error messages go by but in the
end support runs the program and prints

338
00:08:52,079 --> 00:08:52,089
end support runs the program and prints
 

339
00:08:52,089 --> 00:08:53,610
end support runs the program and prints
the final result and we get these three

340
00:08:53,610 --> 00:08:53,620
the final result and we get these three
 

341
00:08:53,620 --> 00:08:56,389
the final result and we get these three
ranks for the three pages I have and

342
00:08:56,389 --> 00:08:56,399
ranks for the three pages I have and
 

343
00:08:56,399 --> 00:09:02,809
ranks for the three pages I have and
apparently page one has the highest rank

344
00:09:02,809 --> 00:09:02,819

 

345
00:09:02,819 --> 00:09:06,480

and I'm not completely sure why but

346
00:09:06,480 --> 00:09:06,490
and I'm not completely sure why but
 

347
00:09:06,490 --> 00:09:09,150
and I'm not completely sure why but
that's what the algorithm ends up doing

348
00:09:09,150 --> 00:09:09,160
that's what the algorithm ends up doing
 

349
00:09:09,160 --> 00:09:10,920
that's what the algorithm ends up doing
so you know of course we're not really

350
00:09:10,920 --> 00:09:10,930
so you know of course we're not really
 

351
00:09:10,930 --> 00:09:13,429
so you know of course we're not really
that interested in the algorithm itself

352
00:09:13,429 --> 00:09:13,439
that interested in the algorithm itself
 

353
00:09:13,439 --> 00:09:18,780
that interested in the algorithm itself
so much as how we execute arc execute

354
00:09:18,780 --> 00:09:18,790
so much as how we execute arc execute
 

355
00:09:18,790 --> 00:09:26,460
so much as how we execute arc execute
sit all right so I'm gonna hand to

356
00:09:26,460 --> 00:09:26,470
sit all right so I'm gonna hand to
 

357
00:09:26,470 --> 00:09:29,160
sit all right so I'm gonna hand to
understand what the programming model is

358
00:09:29,160 --> 00:09:29,170
understand what the programming model is
 

359
00:09:29,170 --> 00:09:33,329
understand what the programming model is
and spark because it's perhaps not quite

360
00:09:33,329 --> 00:09:33,339
and spark because it's perhaps not quite
 

361
00:09:33,339 --> 00:09:36,720
and spark because it's perhaps not quite
what it looks like I'm gonna hand the

362
00:09:36,720 --> 00:09:36,730
what it looks like I'm gonna hand the
 

363
00:09:36,730 --> 00:09:40,769
what it looks like I'm gonna hand the
program line by line to the SPARC

364
00:09:40,769 --> 00:09:40,779
program line by line to the SPARC
 

365
00:09:40,779 --> 00:09:44,490
program line by line to the SPARC
interpreter so you can just fire up this

366
00:09:44,490 --> 00:09:44,500
interpreter so you can just fire up this
 

367
00:09:44,500 --> 00:09:49,230
interpreter so you can just fire up this
spark shell thing and type code to it

368
00:09:49,230 --> 00:09:49,240
spark shell thing and type code to it
 

369
00:09:49,240 --> 00:09:53,420
spark shell thing and type code to it
directly so I've sort of prepared a

370
00:09:53,420 --> 00:09:53,430
directly so I've sort of prepared a
 

371
00:09:53,430 --> 00:09:57,720
directly so I've sort of prepared a
version of the MapReduce program that I

372
00:09:57,720 --> 00:09:57,730
version of the MapReduce program that I
 

373
00:09:57,730 --> 00:10:01,019
version of the MapReduce program that I
can run a line at a time here so the

374
00:10:01,019 --> 00:10:01,029
can run a line at a time here so the
 

375
00:10:01,029 --> 00:10:05,790
can run a line at a time here so the
first line is this line in which it

376
00:10:05,790 --> 00:10:05,800
first line is this line in which it
 

377
00:10:05,800 --> 00:10:08,639
first line is this line in which it
reads the or asking SPARC to read this

378
00:10:08,639 --> 00:10:08,649
reads the or asking SPARC to read this
 

379
00:10:08,649 --> 00:10:11,009
reads the or asking SPARC to read this
input file and it's you know the input

380
00:10:11,009 --> 00:10:11,019
input file and it's you know the input
 

381
00:10:11,019 --> 00:10:16,100
input file and it's you know the input
file I showed with the three pages in it

382
00:10:16,100 --> 00:10:16,110

 

383
00:10:16,110 --> 00:10:19,049

okay so one thing there notice here is

384
00:10:19,049 --> 00:10:19,059
okay so one thing there notice here is
 

385
00:10:19,059 --> 00:10:23,100
okay so one thing there notice here is
is that when Sparky's a file what is

386
00:10:23,100 --> 00:10:23,110
is that when Sparky's a file what is
 

387
00:10:23,110 --> 00:10:27,019
is that when Sparky's a file what is
actually doing is reading a file from a

388
00:10:27,019 --> 00:10:27,029
actually doing is reading a file from a
 

389
00:10:27,029 --> 00:10:29,759
actually doing is reading a file from a
GFS like distributed file system and

390
00:10:29,759 --> 00:10:29,769
GFS like distributed file system and
 

391
00:10:29,769 --> 00:10:33,509
GFS like distributed file system and
happens to be HDFS the Hadoop file

392
00:10:33,509 --> 00:10:33,519
happens to be HDFS the Hadoop file
 

393
00:10:33,519 --> 00:10:36,569
happens to be HDFS the Hadoop file
system but this HDFS file system is very

394
00:10:36,569 --> 00:10:36,579
system but this HDFS file system is very
 

395
00:10:36,579 --> 00:10:38,699
system but this HDFS file system is very
much like GFS so if you have a huge file

396
00:10:38,699 --> 00:10:38,709
much like GFS so if you have a huge file
 

397
00:10:38,709 --> 00:10:40,710
much like GFS so if you have a huge file
as you would with got a file with all

398
00:10:40,710 --> 00:10:40,720
as you would with got a file with all
 

399
00:10:40,720 --> 00:10:44,100
as you would with got a file with all
the URLs all the links and the web on it

400
00:10:44,100 --> 00:10:44,110
the URLs all the links and the web on it
 

401
00:10:44,110 --> 00:10:46,710
the URLs all the links and the web on it
on HDFS is gonna split that file up

402
00:10:46,710 --> 00:10:46,720
on HDFS is gonna split that file up
 

403
00:10:46,720 --> 00:10:49,439
on HDFS is gonna split that file up
among lots and lots you know bite by

404
00:10:49,439 --> 00:10:49,449
among lots and lots you know bite by
 

405
00:10:49,449 --> 00:10:51,720
among lots and lots you know bite by
chunks it's gonna shard the file over

406
00:10:51,720 --> 00:10:51,730
chunks it's gonna shard the file over
 

407
00:10:51,730 --> 00:10:54,840
chunks it's gonna shard the file over
lots and lots of servers and so what

408
00:10:54,840 --> 00:10:54,850
lots and lots of servers and so what
 

409
00:10:54,850 --> 00:10:57,319
lots and lots of servers and so what
reading the file really means is that

410
00:10:57,319 --> 00:10:57,329
reading the file really means is that
 

411
00:10:57,329 --> 00:11:00,749
reading the file really means is that
spark is gonna arrange to run a

412
00:11:00,749 --> 00:11:00,759
spark is gonna arrange to run a
 

413
00:11:00,759 --> 00:11:02,730
spark is gonna arrange to run a
computation on each of many many

414
00:11:02,730 --> 00:11:02,740
computation on each of many many
 

415
00:11:02,740 --> 00:11:05,970
computation on each of many many
machines each of which reads one chunk

416
00:11:05,970 --> 00:11:05,980
machines each of which reads one chunk
 

417
00:11:05,980 --> 00:11:10,199
machines each of which reads one chunk
or one partition of the input file and

418
00:11:10,199 --> 00:11:10,209
or one partition of the input file and
 

419
00:11:10,209 --> 00:11:13,769
or one partition of the input file and
in fact actually the system ends up or

420
00:11:13,769 --> 00:11:13,779
in fact actually the system ends up or
 

421
00:11:13,779 --> 00:11:16,199
in fact actually the system ends up or
HDFS ends up splitting the file big

422
00:11:16,199 --> 00:11:16,209
HDFS ends up splitting the file big
 

423
00:11:16,209 --> 00:11:17,879
HDFS ends up splitting the file big
files typically into many more

424
00:11:17,879 --> 00:11:17,889
files typically into many more
 

425
00:11:17,889 --> 00:11:19,309
files typically into many more
partitions

426
00:11:19,309 --> 00:11:19,319
partitions
 

427
00:11:19,319 --> 00:11:22,379
partitions
then there are worker machines and so

428
00:11:22,379 --> 00:11:22,389
then there are worker machines and so
 

429
00:11:22,389 --> 00:11:23,850
then there are worker machines and so
every worker machine is going to end up

430
00:11:23,850 --> 00:11:23,860
every worker machine is going to end up
 

431
00:11:23,860 --> 00:11:26,400
every worker machine is going to end up
being responsible for looking at

432
00:11:26,400 --> 00:11:26,410
being responsible for looking at
 

433
00:11:26,410 --> 00:11:28,980
being responsible for looking at
multiple partitions of the input files

434
00:11:28,980 --> 00:11:28,990
multiple partitions of the input files
 

435
00:11:28,990 --> 00:11:33,430
multiple partitions of the input files
this is all a lot like the way map works

436
00:11:33,430 --> 00:11:33,440
this is all a lot like the way map works
 

437
00:11:33,440 --> 00:11:37,660
this is all a lot like the way map works
mapreduce okay so this is the first line

438
00:11:37,660 --> 00:11:37,670
mapreduce okay so this is the first line
 

439
00:11:37,670 --> 00:11:41,260
mapreduce okay so this is the first line
in the program and you may wonder what

440
00:11:41,260 --> 00:11:41,270
in the program and you may wonder what
 

441
00:11:41,270 --> 00:11:44,170
in the program and you may wonder what
the variable lines actually hold so in

442
00:11:44,170 --> 00:11:44,180
the variable lines actually hold so in
 

443
00:11:44,180 --> 00:11:46,690
the variable lines actually hold so in
printed the result of lines but with the

444
00:11:46,690 --> 00:11:46,700
printed the result of lines but with the
 

445
00:11:46,700 --> 00:11:50,830
printed the result of lines but with the
lines points - it turns out that even

446
00:11:50,830 --> 00:11:50,840
lines points - it turns out that even
 

447
00:11:50,840 --> 00:11:53,320
lines points - it turns out that even
though it looks like we've typed a line

448
00:11:53,320 --> 00:11:53,330
though it looks like we've typed a line
 

449
00:11:53,330 --> 00:11:55,870
though it looks like we've typed a line
of code that's asking the system to read

450
00:11:55,870 --> 00:11:55,880
of code that's asking the system to read
 

451
00:11:55,880 --> 00:11:58,570
of code that's asking the system to read
a file in fact it hasn't read the file

452
00:11:58,570 --> 00:11:58,580
a file in fact it hasn't read the file
 

453
00:11:58,580 --> 00:12:02,080
a file in fact it hasn't read the file
and won't read the file for a while what

454
00:12:02,080 --> 00:12:02,090
and won't read the file for a while what
 

455
00:12:02,090 --> 00:12:03,460
and won't read the file for a while what
we're really building here with this

456
00:12:03,460 --> 00:12:03,470
we're really building here with this
 

457
00:12:03,470 --> 00:12:07,020
we're really building here with this
code what this code is doing is not

458
00:12:07,020 --> 00:12:07,030
code what this code is doing is not
 

459
00:12:07,030 --> 00:12:09,820
code what this code is doing is not
causing the input to be processed

460
00:12:09,820 --> 00:12:09,830
causing the input to be processed
 

461
00:12:09,830 --> 00:12:13,120
causing the input to be processed
instead what this code does is builds a

462
00:12:13,120 --> 00:12:13,130
instead what this code does is builds a
 

463
00:12:13,130 --> 00:12:16,390
instead what this code does is builds a
lineage graph it builds a recipe for the

464
00:12:16,390 --> 00:12:16,400
lineage graph it builds a recipe for the
 

465
00:12:16,400 --> 00:12:19,240
lineage graph it builds a recipe for the
computation we want like a little kind

466
00:12:19,240 --> 00:12:19,250
computation we want like a little kind
 

467
00:12:19,250 --> 00:12:20,770
computation we want like a little kind
of lineage graph that you see in Figure

468
00:12:20,770 --> 00:12:20,780
of lineage graph that you see in Figure
 

469
00:12:20,780 --> 00:12:23,440
of lineage graph that you see in Figure
three in the paper so what this code is

470
00:12:23,440 --> 00:12:23,450
three in the paper so what this code is
 

471
00:12:23,450 --> 00:12:25,510
three in the paper so what this code is
doing it's just building the lineage

472
00:12:25,510 --> 00:12:25,520
doing it's just building the lineage
 

473
00:12:25,520 --> 00:12:27,310
doing it's just building the lineage
graph building the computation recipe

474
00:12:27,310 --> 00:12:27,320
graph building the computation recipe
 

475
00:12:27,320 --> 00:12:30,880
graph building the computation recipe
and not doing the computation when the

476
00:12:30,880 --> 00:12:30,890
and not doing the computation when the
 

477
00:12:30,890 --> 00:12:32,950
and not doing the computation when the
computations only gonna actually start

478
00:12:32,950 --> 00:12:32,960
computations only gonna actually start
 

479
00:12:32,960 --> 00:12:35,740
computations only gonna actually start
to happen once we execute what the paper

480
00:12:35,740 --> 00:12:35,750
to happen once we execute what the paper
 

481
00:12:35,750 --> 00:12:39,070
to happen once we execute what the paper
calls an action which is a function like

482
00:12:39,070 --> 00:12:39,080
calls an action which is a function like
 

483
00:12:39,080 --> 00:12:42,790
calls an action which is a function like
collect for example to finally tell mark

484
00:12:42,790 --> 00:12:42,800
collect for example to finally tell mark
 

485
00:12:42,800 --> 00:12:44,380
collect for example to finally tell mark
oh look I actually want the output now

486
00:12:44,380 --> 00:12:44,390
oh look I actually want the output now
 

487
00:12:44,390 --> 00:12:48,220
oh look I actually want the output now
please go and actually execute the

488
00:12:48,220 --> 00:12:48,230
please go and actually execute the
 

489
00:12:48,230 --> 00:12:50,350
please go and actually execute the
lineage graph and tell me what the

490
00:12:50,350 --> 00:12:50,360
lineage graph and tell me what the
 

491
00:12:50,360 --> 00:12:51,070
lineage graph and tell me what the
result is

492
00:12:51,070 --> 00:12:51,080
result is
 

493
00:12:51,080 --> 00:12:53,830
result is
so what lines holds is actually a piece

494
00:12:53,830 --> 00:12:53,840
so what lines holds is actually a piece
 

495
00:12:53,840 --> 00:12:58,780
so what lines holds is actually a piece
of the lineage graph not a result now in

496
00:12:58,780 --> 00:12:58,790
of the lineage graph not a result now in
 

497
00:12:58,790 --> 00:13:01,210
of the lineage graph not a result now in
order to understand what the computation

498
00:13:01,210 --> 00:13:01,220
order to understand what the computation
 

499
00:13:01,220 --> 00:13:03,100
order to understand what the computation
will do when we finally run it we could

500
00:13:03,100 --> 00:13:03,110
will do when we finally run it we could
 

501
00:13:03,110 --> 00:13:07,720
will do when we finally run it we could
actually ask SPARC at this point we can

502
00:13:07,720 --> 00:13:07,730
actually ask SPARC at this point we can
 

503
00:13:07,730 --> 00:13:09,640
actually ask SPARC at this point we can
ask the interpreter to please go ahead

504
00:13:09,640 --> 00:13:09,650
ask the interpreter to please go ahead
 

505
00:13:09,650 --> 00:13:14,830
ask the interpreter to please go ahead
and tell us what you know I actually

506
00:13:14,830 --> 00:13:14,840
and tell us what you know I actually
 

507
00:13:14,840 --> 00:13:16,630
and tell us what you know I actually
execute the lineage graph up to this

508
00:13:16,630 --> 00:13:16,640
execute the lineage graph up to this
 

509
00:13:16,640 --> 00:13:19,770
execute the lineage graph up to this
point and tell us what the results are

510
00:13:19,770 --> 00:13:19,780
point and tell us what the results are
 

511
00:13:19,780 --> 00:13:22,300
point and tell us what the results are
so and you do that by calling an action

512
00:13:22,300 --> 00:13:22,310
so and you do that by calling an action
 

513
00:13:22,310 --> 00:13:24,340
so and you do that by calling an action
I'm going to call collect which so just

514
00:13:24,340 --> 00:13:24,350
I'm going to call collect which so just
 

515
00:13:24,350 --> 00:13:27,370
I'm going to call collect which so just
prints out all the results of executing

516
00:13:27,370 --> 00:13:27,380
prints out all the results of executing
 

517
00:13:27,380 --> 00:13:31,060
prints out all the results of executing
the lineage graph so far and what we're

518
00:13:31,060 --> 00:13:31,070
the lineage graph so far and what we're
 

519
00:13:31,070 --> 00:13:32,920
the lineage graph so far and what we're
expecting to see here is you know all

520
00:13:32,920 --> 00:13:32,930
expecting to see here is you know all
 

521
00:13:32,930 --> 00:13:34,780
expecting to see here is you know all
we've asked it to do so far the lineage

522
00:13:34,780 --> 00:13:34,790
we've asked it to do so far the lineage
 

523
00:13:34,790 --> 00:13:36,730
we've asked it to do so far the lineage
graph just says please read a file so

524
00:13:36,730 --> 00:13:36,740
graph just says please read a file so
 

525
00:13:36,740 --> 00:13:38,110
graph just says please read a file so
we're expecting to see that the final

526
00:13:38,110 --> 00:13:38,120
we're expecting to see that the final
 

527
00:13:38,120 --> 00:13:40,560
we're expecting to see that the final
output is just the contents of the file

528
00:13:40,560 --> 00:13:40,570
output is just the contents of the file
 

529
00:13:40,570 --> 00:13:44,010
output is just the contents of the file
and indeed that's what we get and what

530
00:13:44,010 --> 00:13:44,020
and indeed that's what we get and what
 

531
00:13:44,020 --> 00:13:46,080
and indeed that's what we get and what
what

532
00:13:46,080 --> 00:13:46,090
what
 

533
00:13:46,090 --> 00:13:48,880
what
this lineage graph this one

534
00:13:48,880 --> 00:13:48,890
this lineage graph this one
 

535
00:13:48,890 --> 00:13:52,510
this lineage graph this one
transformation lineage graph is results

536
00:13:52,510 --> 00:13:52,520
transformation lineage graph is results
 

537
00:13:52,520 --> 00:13:57,340
transformation lineage graph is results
in is just the sequence of lines one at

538
00:13:57,340 --> 00:13:57,350
in is just the sequence of lines one at
 

539
00:13:57,350 --> 00:14:01,170
in is just the sequence of lines one at
a time so it's really a set of lines a

540
00:14:01,170 --> 00:14:01,180
a time so it's really a set of lines a
 

541
00:14:01,180 --> 00:14:03,610
a time so it's really a set of lines a
set of strings each of which contains

542
00:14:03,610 --> 00:14:03,620
set of strings each of which contains
 

543
00:14:03,620 --> 00:14:05,710
set of strings each of which contains
one line of the input alright so that's

544
00:14:05,710 --> 00:14:05,720
one line of the input alright so that's
 

545
00:14:05,720 --> 00:14:10,000
one line of the input alright so that's
the first line of the program the second

546
00:14:10,000 --> 00:14:10,010
the first line of the program the second
 

547
00:14:10,010 --> 00:14:17,170
the first line of the program the second
line is is collect essentially just

548
00:14:17,170 --> 00:14:17,180
line is is collect essentially just
 

549
00:14:17,180 --> 00:14:19,750
line is is collect essentially just
just-in-time compilation of the symbolic

550
00:14:19,750 --> 00:14:19,760
just-in-time compilation of the symbolic
 

551
00:14:19,760 --> 00:14:23,350
just-in-time compilation of the symbolic
execution chain yeah yeah yeah yeah

552
00:14:23,350 --> 00:14:23,360
execution chain yeah yeah yeah yeah
 

553
00:14:23,360 --> 00:14:25,270
execution chain yeah yeah yeah yeah
that's what's going on so what collect

554
00:14:25,270 --> 00:14:25,280
that's what's going on so what collect
 

555
00:14:25,280 --> 00:14:28,210
that's what's going on so what collect
does is it actually huge amount of stuff

556
00:14:28,210 --> 00:14:28,220
does is it actually huge amount of stuff
 

557
00:14:28,220 --> 00:14:30,270
does is it actually huge amount of stuff
happens if you call collect

558
00:14:30,270 --> 00:14:30,280
happens if you call collect
 

559
00:14:30,280 --> 00:14:34,180
happens if you call collect
it tells SPARC to take the lineage graph

560
00:14:34,180 --> 00:14:34,190
it tells SPARC to take the lineage graph
 

561
00:14:34,190 --> 00:14:37,090
it tells SPARC to take the lineage graph
and produce java bytecodes

562
00:14:37,090 --> 00:14:37,100
and produce java bytecodes
 

563
00:14:37,100 --> 00:14:39,370
and produce java bytecodes
that describe all the various

564
00:14:39,370 --> 00:14:39,380
that describe all the various
 

565
00:14:39,380 --> 00:14:40,930
that describe all the various
transformations you know which in this

566
00:14:40,930 --> 00:14:40,940
transformations you know which in this
 

567
00:14:40,940 --> 00:14:42,280
transformations you know which in this
case it's not very much since we're just

568
00:14:42,280 --> 00:14:42,290
case it's not very much since we're just
 

569
00:14:42,290 --> 00:14:45,160
case it's not very much since we're just
reading a file but so SPARC well when

570
00:14:45,160 --> 00:14:45,170
reading a file but so SPARC well when
 

571
00:14:45,170 --> 00:14:48,030
reading a file but so SPARC well when
you call collect SPARC well figure out

572
00:14:48,030 --> 00:14:48,040
you call collect SPARC well figure out
 

573
00:14:48,040 --> 00:14:50,800
you call collect SPARC well figure out
where the data is you want by looking

574
00:14:50,800 --> 00:14:50,810
where the data is you want by looking
 

575
00:14:50,810 --> 00:14:53,290
where the data is you want by looking
HDFS it'll you know just pick a set of

576
00:14:53,290 --> 00:14:53,300
HDFS it'll you know just pick a set of
 

577
00:14:53,300 --> 00:14:57,370
HDFS it'll you know just pick a set of
workers to run to process the different

578
00:14:57,370 --> 00:14:57,380
workers to run to process the different
 

579
00:14:57,380 --> 00:14:59,110
workers to run to process the different
partitions of the input data it'll

580
00:14:59,110 --> 00:14:59,120
partitions of the input data it'll
 

581
00:14:59,120 --> 00:15:01,570
partitions of the input data it'll
compile the lineage graph and we reach

582
00:15:01,570 --> 00:15:01,580
compile the lineage graph and we reach
 

583
00:15:01,580 --> 00:15:03,460
compile the lineage graph and we reach
transformation in the lineage graph into

584
00:15:03,460 --> 00:15:03,470
transformation in the lineage graph into
 

585
00:15:03,470 --> 00:15:05,650
transformation in the lineage graph into
java bytecodes it sends the byte codes

586
00:15:05,650 --> 00:15:05,660
java bytecodes it sends the byte codes
 

587
00:15:05,660 --> 00:15:08,230
java bytecodes it sends the byte codes
out to the all the worker machines that

588
00:15:08,230 --> 00:15:08,240
out to the all the worker machines that
 

589
00:15:08,240 --> 00:15:10,840
out to the all the worker machines that
spark chose and those worker machines

590
00:15:10,840 --> 00:15:10,850
spark chose and those worker machines
 

591
00:15:10,850 --> 00:15:15,520
spark chose and those worker machines
execute the byte codes and the byte

592
00:15:15,520 --> 00:15:15,530
execute the byte codes and the byte
 

593
00:15:15,530 --> 00:15:18,040
execute the byte codes and the byte
codes say oh you know please read tell

594
00:15:18,040 --> 00:15:18,050
codes say oh you know please read tell
 

595
00:15:18,050 --> 00:15:19,870
codes say oh you know please read tell
each worker to read it's partition at

596
00:15:19,870 --> 00:15:19,880
each worker to read it's partition at
 

597
00:15:19,880 --> 00:15:24,760
each worker to read it's partition at
the input and then finally collect goes

598
00:15:24,760 --> 00:15:24,770
the input and then finally collect goes
 

599
00:15:24,770 --> 00:15:27,720
the input and then finally collect goes
out and fetches all the resulting data

600
00:15:27,720 --> 00:15:27,730
out and fetches all the resulting data
 

601
00:15:27,730 --> 00:15:32,110
out and fetches all the resulting data
back from the workers and so again none

602
00:15:32,110 --> 00:15:32,120
back from the workers and so again none
 

603
00:15:32,120 --> 00:15:33,580
back from the workers and so again none
of this happens until you actually

604
00:15:33,580 --> 00:15:33,590
of this happens until you actually
 

605
00:15:33,590 --> 00:15:34,900
of this happens until you actually
wanted an action and we sort of

606
00:15:34,900 --> 00:15:34,910
wanted an action and we sort of
 

607
00:15:34,910 --> 00:15:37,810
wanted an action and we sort of
prematurely run collect now you wouldn't

608
00:15:37,810 --> 00:15:37,820
prematurely run collect now you wouldn't
 

609
00:15:37,820 --> 00:15:39,160
prematurely run collect now you wouldn't
ordinarily do that I just because I just

610
00:15:39,160 --> 00:15:39,170
ordinarily do that I just because I just
 

611
00:15:39,170 --> 00:15:40,960
ordinarily do that I just because I just
want to see what the the output is to

612
00:15:40,960 --> 00:15:40,970
want to see what the the output is to
 

613
00:15:40,970 --> 00:15:43,450
want to see what the the output is to
understand what the transformations are

614
00:15:43,450 --> 00:15:43,460
understand what the transformations are
 

615
00:15:43,460 --> 00:15:46,919
understand what the transformations are
doing okay

616
00:15:46,919 --> 00:15:46,929
doing okay
 

617
00:15:46,929 --> 00:15:51,480
doing okay
if you look at the code that I'm showing

618
00:15:51,480 --> 00:15:51,490
if you look at the code that I'm showing
 

619
00:15:51,490 --> 00:15:58,169
if you look at the code that I'm showing
the second line is this map call so the

620
00:15:58,169 --> 00:15:58,179
the second line is this map call so the
 

621
00:15:58,179 --> 00:16:01,769
the second line is this map call so the
leave so line sort of refers to the

622
00:16:01,769 --> 00:16:01,779
leave so line sort of refers to the
 

623
00:16:01,779 --> 00:16:03,419
leave so line sort of refers to the
output of the first transformation which

624
00:16:03,419 --> 00:16:03,429
output of the first transformation which
 

625
00:16:03,429 --> 00:16:06,359
output of the first transformation which
is the set of strings correspond to

626
00:16:06,359 --> 00:16:06,369
is the set of strings correspond to
 

627
00:16:06,369 --> 00:16:09,540
is the set of strings correspond to
lines in the input we're gonna call map

628
00:16:09,540 --> 00:16:09,550
lines in the input we're gonna call map
 

629
00:16:09,550 --> 00:16:11,730
lines in the input we're gonna call map
we've asked the system call map on that

630
00:16:11,730 --> 00:16:11,740
we've asked the system call map on that
 

631
00:16:11,740 --> 00:16:13,650
we've asked the system call map on that
and what map does is it runs a function

632
00:16:13,650 --> 00:16:13,660
and what map does is it runs a function
 

633
00:16:13,660 --> 00:16:16,650
and what map does is it runs a function
over each element of the input that is

634
00:16:16,650 --> 00:16:16,660
over each element of the input that is
 

635
00:16:16,660 --> 00:16:18,689
over each element of the input that is
in this case or each line of the input

636
00:16:18,689 --> 00:16:18,699
in this case or each line of the input
 

637
00:16:18,699 --> 00:16:22,009
in this case or each line of the input
and that little function is the S arrow

638
00:16:22,009 --> 00:16:22,019
and that little function is the S arrow
 

639
00:16:22,019 --> 00:16:25,079
and that little function is the S arrow
whatever which basically describes a

640
00:16:25,079 --> 00:16:25,089
whatever which basically describes a
 

641
00:16:25,089 --> 00:16:27,150
whatever which basically describes a
function that calls the split function

642
00:16:27,150 --> 00:16:27,160
function that calls the split function
 

643
00:16:27,160 --> 00:16:30,059
function that calls the split function
on each line split just takes a string

644
00:16:30,059 --> 00:16:30,069
on each line split just takes a string
 

645
00:16:30,069 --> 00:16:34,980
on each line split just takes a string
and returns a array of strings broken at

646
00:16:34,980 --> 00:16:34,990
and returns a array of strings broken at
 

647
00:16:34,990 --> 00:16:37,350
and returns a array of strings broken at
the places where there are spaces and

648
00:16:37,350 --> 00:16:37,360
the places where there are spaces and
 

649
00:16:37,360 --> 00:16:39,720
the places where there are spaces and
the final part of this line that refers

650
00:16:39,720 --> 00:16:39,730
the final part of this line that refers
 

651
00:16:39,730 --> 00:16:42,449
the final part of this line that refers
to parts 0 & 1 says that for each line

652
00:16:42,449 --> 00:16:42,459
to parts 0 & 1 says that for each line
 

653
00:16:42,459 --> 00:16:44,730
to parts 0 & 1 says that for each line
of input we want to at the output of

654
00:16:44,730 --> 00:16:44,740
of input we want to at the output of
 

655
00:16:44,740 --> 00:16:48,660
of input we want to at the output of
this transformation be the first string

656
00:16:48,660 --> 00:16:48,670
this transformation be the first string
 

657
00:16:48,670 --> 00:16:51,030
this transformation be the first string
on the line and then the second string

658
00:16:51,030 --> 00:16:51,040
on the line and then the second string
 

659
00:16:51,040 --> 00:16:52,259
on the line and then the second string
of the line so we're just doing a little

660
00:16:52,259 --> 00:16:52,269
of the line so we're just doing a little
 

661
00:16:52,269 --> 00:16:54,179
of the line so we're just doing a little
transformation to turn these strings

662
00:16:54,179 --> 00:16:54,189
transformation to turn these strings
 

663
00:16:54,189 --> 00:16:55,859
transformation to turn these strings
into something that's a little bit

664
00:16:55,859 --> 00:16:55,869
into something that's a little bit
 

665
00:16:55,869 --> 00:16:59,009
into something that's a little bit
easier to process and again at a

666
00:16:59,009 --> 00:16:59,019
easier to process and again at a
 

667
00:16:59,019 --> 00:17:00,019
easier to process and again at a
curiosity

668
00:17:00,019 --> 00:17:00,029
curiosity
 

669
00:17:00,029 --> 00:17:02,789
curiosity
I'm gonna call collect on links one just

670
00:17:02,789 --> 00:17:02,799
I'm gonna call collect on links one just
 

671
00:17:02,799 --> 00:17:04,679
I'm gonna call collect on links one just
to verify that we understand what it

672
00:17:04,679 --> 00:17:04,689
to verify that we understand what it
 

673
00:17:04,689 --> 00:17:09,319
to verify that we understand what it
does and you can see where as lines held

674
00:17:09,319 --> 00:17:09,329
does and you can see where as lines held
 

675
00:17:09,329 --> 00:17:13,889
does and you can see where as lines held
just string lines links one now holds

676
00:17:13,889 --> 00:17:13,899
just string lines links one now holds
 

677
00:17:13,899 --> 00:17:18,480
just string lines links one now holds
pairs of strings of from URL and to URL

678
00:17:18,480 --> 00:17:18,490
pairs of strings of from URL and to URL
 

679
00:17:18,490 --> 00:17:26,460
pairs of strings of from URL and to URL
one for each link and when this executes

680
00:17:26,460 --> 00:17:26,470
one for each link and when this executes
 

681
00:17:26,470 --> 00:17:28,620
one for each link and when this executes
this map executes it can execute totally

682
00:17:28,620 --> 00:17:28,630
this map executes it can execute totally
 

683
00:17:28,630 --> 00:17:30,659
this map executes it can execute totally
independently on each worker on its own

684
00:17:30,659 --> 00:17:30,669
independently on each worker on its own
 

685
00:17:30,669 --> 00:17:32,789
independently on each worker on its own
partition of the input because it's just

686
00:17:32,789 --> 00:17:32,799
partition of the input because it's just
 

687
00:17:32,799 --> 00:17:34,710
partition of the input because it's just
considering each line independently

688
00:17:34,710 --> 00:17:34,720
considering each line independently
 

689
00:17:34,720 --> 00:17:37,620
considering each line independently
there's no interaction involved between

690
00:17:37,620 --> 00:17:37,630
there's no interaction involved between
 

691
00:17:37,630 --> 00:17:39,090
there's no interaction involved between
different lines or different partitions

692
00:17:39,090 --> 00:17:39,100
different lines or different partitions
 

693
00:17:39,100 --> 00:17:41,940
different lines or different partitions
these are it's running if these this map

694
00:17:41,940 --> 00:17:41,950
these are it's running if these this map
 

695
00:17:41,950 --> 00:17:45,210
these are it's running if these this map
is a purely local operation on each

696
00:17:45,210 --> 00:17:45,220
is a purely local operation on each
 

697
00:17:45,220 --> 00:17:47,850
is a purely local operation on each
input record so can run totally in

698
00:17:47,850 --> 00:17:47,860
input record so can run totally in
 

699
00:17:47,860 --> 00:17:49,980
input record so can run totally in
parallel on all the workers on all their

700
00:17:49,980 --> 00:17:49,990
parallel on all the workers on all their
 

701
00:17:49,990 --> 00:17:55,919
parallel on all the workers on all their
partitions ok the next line in the

702
00:17:55,919 --> 00:17:55,929
partitions ok the next line in the
 

703
00:17:55,929 --> 00:17:59,390
partitions ok the next line in the
program is this called the distinct and

704
00:17:59,390 --> 00:17:59,400
program is this called the distinct and
 

705
00:17:59,400 --> 00:18:02,310
program is this called the distinct and
what's going on here is that we only

706
00:18:02,310 --> 00:18:02,320
what's going on here is that we only
 

707
00:18:02,320 --> 00:18:04,799
what's going on here is that we only
want to count each link once so if a

708
00:18:04,799 --> 00:18:04,809
want to count each link once so if a
 

709
00:18:04,809 --> 00:18:07,380
want to count each link once so if a
given page has multiple links to another

710
00:18:07,380 --> 00:18:07,390
given page has multiple links to another
 

711
00:18:07,390 --> 00:18:10,980
given page has multiple links to another
page we want to only consider one of

712
00:18:10,980 --> 00:18:10,990
page we want to only consider one of
 

713
00:18:10,990 --> 00:18:15,779
page we want to only consider one of
them for the purposes of PageRank and so

714
00:18:15,779 --> 00:18:15,789
them for the purposes of PageRank and so
 

715
00:18:15,789 --> 00:18:17,669
them for the purposes of PageRank and so
this just looks for duplicates now if

716
00:18:17,669 --> 00:18:17,679
this just looks for duplicates now if
 

717
00:18:17,679 --> 00:18:19,700
this just looks for duplicates now if
you think about what it actually takes

718
00:18:19,700 --> 00:18:19,710
you think about what it actually takes
 

719
00:18:19,710 --> 00:18:23,070
you think about what it actually takes
to look for duplicates in a you know

720
00:18:23,070 --> 00:18:23,080
to look for duplicates in a you know
 

721
00:18:23,080 --> 00:18:28,310
to look for duplicates in a you know
multi terabyte collection of data items

722
00:18:28,310 --> 00:18:28,320
multi terabyte collection of data items
 

723
00:18:28,320 --> 00:18:30,029
multi terabyte collection of data items
it's no joke

724
00:18:30,029 --> 00:18:30,039
it's no joke
 

725
00:18:30,039 --> 00:18:32,100
it's no joke
because the data items are in some

726
00:18:32,100 --> 00:18:32,110
because the data items are in some
 

727
00:18:32,110 --> 00:18:34,289
because the data items are in some
random order and the input and what

728
00:18:34,289 --> 00:18:34,299
random order and the input and what
 

729
00:18:34,299 --> 00:18:36,899
random order and the input and what
distinct needs to do since an e sirup

730
00:18:36,899 --> 00:18:36,909
distinct needs to do since an e sirup
 

731
00:18:36,909 --> 00:18:39,659
distinct needs to do since an e sirup
replace each duplicated input with a

732
00:18:39,659 --> 00:18:39,669
replace each duplicated input with a
 

733
00:18:39,669 --> 00:18:42,930
replace each duplicated input with a
single input distinct needs to somehow

734
00:18:42,930 --> 00:18:42,940
single input distinct needs to somehow
 

735
00:18:42,940 --> 00:18:45,960
single input distinct needs to somehow
bring together all of the items that are

736
00:18:45,960 --> 00:18:45,970
bring together all of the items that are
 

737
00:18:45,970 --> 00:18:48,000
bring together all of the items that are
identical and that's going to require

738
00:18:48,000 --> 00:18:48,010
identical and that's going to require
 

739
00:18:48,010 --> 00:18:49,770
identical and that's going to require
communication remember that all these

740
00:18:49,770 --> 00:18:49,780
communication remember that all these
 

741
00:18:49,780 --> 00:18:51,710
communication remember that all these
data is spread out over all the workers

742
00:18:51,710 --> 00:18:51,720
data is spread out over all the workers
 

743
00:18:51,720 --> 00:18:54,090
data is spread out over all the workers
we want to make sure that any you know

744
00:18:54,090 --> 00:18:54,100
we want to make sure that any you know
 

745
00:18:54,100 --> 00:18:55,649
we want to make sure that any you know
that we bring we sort of shuffle the

746
00:18:55,649 --> 00:18:55,659
that we bring we sort of shuffle the
 

747
00:18:55,659 --> 00:18:57,990
that we bring we sort of shuffle the
data around so that any two items that

748
00:18:57,990 --> 00:18:58,000
data around so that any two items that
 

749
00:18:58,000 --> 00:18:59,610
data around so that any two items that
are identical or on the same worker so

750
00:18:59,610 --> 00:18:59,620
are identical or on the same worker so
 

751
00:18:59,620 --> 00:19:00,960
are identical or on the same worker so
that that worker can do this I'll wait a

752
00:19:00,960 --> 00:19:00,970
that that worker can do this I'll wait a
 

753
00:19:00,970 --> 00:19:02,070
that that worker can do this I'll wait a
minute there's three of these I'm gonna

754
00:19:02,070 --> 00:19:02,080
minute there's three of these I'm gonna
 

755
00:19:02,080 --> 00:19:04,440
minute there's three of these I'm gonna
replace it these three with a single one

756
00:19:04,440 --> 00:19:04,450
replace it these three with a single one
 

757
00:19:04,450 --> 00:19:06,779
replace it these three with a single one
I mean that means that distinct when it

758
00:19:06,779 --> 00:19:06,789
I mean that means that distinct when it
 

759
00:19:06,789 --> 00:19:09,120
I mean that means that distinct when it
finally comes to execute requires

760
00:19:09,120 --> 00:19:09,130
finally comes to execute requires
 

761
00:19:09,130 --> 00:19:12,750
finally comes to execute requires
communication it's a shuffle and so the

762
00:19:12,750 --> 00:19:12,760
communication it's a shuffle and so the
 

763
00:19:12,760 --> 00:19:13,980
communication it's a shuffle and so the
shuffle is going to be driven by either

764
00:19:13,980 --> 00:19:13,990
shuffle is going to be driven by either
 

765
00:19:13,990 --> 00:19:17,100
shuffle is going to be driven by either
hashing the items the hashing the items

766
00:19:17,100 --> 00:19:17,110
hashing the items the hashing the items
 

767
00:19:17,110 --> 00:19:18,390
hashing the items the hashing the items
to pick the worker that will process

768
00:19:18,390 --> 00:19:18,400
to pick the worker that will process
 

769
00:19:18,400 --> 00:19:20,159
to pick the worker that will process
that item and then sending the item

770
00:19:20,159 --> 00:19:20,169
that item and then sending the item
 

771
00:19:20,169 --> 00:19:22,049
that item and then sending the item
across the network or you know possibly

772
00:19:22,049 --> 00:19:22,059
across the network or you know possibly
 

773
00:19:22,059 --> 00:19:24,450
across the network or you know possibly
you could be implemented with a sort or

774
00:19:24,450 --> 00:19:24,460
you could be implemented with a sort or
 

775
00:19:24,460 --> 00:19:26,310
you could be implemented with a sort or
the system sort of sorts all the input

776
00:19:26,310 --> 00:19:26,320
the system sort of sorts all the input
 

777
00:19:26,320 --> 00:19:31,860
the system sort of sorts all the input
and then splits up the sorted input

778
00:19:31,860 --> 00:19:31,870

 

779
00:19:31,870 --> 00:19:35,650

overall the workers I'd actually don't

780
00:19:35,650 --> 00:19:35,660
overall the workers I'd actually don't
 

781
00:19:35,660 --> 00:19:37,900
overall the workers I'd actually don't
know which it does but anyway I'm gonna

782
00:19:37,900 --> 00:19:37,910
know which it does but anyway I'm gonna
 

783
00:19:37,910 --> 00:19:40,480
know which it does but anyway I'm gonna
require a lot of computation in this

784
00:19:40,480 --> 00:19:40,490
require a lot of computation in this
 

785
00:19:40,490 --> 00:19:42,430
require a lot of computation in this
case however almost fact nothing

786
00:19:42,430 --> 00:19:42,440
case however almost fact nothing
 

787
00:19:42,440 --> 00:19:44,230
case however almost fact nothing
whatsoever happens because there were no

788
00:19:44,230 --> 00:19:44,240
whatsoever happens because there were no
 

789
00:19:44,240 --> 00:19:49,290
whatsoever happens because there were no
duplicates and sorry whoops

790
00:19:49,290 --> 00:19:49,300
duplicates and sorry whoops
 

791
00:19:49,300 --> 00:19:54,850
duplicates and sorry whoops
links to all right so anyone collect and

792
00:19:54,850 --> 00:19:54,860
links to all right so anyone collect and
 

793
00:19:54,860 --> 00:19:58,360
links to all right so anyone collect and
the links to which is the output a

794
00:19:58,360 --> 00:19:58,370
the links to which is the output a
 

795
00:19:58,370 --> 00:20:02,640
the links to which is the output a
distinct is basically except for order

796
00:20:02,640 --> 00:20:02,650
distinct is basically except for order
 

797
00:20:02,650 --> 00:20:05,620
distinct is basically except for order
identical two links one which was the

798
00:20:05,620 --> 00:20:05,630
identical two links one which was the
 

799
00:20:05,630 --> 00:20:07,810
identical two links one which was the
input to that transformation and the

800
00:20:07,810 --> 00:20:07,820
input to that transformation and the
 

801
00:20:07,820 --> 00:20:09,070
input to that transformation and the
orders change because of course it has

802
00:20:09,070 --> 00:20:09,080
orders change because of course it has
 

803
00:20:09,080 --> 00:20:12,220
orders change because of course it has
to hash or sort or something all right

804
00:20:12,220 --> 00:20:12,230
to hash or sort or something all right
 

805
00:20:12,230 --> 00:20:19,300
to hash or sort or something all right
the next the next transformation is is

806
00:20:19,300 --> 00:20:19,310
the next the next transformation is is
 

807
00:20:19,310 --> 00:20:24,550
the next the next transformation is is
grouped by key and here what we're

808
00:20:24,550 --> 00:20:24,560
grouped by key and here what we're
 

809
00:20:24,560 --> 00:20:27,880
grouped by key and here what we're
heading towards is we want to collect

810
00:20:27,880 --> 00:20:27,890
heading towards is we want to collect
 

811
00:20:27,890 --> 00:20:33,040
heading towards is we want to collect
all of the links it turns out for the

812
00:20:33,040 --> 00:20:33,050
all of the links it turns out for the
 

813
00:20:33,050 --> 00:20:35,110
all of the links it turns out for the
computation with little C we want to

814
00:20:35,110 --> 00:20:35,120
computation with little C we want to
 

815
00:20:35,120 --> 00:20:36,670
computation with little C we want to
collect together all the links from a

816
00:20:36,670 --> 00:20:36,680
collect together all the links from a
 

817
00:20:36,680 --> 00:20:40,390
collect together all the links from a
given page into one place so the group

818
00:20:40,390 --> 00:20:40,400
given page into one place so the group
 

819
00:20:40,400 --> 00:20:43,090
given page into one place so the group
by key is gonna group by it's gonna move

820
00:20:43,090 --> 00:20:43,100
by key is gonna group by it's gonna move
 

821
00:20:43,100 --> 00:20:45,370
by key is gonna group by it's gonna move
all the records all these from two URL

822
00:20:45,370 --> 00:20:45,380
all the records all these from two URL
 

823
00:20:45,380 --> 00:20:47,830
all the records all these from two URL
pairs it's gonna group them by the from

824
00:20:47,830 --> 00:20:47,840
pairs it's gonna group them by the from
 

825
00:20:47,840 --> 00:20:51,150
pairs it's gonna group them by the from
URL that is it's gonna bring together

826
00:20:51,150 --> 00:20:51,160
URL that is it's gonna bring together
 

827
00:20:51,160 --> 00:20:55,480
URL that is it's gonna bring together
all the links that are from the same

828
00:20:55,480 --> 00:20:55,490
all the links that are from the same
 

829
00:20:55,490 --> 00:20:57,640
all the links that are from the same
page and it's gonna actually collapse

830
00:20:57,640 --> 00:20:57,650
page and it's gonna actually collapse
 

831
00:20:57,650 --> 00:20:59,830
page and it's gonna actually collapse
them down into the whole collection of

832
00:20:59,830 --> 00:20:59,840
them down into the whole collection of
 

833
00:20:59,840 --> 00:21:01,990
them down into the whole collection of
links from each page is gonna collapse

834
00:21:01,990 --> 00:21:02,000
links from each page is gonna collapse
 

835
00:21:02,000 --> 00:21:04,180
links from each page is gonna collapse
them down into a list of links into that

836
00:21:04,180 --> 00:21:04,190
them down into a list of links into that
 

837
00:21:04,190 --> 00:21:07,770
them down into a list of links into that
pages URL plus a list of the links that

838
00:21:07,770 --> 00:21:07,780
pages URL plus a list of the links that
 

839
00:21:07,780 --> 00:21:10,960
pages URL plus a list of the links that
start at that page and again this is

840
00:21:10,960 --> 00:21:10,970
start at that page and again this is
 

841
00:21:10,970 --> 00:21:17,040
start at that page and again this is
gonna require communication although

842
00:21:17,040 --> 00:21:17,050
gonna require communication although
 

843
00:21:17,050 --> 00:21:19,480
gonna require communication although
spark I suspect spark is clever enough

844
00:21:19,480 --> 00:21:19,490
spark I suspect spark is clever enough
 

845
00:21:19,490 --> 00:21:21,580
spark I suspect spark is clever enough
to optimize this because the distinct

846
00:21:21,580 --> 00:21:21,590
to optimize this because the distinct
 

847
00:21:21,590 --> 00:21:27,610
to optimize this because the distinct
already um put all records with the same

848
00:21:27,610 --> 00:21:27,620
already um put all records with the same
 

849
00:21:27,620 --> 00:21:31,570
already um put all records with the same
from URL on the same worker the group by

850
00:21:31,570 --> 00:21:31,580
from URL on the same worker the group by
 

851
00:21:31,580 --> 00:21:35,170
from URL on the same worker the group by
key could easily and may well just I'm

852
00:21:35,170 --> 00:21:35,180
key could easily and may well just I'm
 

853
00:21:35,180 --> 00:21:36,490
key could easily and may well just I'm
not have to communicate at all because

854
00:21:36,490 --> 00:21:36,500
not have to communicate at all because
 

855
00:21:36,500 --> 00:21:38,080
not have to communicate at all because
it can observe that the data is already

856
00:21:38,080 --> 00:21:38,090
it can observe that the data is already
 

857
00:21:38,090 --> 00:21:41,950
it can observe that the data is already
grouped by the from URL key all right so

858
00:21:41,950 --> 00:21:41,960
grouped by the from URL key all right so
 

859
00:21:41,960 --> 00:21:44,560
grouped by the from URL key all right so
let's print links three

860
00:21:44,560 --> 00:21:44,570
let's print links three
 

861
00:21:44,570 --> 00:21:47,139
let's print links three
let's run collect actually drive the

862
00:21:47,139 --> 00:21:47,149
let's run collect actually drive the
 

863
00:21:47,149 --> 00:21:52,919
let's run collect actually drive the
computation and see what the result is

864
00:21:52,919 --> 00:21:52,929
computation and see what the result is
 

865
00:21:52,929 --> 00:21:55,419
computation and see what the result is
and indeed what we're looking at here is

866
00:21:55,419 --> 00:21:55,429
and indeed what we're looking at here is
 

867
00:21:55,429 --> 00:21:59,560
and indeed what we're looking at here is
an array of couples where the first part

868
00:21:59,560 --> 00:21:59,570
an array of couples where the first part
 

869
00:21:59,570 --> 00:22:01,629
an array of couples where the first part
of each tuple is the URL the from page

870
00:22:01,629 --> 00:22:01,639
of each tuple is the URL the from page
 

871
00:22:01,639 --> 00:22:05,529
of each tuple is the URL the from page
and the second is the list of links that

872
00:22:05,529 --> 00:22:05,539
and the second is the list of links that
 

873
00:22:05,539 --> 00:22:08,199
and the second is the list of links that
start at that front page and so you can

874
00:22:08,199 --> 00:22:08,209
start at that front page and so you can
 

875
00:22:08,209 --> 00:22:10,149
start at that front page and so you can
see the YouTube has a link to you two

876
00:22:10,149 --> 00:22:10,159
see the YouTube has a link to you two
 

877
00:22:10,159 --> 00:22:12,189
see the YouTube has a link to you two
and three you three as a link to just u

878
00:22:12,189 --> 00:22:12,199
and three you three as a link to just u
 

879
00:22:12,199 --> 00:22:22,899
and three you three as a link to just u
1 and u 1 has a link to u 1 & u 3 okay

880
00:22:22,899 --> 00:22:22,909
1 and u 1 has a link to u 1 & u 3 okay
 

881
00:22:22,909 --> 00:22:29,049
1 and u 1 has a link to u 1 & u 3 okay
so that's link 3 now the iteration is

882
00:22:29,049 --> 00:22:29,059
so that's link 3 now the iteration is
 

883
00:22:29,059 --> 00:22:30,759
so that's link 3 now the iteration is
going to start in a couple lines from

884
00:22:30,759 --> 00:22:30,769
going to start in a couple lines from
 

885
00:22:30,769 --> 00:22:32,859
going to start in a couple lines from
here it's gonna use these things over

886
00:22:32,859 --> 00:22:32,869
here it's gonna use these things over
 

887
00:22:32,869 --> 00:22:35,919
here it's gonna use these things over
and over again each iteration of the

888
00:22:35,919 --> 00:22:35,929
and over again each iteration of the
 

889
00:22:35,929 --> 00:22:39,609
and over again each iteration of the
loop is going to use this this

890
00:22:39,609 --> 00:22:39,619
loop is going to use this this
 

891
00:22:39,619 --> 00:22:44,619
loop is going to use this this
information in links 3 in order to sort

892
00:22:44,619 --> 00:22:44,629
information in links 3 in order to sort
 

893
00:22:44,629 --> 00:22:46,539
information in links 3 in order to sort
of propagate probabilities in order to

894
00:22:46,539 --> 00:22:46,549
of propagate probabilities in order to
 

895
00:22:46,549 --> 00:22:49,419
of propagate probabilities in order to
sort of simulate these user clicking I'm

896
00:22:49,419 --> 00:22:49,429
sort of simulate these user clicking I'm
 

897
00:22:49,429 --> 00:22:52,449
sort of simulate these user clicking I'm
from from all pages to all other link to

898
00:22:52,449 --> 00:22:52,459
from from all pages to all other link to
 

899
00:22:52,459 --> 00:22:55,299
from from all pages to all other link to
two pages so this length stuff is these

900
00:22:55,299 --> 00:22:55,309
two pages so this length stuff is these
 

901
00:22:55,309 --> 00:22:57,129
two pages so this length stuff is these
links data is gonna be used over and

902
00:22:57,129 --> 00:22:57,139
links data is gonna be used over and
 

903
00:22:57,139 --> 00:22:58,449
links data is gonna be used over and
over again and we're gonna want to save

904
00:22:58,449 --> 00:22:58,459
over again and we're gonna want to save
 

905
00:22:58,459 --> 00:23:00,249
over again and we're gonna want to save
it it turns out that each time I've

906
00:23:00,249 --> 00:23:00,259
it it turns out that each time I've
 

907
00:23:00,259 --> 00:23:02,699
it it turns out that each time I've
called collect so far spark has

908
00:23:02,699 --> 00:23:02,709
called collect so far spark has
 

909
00:23:02,709 --> 00:23:05,079
called collect so far spark has
re-execute 'add the computation from

910
00:23:05,079 --> 00:23:05,089
re-execute 'add the computation from
 

911
00:23:05,089 --> 00:23:06,969
re-execute 'add the computation from
scratch so every call to collect I've

912
00:23:06,969 --> 00:23:06,979
scratch so every call to collect I've
 

913
00:23:06,979 --> 00:23:09,789
scratch so every call to collect I've
made has involved spark rereading the

914
00:23:09,789 --> 00:23:09,799
made has involved spark rereading the
 

915
00:23:09,799 --> 00:23:12,279
made has involved spark rereading the
input file re running that first map

916
00:23:12,279 --> 00:23:12,289
input file re running that first map
 

917
00:23:12,289 --> 00:23:14,799
input file re running that first map
rerunning the distinct and if I were to

918
00:23:14,799 --> 00:23:14,809
rerunning the distinct and if I were to
 

919
00:23:14,809 --> 00:23:17,139
rerunning the distinct and if I were to
call collect again it would rerun this

920
00:23:17,139 --> 00:23:17,149
call collect again it would rerun this
 

921
00:23:17,149 --> 00:23:18,759
call collect again it would rerun this
route by key but we don't want to have

922
00:23:18,759 --> 00:23:18,769
route by key but we don't want to have
 

923
00:23:18,769 --> 00:23:20,289
route by key but we don't want to have
to do that over and over again on sort

924
00:23:20,289 --> 00:23:20,299
to do that over and over again on sort
 

925
00:23:20,299 --> 00:23:25,269
to do that over and over again on sort
of multiple terabytes of links for each

926
00:23:25,269 --> 00:23:25,279
of multiple terabytes of links for each
 

927
00:23:25,279 --> 00:23:28,029
of multiple terabytes of links for each
loop iteration because we've computed it

928
00:23:28,029 --> 00:23:28,039
loop iteration because we've computed it
 

929
00:23:28,039 --> 00:23:29,649
loop iteration because we've computed it
once and it's gonna state this list of

930
00:23:29,649 --> 00:23:29,659
once and it's gonna state this list of
 

931
00:23:29,659 --> 00:23:31,359
once and it's gonna state this list of
links is gonna stay the same we just

932
00:23:31,359 --> 00:23:31,369
links is gonna stay the same we just
 

933
00:23:31,369 --> 00:23:34,659
links is gonna stay the same we just
want to save it and reuse it so in order

934
00:23:34,659 --> 00:23:34,669
want to save it and reuse it so in order
 

935
00:23:34,669 --> 00:23:38,409
want to save it and reuse it so in order
to tell spark that look we want to use

936
00:23:38,409 --> 00:23:38,419
to tell spark that look we want to use
 

937
00:23:38,419 --> 00:23:39,699
to tell spark that look we want to use
this over and over again the programmer

938
00:23:39,699 --> 00:23:39,709
this over and over again the programmer
 

939
00:23:39,709 --> 00:23:42,999
this over and over again the programmer
is required to explicitly what the paper

940
00:23:42,999 --> 00:23:43,009
is required to explicitly what the paper
 

941
00:23:43,009 --> 00:23:48,669
is required to explicitly what the paper
calls persist this data and in fact

942
00:23:48,669 --> 00:23:48,679
calls persist this data and in fact
 

943
00:23:48,679 --> 00:23:51,790
calls persist this data and in fact
modern spark the function you call

944
00:23:51,790 --> 00:23:51,800
modern spark the function you call
 

945
00:23:51,800 --> 00:23:53,260
modern spark the function you call
not persist if you want to sleep in a

946
00:23:53,260 --> 00:23:53,270
not persist if you want to sleep in a
 

947
00:23:53,270 --> 00:23:55,900
not persist if you want to sleep in a
memory but but it's called cash and so

948
00:23:55,900 --> 00:23:55,910
memory but but it's called cash and so
 

949
00:23:55,910 --> 00:23:59,020
memory but but it's called cash and so
links for is just identical the links we

950
00:23:59,020 --> 00:23:59,030
links for is just identical the links we
 

951
00:23:59,030 --> 00:24:03,550
links for is just identical the links we
accept with the annotation that we'd

952
00:24:03,550 --> 00:24:03,560
accept with the annotation that we'd
 

953
00:24:03,560 --> 00:24:06,610
accept with the annotation that we'd
like sparked keep links for in memory

954
00:24:06,610 --> 00:24:06,620
like sparked keep links for in memory
 

955
00:24:06,620 --> 00:24:07,900
like sparked keep links for in memory
because we're gonna use it over and over

956
00:24:07,900 --> 00:24:07,910
because we're gonna use it over and over
 

957
00:24:07,910 --> 00:24:14,290
because we're gonna use it over and over
again ok so that the last thing we need

958
00:24:14,290 --> 00:24:14,300
again ok so that the last thing we need
 

959
00:24:14,300 --> 00:24:16,330
again ok so that the last thing we need
to do before the loop starts is we're

960
00:24:16,330 --> 00:24:16,340
to do before the loop starts is we're
 

961
00:24:16,340 --> 00:24:20,500
to do before the loop starts is we're
gonna have a set of page ranks for every

962
00:24:20,500 --> 00:24:20,510
gonna have a set of page ranks for every
 

963
00:24:20,510 --> 00:24:23,560
gonna have a set of page ranks for every
page indexed by source URL and we need

964
00:24:23,560 --> 00:24:23,570
page indexed by source URL and we need
 

965
00:24:23,570 --> 00:24:28,240
page indexed by source URL and we need
to initialize every pages rank it's not

966
00:24:28,240 --> 00:24:28,250
to initialize every pages rank it's not
 

967
00:24:28,250 --> 00:24:29,400
to initialize every pages rank it's not
really ranks here it's kind of

968
00:24:29,400 --> 00:24:29,410
really ranks here it's kind of
 

969
00:24:29,410 --> 00:24:33,040
really ranks here it's kind of
probabilities we're gonna initialize all

970
00:24:33,040 --> 00:24:33,050
probabilities we're gonna initialize all
 

971
00:24:33,050 --> 00:24:35,890
probabilities we're gonna initialize all
the probabilities to one so they all

972
00:24:35,890 --> 00:24:35,900
the probabilities to one so they all
 

973
00:24:35,900 --> 00:24:38,290
the probabilities to one so they all
start out with a probability one with

974
00:24:38,290 --> 00:24:38,300
start out with a probability one with
 

975
00:24:38,300 --> 00:24:41,650
start out with a probability one with
the same rank but we're gonna well we're

976
00:24:41,650 --> 00:24:41,660
the same rank but we're gonna well we're
 

977
00:24:41,660 --> 00:24:43,690
the same rank but we're gonna well we're
gonna actually you code that looks like

978
00:24:43,690 --> 00:24:43,700
gonna actually you code that looks like
 

979
00:24:43,700 --> 00:24:49,600
gonna actually you code that looks like
it's changing ranks but in fact when we

980
00:24:49,600 --> 00:24:49,610
it's changing ranks but in fact when we
 

981
00:24:49,610 --> 00:24:52,120
it's changing ranks but in fact when we
execute the loop in the code I'm showing

982
00:24:52,120 --> 00:24:52,130
execute the loop in the code I'm showing
 

983
00:24:52,130 --> 00:24:54,400
execute the loop in the code I'm showing
it really produces a new version of

984
00:24:54,400 --> 00:24:54,410
it really produces a new version of
 

985
00:24:54,410 --> 00:24:56,530
it really produces a new version of
ranks for every loop iteration that's

986
00:24:56,530 --> 00:24:56,540
ranks for every loop iteration that's
 

987
00:24:56,540 --> 00:25:00,160
ranks for every loop iteration that's
updated to reflect the fact that the

988
00:25:00,160 --> 00:25:00,170
updated to reflect the fact that the
 

989
00:25:00,170 --> 00:25:02,620
updated to reflect the fact that the
code algorithm is kind of pushed page

990
00:25:02,620 --> 00:25:02,630
code algorithm is kind of pushed page
 

991
00:25:02,630 --> 00:25:07,860
code algorithm is kind of pushed page
ranks from each from each P

992
00:25:07,860 --> 00:25:07,870
ranks from each from each P
 

993
00:25:07,870 --> 00:25:10,140
ranks from each from each P
to the page is that it links to so let's

994
00:25:10,140 --> 00:25:10,150
to the page is that it links to so let's
 

995
00:25:10,150 --> 00:25:13,040
to the page is that it links to so let's
print ranks also to see what's inside

996
00:25:13,040 --> 00:25:13,050
print ranks also to see what's inside
 

997
00:25:13,050 --> 00:25:17,250
print ranks also to see what's inside
it's just a mapping from URL from source

998
00:25:17,250 --> 00:25:17,260
it's just a mapping from URL from source
 

999
00:25:17,260 --> 00:25:20,460
it's just a mapping from URL from source
URL to the current page rank value for

1000
00:25:20,460 --> 00:25:20,470
URL to the current page rank value for
 

1001
00:25:20,470 --> 00:25:23,250
URL to the current page rank value for
every page ok not gonna start executing

1002
00:25:23,250 --> 00:25:23,260
every page ok not gonna start executing
 

1003
00:25:23,260 --> 00:25:27,840
every page ok not gonna start executing
inside the spark allow the user to

1004
00:25:27,840 --> 00:25:27,850
inside the spark allow the user to
 

1005
00:25:27,850 --> 00:25:30,210
inside the spark allow the user to
request more fine-grained scheduling

1006
00:25:30,210 --> 00:25:30,220
request more fine-grained scheduling
 

1007
00:25:30,220 --> 00:25:32,490
request more fine-grained scheduling
primitives than cache that is to control

1008
00:25:32,490 --> 00:25:32,500
primitives than cache that is to control
 

1009
00:25:32,500 --> 00:25:33,690
primitives than cache that is to control
where that is stored or how the

1010
00:25:33,690 --> 00:25:33,700
where that is stored or how the
 

1011
00:25:33,700 --> 00:25:38,250
where that is stored or how the
computations are performed well yeah so

1012
00:25:38,250 --> 00:25:38,260
computations are performed well yeah so
 

1013
00:25:38,260 --> 00:25:41,010
computations are performed well yeah so
cache cache is a special case of a more

1014
00:25:41,010 --> 00:25:41,020
cache cache is a special case of a more
 

1015
00:25:41,020 --> 00:25:44,640
cache cache is a special case of a more
general persist call which can tell

1016
00:25:44,640 --> 00:25:44,650
general persist call which can tell
 

1017
00:25:44,650 --> 00:25:46,770
general persist call which can tell
spark look I want to you know save this

1018
00:25:46,770 --> 00:25:46,780
spark look I want to you know save this
 

1019
00:25:46,780 --> 00:25:49,190
spark look I want to you know save this
data in memory or I want to save it in

1020
00:25:49,190 --> 00:25:49,200
data in memory or I want to save it in
 

1021
00:25:49,200 --> 00:25:52,169
data in memory or I want to save it in
HDFS so that it's replicated and all

1022
00:25:52,169 --> 00:25:52,179
HDFS so that it's replicated and all
 

1023
00:25:52,179 --> 00:25:53,820
HDFS so that it's replicated and all
survived crashes so you got a little

1024
00:25:53,820 --> 00:25:53,830
survived crashes so you got a little
 

1025
00:25:53,830 --> 00:25:58,650
survived crashes so you got a little
flexibility there in general you know we

1026
00:25:58,650 --> 00:25:58,660
flexibility there in general you know we
 

1027
00:25:58,660 --> 00:26:00,000
flexibility there in general you know we
didn't have to say anything about the

1028
00:26:00,000 --> 00:26:00,010
didn't have to say anything about the
 

1029
00:26:00,010 --> 00:26:04,230
didn't have to say anything about the
partitioning in this code and spark will

1030
00:26:04,230 --> 00:26:04,240
partitioning in this code and spark will
 

1031
00:26:04,240 --> 00:26:07,110
partitioning in this code and spark will
just choose something at first the

1032
00:26:07,110 --> 00:26:07,120
just choose something at first the
 

1033
00:26:07,120 --> 00:26:09,500
just choose something at first the
partitioning is driven by the

1034
00:26:09,500 --> 00:26:09,510
partitioning is driven by the
 

1035
00:26:09,510 --> 00:26:11,640
partitioning is driven by the
partitioning of the original input files

1036
00:26:11,640 --> 00:26:11,650
partitioning of the original input files
 

1037
00:26:11,650 --> 00:26:15,990
partitioning of the original input files
but when we run transformations that had

1038
00:26:15,990 --> 00:26:16,000
but when we run transformations that had
 

1039
00:26:16,000 --> 00:26:17,430
but when we run transformations that had
to shuffle had to change the

1040
00:26:17,430 --> 00:26:17,440
to shuffle had to change the
 

1041
00:26:17,440 --> 00:26:19,049
to shuffle had to change the
partitioning like distinct it does that

1042
00:26:19,049 --> 00:26:19,059
partitioning like distinct it does that
 

1043
00:26:19,059 --> 00:26:21,690
partitioning like distinct it does that
and group by key does that spark will do

1044
00:26:21,690 --> 00:26:21,700
and group by key does that spark will do
 

1045
00:26:21,700 --> 00:26:25,049
and group by key does that spark will do
something internally that if we don't do

1046
00:26:25,049 --> 00:26:25,059
something internally that if we don't do
 

1047
00:26:25,059 --> 00:26:26,490
something internally that if we don't do
any we don't say anything it'll just

1048
00:26:26,490 --> 00:26:26,500
any we don't say anything it'll just
 

1049
00:26:26,500 --> 00:26:28,950
any we don't say anything it'll just
pick some scheme like hashing the keys

1050
00:26:28,950 --> 00:26:28,960
pick some scheme like hashing the keys
 

1051
00:26:28,960 --> 00:26:30,980
pick some scheme like hashing the keys
over the available workers for example

1052
00:26:30,980 --> 00:26:30,990
over the available workers for example
 

1053
00:26:30,990 --> 00:26:34,080
over the available workers for example
but you can tell it look you know I it

1054
00:26:34,080 --> 00:26:34,090
but you can tell it look you know I it
 

1055
00:26:34,090 --> 00:26:35,640
but you can tell it look you know I it
turns out that this particular way of

1056
00:26:35,640 --> 00:26:35,650
turns out that this particular way of
 

1057
00:26:35,650 --> 00:26:39,120
turns out that this particular way of
partitioning the data you know use a

1058
00:26:39,120 --> 00:26:39,130
partitioning the data you know use a
 

1059
00:26:39,130 --> 00:26:40,500
partitioning the data you know use a
different hash function or maybe

1060
00:26:40,500 --> 00:26:40,510
different hash function or maybe
 

1061
00:26:40,510 --> 00:26:42,480
different hash function or maybe
partitioned by ranges instead of hashing

1062
00:26:42,480 --> 00:26:42,490
partitioned by ranges instead of hashing
 

1063
00:26:42,490 --> 00:26:45,990
partitioned by ranges instead of hashing
you can tell it if you like more clever

1064
00:26:45,990 --> 00:26:46,000
you can tell it if you like more clever
 

1065
00:26:46,000 --> 00:26:53,940
you can tell it if you like more clever
ways to control the partitioning okay so

1066
00:26:53,940 --> 00:26:53,950
ways to control the partitioning okay so
 

1067
00:26:53,950 --> 00:26:55,160
ways to control the partitioning okay so
I'm about to start

1068
00:26:55,160 --> 00:26:55,170
I'm about to start
 

1069
00:26:55,170 --> 00:26:57,770
I'm about to start
the first thing the loop does and I hope

1070
00:26:57,770 --> 00:26:57,780
the first thing the loop does and I hope
 

1071
00:26:57,780 --> 00:27:02,330
the first thing the loop does and I hope
you can see the the code on line 12 we

1072
00:27:02,330 --> 00:27:02,340
you can see the the code on line 12 we
 

1073
00:27:02,340 --> 00:27:06,140
you can see the the code on line 12 we
actually gonna run this join this is the

1074
00:27:06,140 --> 00:27:06,150
actually gonna run this join this is the
 

1075
00:27:06,150 --> 00:27:08,750
actually gonna run this join this is the
first statement of the first iteration

1076
00:27:08,750 --> 00:27:08,760
first statement of the first iteration
 

1077
00:27:08,760 --> 00:27:12,890
first statement of the first iteration
of the loop with this joint is doing is

1078
00:27:12,890 --> 00:27:12,900
of the loop with this joint is doing is
 

1079
00:27:12,900 --> 00:27:17,140
of the loop with this joint is doing is
joining the links with the ranks and

1080
00:27:17,140 --> 00:27:17,150
joining the links with the ranks and
 

1081
00:27:17,150 --> 00:27:20,620
joining the links with the ranks and
what that does is pull together the

1082
00:27:20,620 --> 00:27:20,630
what that does is pull together the
 

1083
00:27:20,630 --> 00:27:22,700
what that does is pull together the
corresponding entries in the links which

1084
00:27:22,700 --> 00:27:22,710
corresponding entries in the links which
 

1085
00:27:22,710 --> 00:27:24,710
corresponding entries in the links which
said for every URL what is the point

1086
00:27:24,710 --> 00:27:24,720
said for every URL what is the point
 

1087
00:27:24,720 --> 00:27:28,040
said for every URL what is the point
what does it have links to and I'm sort

1088
00:27:28,040 --> 00:27:28,050
what does it have links to and I'm sort
 

1089
00:27:28,050 --> 00:27:29,810
what does it have links to and I'm sort
of putting together the links with the

1090
00:27:29,810 --> 00:27:29,820
of putting together the links with the
 

1091
00:27:29,820 --> 00:27:31,490
of putting together the links with the
ranks and but the rank says is for every

1092
00:27:31,490 --> 00:27:31,500
ranks and but the rank says is for every
 

1093
00:27:31,500 --> 00:27:33,710
ranks and but the rank says is for every
URL what's this current PageRank so now

1094
00:27:33,710 --> 00:27:33,720
URL what's this current PageRank so now
 

1095
00:27:33,720 --> 00:27:38,570
URL what's this current PageRank so now
we have together and a single item for

1096
00:27:38,570 --> 00:27:38,580
we have together and a single item for
 

1097
00:27:38,580 --> 00:27:39,620
we have together and a single item for
every page

1098
00:27:39,620 --> 00:27:39,630
every page
 

1099
00:27:39,630 --> 00:27:41,870
every page
both what its current PageRank is and

1100
00:27:41,870 --> 00:27:41,880
both what its current PageRank is and
 

1101
00:27:41,880 --> 00:27:43,640
both what its current PageRank is and
what links it points to because we're

1102
00:27:43,640 --> 00:27:43,650
what links it points to because we're
 

1103
00:27:43,650 --> 00:27:47,210
what links it points to because we're
gonna push every pages current PageRank

1104
00:27:47,210 --> 00:27:47,220
gonna push every pages current PageRank
 

1105
00:27:47,220 --> 00:27:50,390
gonna push every pages current PageRank
to all the pages it appoints to and

1106
00:27:50,390 --> 00:27:50,400
to all the pages it appoints to and
 

1107
00:27:50,400 --> 00:27:52,340
to all the pages it appoints to and
again this joint is uh is what the paper

1108
00:27:52,340 --> 00:27:52,350
again this joint is uh is what the paper
 

1109
00:27:52,350 --> 00:27:57,830
again this joint is uh is what the paper
calls a wide transformation because it

1110
00:27:57,830 --> 00:27:57,840
calls a wide transformation because it
 

1111
00:27:57,840 --> 00:28:04,100
calls a wide transformation because it
doesn't it's not a local the I mean it

1112
00:28:04,100 --> 00:28:04,110
doesn't it's not a local the I mean it
 

1113
00:28:04,110 --> 00:28:07,660
doesn't it's not a local the I mean it
needs to it may need to shuffle the data

1114
00:28:07,660 --> 00:28:07,670
needs to it may need to shuffle the data
 

1115
00:28:07,670 --> 00:28:10,610
needs to it may need to shuffle the data
by the URL key in order to bring

1116
00:28:10,610 --> 00:28:10,620
by the URL key in order to bring
 

1117
00:28:10,620 --> 00:28:13,220
by the URL key in order to bring
corresponding elements of links and

1118
00:28:13,220 --> 00:28:13,230
corresponding elements of links and
 

1119
00:28:13,230 --> 00:28:17,480
corresponding elements of links and
ranks together now in fact I believe

1120
00:28:17,480 --> 00:28:17,490
ranks together now in fact I believe
 

1121
00:28:17,490 --> 00:28:19,640
ranks together now in fact I believe
spark is clever enough to notice that

1122
00:28:19,640 --> 00:28:19,650
spark is clever enough to notice that
 

1123
00:28:19,650 --> 00:28:23,000
spark is clever enough to notice that
links and ranks are already partitioned

1124
00:28:23,000 --> 00:28:23,010
links and ranks are already partitioned
 

1125
00:28:23,010 --> 00:28:27,200
links and ranks are already partitioned
by key in the same way actually that

1126
00:28:27,200 --> 00:28:27,210
by key in the same way actually that
 

1127
00:28:27,210 --> 00:28:30,110
by key in the same way actually that
assumes that it cleverly created links

1128
00:28:30,110 --> 00:28:30,120
assumes that it cleverly created links
 

1129
00:28:30,120 --> 00:28:32,990
assumes that it cleverly created links
well when we created ranks its assumes

1130
00:28:32,990 --> 00:28:33,000
well when we created ranks its assumes
 

1131
00:28:33,000 --> 00:28:34,870
well when we created ranks its assumes
that it cleverly created

1132
00:28:34,870 --> 00:28:34,880
that it cleverly created
 

1133
00:28:34,880 --> 00:28:39,500
that it cleverly created
ranks using the same hash scheme as used

1134
00:28:39,500 --> 00:28:39,510
ranks using the same hash scheme as used
 

1135
00:28:39,510 --> 00:28:41,810
ranks using the same hash scheme as used
when it created links but if it was that

1136
00:28:41,810 --> 00:28:41,820
when it created links but if it was that
 

1137
00:28:41,820 --> 00:28:43,520
when it created links but if it was that
clever then it will notice that links

1138
00:28:43,520 --> 00:28:43,530
clever then it will notice that links
 

1139
00:28:43,530 --> 00:28:45,730
clever then it will notice that links
and ranks are passed in the same way

1140
00:28:45,730 --> 00:28:45,740
and ranks are passed in the same way
 

1141
00:28:45,740 --> 00:28:48,620
and ranks are passed in the same way
that is to say that the links ranks are

1142
00:28:48,620 --> 00:28:48,630
that is to say that the links ranks are
 

1143
00:28:48,630 --> 00:28:53,660
that is to say that the links ranks are
already on the same workers or sorry the

1144
00:28:53,660 --> 00:28:53,670
already on the same workers or sorry the
 

1145
00:28:53,670 --> 00:28:55,370
already on the same workers or sorry the
corresponding partitions with the same

1146
00:28:55,370 --> 00:28:55,380
corresponding partitions with the same
 

1147
00:28:55,380 --> 00:28:57,040
corresponding partitions with the same
keys are already in the same workers and

1148
00:28:57,040 --> 00:28:57,050
keys are already in the same workers and
 

1149
00:28:57,050 --> 00:29:00,110
keys are already in the same workers and
hopefully spark will notice that and not

1150
00:29:00,110 --> 00:29:00,120
hopefully spark will notice that and not
 

1151
00:29:00,120 --> 00:29:01,940
hopefully spark will notice that and not
have to move any data around if

1152
00:29:01,940 --> 00:29:01,950
have to move any data around if
 

1153
00:29:01,950 --> 00:29:03,200
have to move any data around if
something goes wrong though in links and

1154
00:29:03,200 --> 00:29:03,210
something goes wrong though in links and
 

1155
00:29:03,210 --> 00:29:04,310
something goes wrong though in links and
ranks are partitioned in different ways

1156
00:29:04,310 --> 00:29:04,320
ranks are partitioned in different ways
 

1157
00:29:04,320 --> 00:29:05,690
ranks are partitioned in different ways
then data will have to move at this

1158
00:29:05,690 --> 00:29:05,700
then data will have to move at this
 

1159
00:29:05,700 --> 00:29:06,780
then data will have to move at this
point

1160
00:29:06,780 --> 00:29:06,790
point
 

1161
00:29:06,790 --> 00:29:10,400
point
to join up corresponding keys in the two

1162
00:29:10,400 --> 00:29:10,410
to join up corresponding keys in the two
 

1163
00:29:10,410 --> 00:29:15,120
to join up corresponding keys in the two
and the two rdd's alright so JJ

1164
00:29:15,120 --> 00:29:15,130
and the two rdd's alright so JJ
 

1165
00:29:15,130 --> 00:29:17,970
and the two rdd's alright so JJ
contained now contains both every pages

1166
00:29:17,970 --> 00:29:17,980
contained now contains both every pages
 

1167
00:29:17,980 --> 00:29:25,500
contained now contains both every pages
rank and every pages list of links as

1168
00:29:25,500 --> 00:29:25,510
rank and every pages list of links as
 

1169
00:29:25,510 --> 00:29:28,650
rank and every pages list of links as
you can see now we have a even more

1170
00:29:28,650 --> 00:29:28,660
you can see now we have a even more
 

1171
00:29:28,660 --> 00:29:31,200
you can see now we have a even more
complex data structure it's an array

1172
00:29:31,200 --> 00:29:31,210
complex data structure it's an array
 

1173
00:29:31,210 --> 00:29:34,110
complex data structure it's an array
with an element per page with the pages

1174
00:29:34,110 --> 00:29:34,120
with an element per page with the pages
 

1175
00:29:34,120 --> 00:29:37,650
with an element per page with the pages
URL with a list of the links and the one

1176
00:29:37,650 --> 00:29:37,660
URL with a list of the links and the one
 

1177
00:29:37,660 --> 00:29:40,410
URL with a list of the links and the one
point over there is the page you choose

1178
00:29:40,410 --> 00:29:40,420
point over there is the page you choose
 

1179
00:29:40,420 --> 00:29:45,030
point over there is the page you choose
current rank and these are all all this

1180
00:29:45,030 --> 00:29:45,040
current rank and these are all all this
 

1181
00:29:45,040 --> 00:29:47,310
current rank and these are all all this
information is any sort of a single

1182
00:29:47,310 --> 00:29:47,320
information is any sort of a single
 

1183
00:29:47,320 --> 00:29:48,870
information is any sort of a single
record that has all this information for

1184
00:29:48,870 --> 00:29:48,880
record that has all this information for
 

1185
00:29:48,880 --> 00:29:52,280
record that has all this information for
each page together where we need it

1186
00:29:52,280 --> 00:29:52,290
each page together where we need it
 

1187
00:29:52,290 --> 00:29:56,070
each page together where we need it
alright the next step is that we're

1188
00:29:56,070 --> 00:29:56,080
alright the next step is that we're
 

1189
00:29:56,080 --> 00:29:58,110
alright the next step is that we're
gonna figure out every page is gonna

1190
00:29:58,110 --> 00:29:58,120
gonna figure out every page is gonna
 

1191
00:29:58,120 --> 00:30:02,190
gonna figure out every page is gonna
push a fraction of its current page rank

1192
00:30:02,190 --> 00:30:02,200
push a fraction of its current page rank
 

1193
00:30:02,200 --> 00:30:04,020
push a fraction of its current page rank
to all the pages that it links to it's

1194
00:30:04,020 --> 00:30:04,030
to all the pages that it links to it's
 

1195
00:30:04,030 --> 00:30:05,430
to all the pages that it links to it's
kind of sort of divided up its current

1196
00:30:05,430 --> 00:30:05,440
kind of sort of divided up its current
 

1197
00:30:05,440 --> 00:30:07,230
kind of sort of divided up its current
page rank among all the pages it links

1198
00:30:07,230 --> 00:30:07,240
page rank among all the pages it links
 

1199
00:30:07,240 --> 00:30:11,150
page rank among all the pages it links
to

1200
00:30:11,150 --> 00:30:11,160

 

1201
00:30:11,160 --> 00:30:16,430

and that's what this contribs does you

1202
00:30:16,430 --> 00:30:16,440
and that's what this contribs does you
 

1203
00:30:16,440 --> 00:30:18,410
and that's what this contribs does you
know basically what's going on is that

1204
00:30:18,410 --> 00:30:18,420
know basically what's going on is that
 

1205
00:30:18,420 --> 00:30:23,960
know basically what's going on is that
it's a one another one call to map and

1206
00:30:23,960 --> 00:30:23,970
it's a one another one call to map and
 

1207
00:30:23,970 --> 00:30:27,050
it's a one another one call to map and
we're mapping over the for each page

1208
00:30:27,050 --> 00:30:27,060
we're mapping over the for each page
 

1209
00:30:27,060 --> 00:30:29,960
we're mapping over the for each page
were running map over the URLs that that

1210
00:30:29,960 --> 00:30:29,970
were running map over the URLs that that
 

1211
00:30:29,970 --> 00:30:32,210
were running map over the URLs that that
pages points to and for each page it

1212
00:30:32,210 --> 00:30:32,220
pages points to and for each page it
 

1213
00:30:32,220 --> 00:30:37,370
pages points to and for each page it
points to we're just calculating this

1214
00:30:37,370 --> 00:30:37,380
points to we're just calculating this
 

1215
00:30:37,380 --> 00:30:39,590
points to we're just calculating this
number which is the from pages current

1216
00:30:39,590 --> 00:30:39,600
number which is the from pages current
 

1217
00:30:39,600 --> 00:30:41,540
number which is the from pages current
rank divided by the total number of

1218
00:30:41,540 --> 00:30:41,550
rank divided by the total number of
 

1219
00:30:41,550 --> 00:30:44,120
rank divided by the total number of
pages that points to so this sort of

1220
00:30:44,120 --> 00:30:44,130
pages that points to so this sort of
 

1221
00:30:44,130 --> 00:30:47,030
pages that points to so this sort of
figured you know creates a mapping from

1222
00:30:47,030 --> 00:30:47,040
figured you know creates a mapping from
 

1223
00:30:47,040 --> 00:30:50,390
figured you know creates a mapping from
link name to one of the many

1224
00:30:50,390 --> 00:30:50,400
link name to one of the many
 

1225
00:30:50,400 --> 00:30:55,280
link name to one of the many
contributions to that pages new page

1226
00:30:55,280 --> 00:30:55,290
contributions to that pages new page
 

1227
00:30:55,290 --> 00:31:04,040
contributions to that pages new page
rank and we can sneak peek it what this

1228
00:31:04,040 --> 00:31:04,050
rank and we can sneak peek it what this
 

1229
00:31:04,050 --> 00:31:07,460
rank and we can sneak peek it what this
is gonna produce I think is a much

1230
00:31:07,460 --> 00:31:07,470
is gonna produce I think is a much
 

1231
00:31:07,470 --> 00:31:10,010
is gonna produce I think is a much
simpler thing it just as a list of URLs

1232
00:31:10,010 --> 00:31:10,020
simpler thing it just as a list of URLs
 

1233
00:31:10,020 --> 00:31:13,730
simpler thing it just as a list of URLs
and contributions to the URLs page ranks

1234
00:31:13,730 --> 00:31:13,740
and contributions to the URLs page ranks
 

1235
00:31:13,740 --> 00:31:15,500
and contributions to the URLs page ranks
and there's there's more there's you

1236
00:31:15,500 --> 00:31:15,510
and there's there's more there's you
 

1237
00:31:15,510 --> 00:31:16,940
and there's there's more there's you
know more than one record for each URL

1238
00:31:16,940 --> 00:31:16,950
know more than one record for each URL
 

1239
00:31:16,950 --> 00:31:19,730
know more than one record for each URL
here because there's gonna for any given

1240
00:31:19,730 --> 00:31:19,740
here because there's gonna for any given
 

1241
00:31:19,740 --> 00:31:21,290
here because there's gonna for any given
page there's gonna be a record here for

1242
00:31:21,290 --> 00:31:21,300
page there's gonna be a record here for
 

1243
00:31:21,300 --> 00:31:22,790
page there's gonna be a record here for
every single link that points to it

1244
00:31:22,790 --> 00:31:22,800
every single link that points to it
 

1245
00:31:22,800 --> 00:31:27,050
every single link that points to it
indicating this contribution of from

1246
00:31:27,050 --> 00:31:27,060
indicating this contribution of from
 

1247
00:31:27,060 --> 00:31:29,390
indicating this contribution of from
whatever that link came from to this

1248
00:31:29,390 --> 00:31:29,400
whatever that link came from to this
 

1249
00:31:29,400 --> 00:31:32,650
whatever that link came from to this
page to this pages new updated PageRank

1250
00:31:32,650 --> 00:31:32,660
page to this pages new updated PageRank
 

1251
00:31:32,660 --> 00:31:35,120
page to this pages new updated PageRank
what has to happen now is that we need

1252
00:31:35,120 --> 00:31:35,130
what has to happen now is that we need
 

1253
00:31:35,130 --> 00:31:38,720
what has to happen now is that we need
to sum up for every page we need to sum

1254
00:31:38,720 --> 00:31:38,730
to sum up for every page we need to sum
 

1255
00:31:38,730 --> 00:31:42,410
to sum up for every page we need to sum
up the PageRank contributions for that

1256
00:31:42,410 --> 00:31:42,420
up the PageRank contributions for that
 

1257
00:31:42,420 --> 00:31:44,210
up the PageRank contributions for that
page that are in contribs so again we

1258
00:31:44,210 --> 00:31:44,220
page that are in contribs so again we
 

1259
00:31:44,220 --> 00:31:46,460
page that are in contribs so again we
going to need to do a shuffle here it's

1260
00:31:46,460 --> 00:31:46,470
going to need to do a shuffle here it's
 

1261
00:31:46,470 --> 00:31:49,460
going to need to do a shuffle here it's
gonna be a wide a transformation with a

1262
00:31:49,460 --> 00:31:49,470
gonna be a wide a transformation with a
 

1263
00:31:49,470 --> 00:31:50,840
gonna be a wide a transformation with a
wide input because we need to bring

1264
00:31:50,840 --> 00:31:50,850
wide input because we need to bring
 

1265
00:31:50,850 --> 00:31:55,060
wide input because we need to bring
together all of the elements of contribs

1266
00:31:55,060 --> 00:31:55,070
together all of the elements of contribs
 

1267
00:31:55,070 --> 00:31:57,410
together all of the elements of contribs
for each page we need to bring together

1268
00:31:57,410 --> 00:31:57,420
for each page we need to bring together
 

1269
00:31:57,420 --> 00:31:59,270
for each page we need to bring together
and to the same worker to the same

1270
00:31:59,270 --> 00:31:59,280
and to the same worker to the same
 

1271
00:31:59,280 --> 00:32:03,520
and to the same worker to the same
partition so they can all be summed up

1272
00:32:03,520 --> 00:32:03,530

 

1273
00:32:03,530 --> 00:32:07,610

and the way that's done the bay PageRank

1274
00:32:07,610 --> 00:32:07,620
and the way that's done the bay PageRank
 

1275
00:32:07,620 --> 00:32:10,460
and the way that's done the bay PageRank
does that is with this reduced by key

1276
00:32:10,460 --> 00:32:10,470
does that is with this reduced by key
 

1277
00:32:10,470 --> 00:32:15,910
does that is with this reduced by key
call would reduce spike he does is

1278
00:32:15,910 --> 00:32:15,920
call would reduce spike he does is
 

1279
00:32:15,920 --> 00:32:17,470
call would reduce spike he does is
it first of all it brings together all

1280
00:32:17,470 --> 00:32:17,480
it first of all it brings together all
 

1281
00:32:17,480 --> 00:32:19,980
it first of all it brings together all
the records with the same key and then

1282
00:32:19,980 --> 00:32:19,990
the records with the same key and then
 

1283
00:32:19,990 --> 00:32:24,340
the records with the same key and then
sums up the second element of each one

1284
00:32:24,340 --> 00:32:24,350
sums up the second element of each one
 

1285
00:32:24,350 --> 00:32:26,860
sums up the second element of each one
of those records for a given key and

1286
00:32:26,860 --> 00:32:26,870
of those records for a given key and
 

1287
00:32:26,870 --> 00:32:30,310
of those records for a given key and
produces as output the key which is a

1288
00:32:30,310 --> 00:32:30,320
produces as output the key which is a
 

1289
00:32:30,320 --> 00:32:33,730
produces as output the key which is a
URL and the sum of the numbers which is

1290
00:32:33,730 --> 00:32:33,740
URL and the sum of the numbers which is
 

1291
00:32:33,740 --> 00:32:39,010
URL and the sum of the numbers which is
the updated PageRank there's actually

1292
00:32:39,010 --> 00:32:39,020
the updated PageRank there's actually
 

1293
00:32:39,020 --> 00:32:40,690
the updated PageRank there's actually
two transformations here the first ones

1294
00:32:40,690 --> 00:32:40,700
two transformations here the first ones
 

1295
00:32:40,700 --> 00:32:43,090
two transformations here the first ones
is reduced by key and the second is this

1296
00:32:43,090 --> 00:32:43,100
is reduced by key and the second is this
 

1297
00:32:43,100 --> 00:32:46,390
is reduced by key and the second is this
map values which and and this is the

1298
00:32:46,390 --> 00:32:46,400
map values which and and this is the
 

1299
00:32:46,400 --> 00:32:49,630
map values which and and this is the
part that implements the 15% probability

1300
00:32:49,630 --> 00:32:49,640
part that implements the 15% probability
 

1301
00:32:49,640 --> 00:32:52,240
part that implements the 15% probability
of going to a random page and the 85%

1302
00:32:52,240 --> 00:32:52,250
of going to a random page and the 85%
 

1303
00:32:52,250 --> 00:33:00,210
of going to a random page and the 85%
chance of following a link all right

1304
00:33:00,210 --> 00:33:00,220
chance of following a link all right
 

1305
00:33:00,220 --> 00:33:02,350
chance of following a link all right
let's look at ranks by the way even

1306
00:33:02,350 --> 00:33:02,360
let's look at ranks by the way even
 

1307
00:33:02,360 --> 00:33:04,450
let's look at ranks by the way even
though we've assigned two ranks here um

1308
00:33:04,450 --> 00:33:04,460
though we've assigned two ranks here um
 

1309
00:33:04,460 --> 00:33:06,580
though we've assigned two ranks here um
what this is going to end up doing is

1310
00:33:06,580 --> 00:33:06,590
what this is going to end up doing is
 

1311
00:33:06,590 --> 00:33:08,740
what this is going to end up doing is
creating an entirely new transformation

1312
00:33:08,740 --> 00:33:08,750
creating an entirely new transformation
 

1313
00:33:08,750 --> 00:33:12,070
creating an entirely new transformation
I'm so not it's not changing the value

1314
00:33:12,070 --> 00:33:12,080
I'm so not it's not changing the value
 

1315
00:33:12,080 --> 00:33:14,770
I'm so not it's not changing the value
is already computed or when it comes to

1316
00:33:14,770 --> 00:33:14,780
is already computed or when it comes to
 

1317
00:33:14,780 --> 00:33:16,060
is already computed or when it comes to
executing this it won't change any

1318
00:33:16,060 --> 00:33:16,070
executing this it won't change any
 

1319
00:33:16,070 --> 00:33:17,650
executing this it won't change any
values are already computed it just

1320
00:33:17,650 --> 00:33:17,660
values are already computed it just
 

1321
00:33:17,660 --> 00:33:20,830
values are already computed it just
creates a new a new transformation with

1322
00:33:20,830 --> 00:33:20,840
creates a new a new transformation with
 

1323
00:33:20,840 --> 00:33:27,490
creates a new a new transformation with
new output and we can see what's gonna

1324
00:33:27,490 --> 00:33:27,500
new output and we can see what's gonna
 

1325
00:33:27,500 --> 00:33:29,830
new output and we can see what's gonna
happen in indeed we now have member

1326
00:33:29,830 --> 00:33:29,840
happen in indeed we now have member
 

1327
00:33:29,840 --> 00:33:32,380
happen in indeed we now have member
ranks originally was just a bunch of

1328
00:33:32,380 --> 00:33:32,390
ranks originally was just a bunch of
 

1329
00:33:32,390 --> 00:33:35,920
ranks originally was just a bunch of
pairs of URL PageRank now again we

1330
00:33:35,920 --> 00:33:35,930
pairs of URL PageRank now again we
 

1331
00:33:35,930 --> 00:33:37,180
pairs of URL PageRank now again we
appears if you are I'll page rank

1332
00:33:37,180 --> 00:33:37,190
appears if you are I'll page rank
 

1333
00:33:37,190 --> 00:33:38,530
appears if you are I'll page rank
another different we'd actually updated

1334
00:33:38,530 --> 00:33:38,540
another different we'd actually updated
 

1335
00:33:38,540 --> 00:33:43,560
another different we'd actually updated
them sort of changed them by one step

1336
00:33:43,560 --> 00:33:43,570

 

1337
00:33:43,570 --> 00:33:45,690

and I don't know if you remember the

1338
00:33:45,690 --> 00:33:45,700
and I don't know if you remember the
 

1339
00:33:45,700 --> 00:33:48,640
and I don't know if you remember the
original PageRank values we saw but

1340
00:33:48,640 --> 00:33:48,650
original PageRank values we saw but
 

1341
00:33:48,650 --> 00:33:51,550
original PageRank values we saw but
these are closer to those final output

1342
00:33:51,550 --> 00:33:51,560
these are closer to those final output
 

1343
00:33:51,560 --> 00:33:54,340
these are closer to those final output
that we saw then the original values of

1344
00:33:54,340 --> 00:33:54,350
that we saw then the original values of
 

1345
00:33:54,350 --> 00:33:58,270
that we saw then the original values of
all one are okay so that was one

1346
00:33:58,270 --> 00:33:58,280
all one are okay so that was one
 

1347
00:33:58,280 --> 00:34:01,180
all one are okay so that was one
iteration of the algorithm when the loop

1348
00:34:01,180 --> 00:34:01,190
iteration of the algorithm when the loop
 

1349
00:34:01,190 --> 00:34:02,770
iteration of the algorithm when the loop
goes back up to the top it's gonna do

1350
00:34:02,770 --> 00:34:02,780
goes back up to the top it's gonna do
 

1351
00:34:02,780 --> 00:34:08,220
goes back up to the top it's gonna do
the same join flat map and reduce by key

1352
00:34:08,220 --> 00:34:08,230
the same join flat map and reduce by key
 

1353
00:34:08,230 --> 00:34:13,330
the same join flat map and reduce by key
and each time it's again you know what

1354
00:34:13,330 --> 00:34:13,340
and each time it's again you know what
 

1355
00:34:13,340 --> 00:34:15,100
and each time it's again you know what
the loop is actually doing is producing

1356
00:34:15,100 --> 00:34:15,110
the loop is actually doing is producing
 

1357
00:34:15,110 --> 00:34:18,610
the loop is actually doing is producing
this lineage graph and so it's not

1358
00:34:18,610 --> 00:34:18,620
this lineage graph and so it's not
 

1359
00:34:18,620 --> 00:34:20,320
this lineage graph and so it's not
updating the variables that are

1360
00:34:20,320 --> 00:34:20,330
updating the variables that are
 

1361
00:34:20,330 --> 00:34:21,640
updating the variables that are
mentioned in the loop it's really

1362
00:34:21,640 --> 00:34:21,650
mentioned in the loop it's really
 

1363
00:34:21,650 --> 00:34:25,200
mentioned in the loop it's really
creating essentially appending new

1364
00:34:25,200 --> 00:34:25,210
creating essentially appending new
 

1365
00:34:25,210 --> 00:34:27,730
creating essentially appending new
transformation nodes to the lineage

1366
00:34:27,730 --> 00:34:27,740
transformation nodes to the lineage
 

1367
00:34:27,740 --> 00:34:30,389
transformation nodes to the lineage
graph that it's building

1368
00:34:30,389 --> 00:34:30,399
graph that it's building
 

1369
00:34:30,399 --> 00:34:34,270
graph that it's building
but I've only run that Elite once after

1370
00:34:34,270 --> 00:34:34,280
but I've only run that Elite once after
 

1371
00:34:34,280 --> 00:34:37,510
but I've only run that Elite once after
the loop and then now this is what the

1372
00:34:37,510 --> 00:34:37,520
the loop and then now this is what the
 

1373
00:34:37,520 --> 00:34:39,280
the loop and then now this is what the
real code does the real code actually

1374
00:34:39,280 --> 00:34:39,290
real code does the real code actually
 

1375
00:34:39,290 --> 00:34:42,399
real code does the real code actually
runs collect at this point and so they

1376
00:34:42,399 --> 00:34:42,409
runs collect at this point and so they
 

1377
00:34:42,409 --> 00:34:43,780
runs collect at this point and so they
were in the real PageRank implementation

1378
00:34:43,780 --> 00:34:43,790
were in the real PageRank implementation
 

1379
00:34:43,790 --> 00:34:46,659
were in the real PageRank implementation
only at this point with the computation

1380
00:34:46,659 --> 00:34:46,669
only at this point with the computation
 

1381
00:34:46,669 --> 00:34:49,119
only at this point with the computation
even start because of the call to

1382
00:34:49,119 --> 00:34:49,129
even start because of the call to
 

1383
00:34:49,129 --> 00:34:50,470
even start because of the call to
collect here and I go off and read the

1384
00:34:50,470 --> 00:34:50,480
collect here and I go off and read the
 

1385
00:34:50,480 --> 00:34:52,540
collect here and I go off and read the
end burden we're on the input through

1386
00:34:52,540 --> 00:34:52,550
end burden we're on the input through
 

1387
00:34:52,550 --> 00:34:54,899
end burden we're on the input through
all these transformations and shuffles

1388
00:34:54,899 --> 00:34:54,909
all these transformations and shuffles
 

1389
00:34:54,909 --> 00:34:57,880
all these transformations and shuffles
for the wide dependencies and finally

1390
00:34:57,880 --> 00:34:57,890
for the wide dependencies and finally
 

1391
00:34:57,890 --> 00:34:59,710
for the wide dependencies and finally
collect the output together on the

1392
00:34:59,710 --> 00:34:59,720
collect the output together on the
 

1393
00:34:59,720 --> 00:35:02,410
collect the output together on the
computer that's running this program by

1394
00:35:02,410 --> 00:35:02,420
computer that's running this program by
 

1395
00:35:02,420 --> 00:35:03,520
computer that's running this program by
the way the computer that runs the

1396
00:35:03,520 --> 00:35:03,530
the way the computer that runs the
 

1397
00:35:03,530 --> 00:35:05,380
the way the computer that runs the
program that the paper calls it the

1398
00:35:05,380 --> 00:35:05,390
program that the paper calls it the
 

1399
00:35:05,390 --> 00:35:07,930
program that the paper calls it the
driver the driver computer is the one

1400
00:35:07,930 --> 00:35:07,940
driver the driver computer is the one
 

1401
00:35:07,940 --> 00:35:09,640
driver the driver computer is the one
that actually runs this scallop program

1402
00:35:09,640 --> 00:35:09,650
that actually runs this scallop program
 

1403
00:35:09,650 --> 00:35:13,210
that actually runs this scallop program
that's kind of driving the spark

1404
00:35:13,210 --> 00:35:13,220
that's kind of driving the spark
 

1405
00:35:13,220 --> 00:35:15,940
that's kind of driving the spark
computation and then the program takes

1406
00:35:15,940 --> 00:35:15,950
computation and then the program takes
 

1407
00:35:15,950 --> 00:35:18,849
computation and then the program takes
this output variable and runs it through

1408
00:35:18,849 --> 00:35:18,859
this output variable and runs it through
 

1409
00:35:18,859 --> 00:35:26,650
this output variable and runs it through
a nice nicely formatted print on each of

1410
00:35:26,650 --> 00:35:26,660
a nice nicely formatted print on each of
 

1411
00:35:26,660 --> 00:35:35,200
a nice nicely formatted print on each of
the records in the collect up okay so

1412
00:35:35,200 --> 00:35:35,210
the records in the collect up okay so
 

1413
00:35:35,210 --> 00:35:39,130
the records in the collect up okay so
that's the

1414
00:35:39,130 --> 00:35:39,140

 

1415
00:35:39,140 --> 00:35:41,200

kind of style of programming that people

1416
00:35:41,200 --> 00:35:41,210
kind of style of programming that people
 

1417
00:35:41,210 --> 00:35:51,900
kind of style of programming that people
use for Scala and I mean for for spark

1418
00:35:51,900 --> 00:35:51,910

 

1419
00:35:51,910 --> 00:35:54,279

went one thing to note here relative to

1420
00:35:54,279 --> 00:35:54,289
went one thing to note here relative to
 

1421
00:35:54,289 --> 00:35:57,910
went one thing to note here relative to
MapReduce is that this program well you

1422
00:35:57,910 --> 00:35:57,920
MapReduce is that this program well you
 

1423
00:35:57,920 --> 00:35:59,620
MapReduce is that this program well you
know and look looks a little bit complex

1424
00:35:59,620 --> 00:35:59,630
know and look looks a little bit complex
 

1425
00:35:59,630 --> 00:36:02,710
know and look looks a little bit complex
but the fact is that this program is

1426
00:36:02,710 --> 00:36:02,720
but the fact is that this program is
 

1427
00:36:02,720 --> 00:36:07,620
but the fact is that this program is
doing the work of many many MapReduce or

1428
00:36:07,620 --> 00:36:07,630
doing the work of many many MapReduce or
 

1429
00:36:07,630 --> 00:36:09,670
doing the work of many many MapReduce or
doing an amount of work that would

1430
00:36:09,670 --> 00:36:09,680
doing an amount of work that would
 

1431
00:36:09,680 --> 00:36:12,039
doing an amount of work that would
require many separate MapReduce programs

1432
00:36:12,039 --> 00:36:12,049
require many separate MapReduce programs
 

1433
00:36:12,049 --> 00:36:16,690
require many separate MapReduce programs
in order to implement so you know it's

1434
00:36:16,690 --> 00:36:16,700
in order to implement so you know it's
 

1435
00:36:16,700 --> 00:36:18,849
in order to implement so you know it's
21 lines and maybe you used two

1436
00:36:18,849 --> 00:36:18,859
21 lines and maybe you used two
 

1437
00:36:18,859 --> 00:36:20,230
21 lines and maybe you used two
MapReduce programs that are simpler than

1438
00:36:20,230 --> 00:36:20,240
MapReduce programs that are simpler than
 

1439
00:36:20,240 --> 00:36:22,569
MapReduce programs that are simpler than
that but this is doing a lot of work for

1440
00:36:22,569 --> 00:36:22,579
that but this is doing a lot of work for
 

1441
00:36:22,579 --> 00:36:25,329
that but this is doing a lot of work for
21 lines and it turns out that this is

1442
00:36:25,329 --> 00:36:25,339
21 lines and it turns out that this is
 

1443
00:36:25,339 --> 00:36:27,099
21 lines and it turns out that this is
you know this is sort of a real

1444
00:36:27,099 --> 00:36:27,109
you know this is sort of a real
 

1445
00:36:27,109 --> 00:36:29,440
you know this is sort of a real
algorithm to so it's like a pretty

1446
00:36:29,440 --> 00:36:29,450
algorithm to so it's like a pretty
 

1447
00:36:29,450 --> 00:36:32,049
algorithm to so it's like a pretty
concise and easy program easy to program

1448
00:36:32,049 --> 00:36:32,059
concise and easy program easy to program
 

1449
00:36:32,059 --> 00:36:37,059
concise and easy program easy to program
way to express vast Big Data

1450
00:36:37,059 --> 00:36:37,069
way to express vast Big Data
 

1451
00:36:37,069 --> 00:36:42,759
way to express vast Big Data
computations you know people like pretty

1452
00:36:42,759 --> 00:36:42,769
computations you know people like pretty
 

1453
00:36:42,769 --> 00:36:50,760
computations you know people like pretty
successful okay so again

1454
00:36:50,760 --> 00:36:50,770
successful okay so again
 

1455
00:36:50,770 --> 00:36:52,680
successful okay so again
just want to repeat that until the final

1456
00:36:52,680 --> 00:36:52,690
just want to repeat that until the final
 

1457
00:36:52,690 --> 00:36:54,690
just want to repeat that until the final
collect or this code is doing is

1458
00:36:54,690 --> 00:36:54,700
collect or this code is doing is
 

1459
00:36:54,700 --> 00:36:56,730
collect or this code is doing is
generating a lineage graph and not

1460
00:36:56,730 --> 00:36:56,740
generating a lineage graph and not
 

1461
00:36:56,740 --> 00:36:58,590
generating a lineage graph and not
processing the data and the the lineage

1462
00:36:58,590 --> 00:36:58,600
processing the data and the the lineage
 

1463
00:36:58,600 --> 00:37:01,380
processing the data and the the lineage
graph that it produces actually the

1464
00:37:01,380 --> 00:37:01,390
graph that it produces actually the
 

1465
00:37:01,390 --> 00:37:01,680
graph that it produces actually the
paper

1466
00:37:01,680 --> 00:37:01,690
paper
 

1467
00:37:01,690 --> 00:37:05,010
paper
I'm just copied this from the paper this

1468
00:37:05,010 --> 00:37:05,020
I'm just copied this from the paper this
 

1469
00:37:05,020 --> 00:37:06,270
I'm just copied this from the paper this
is what the lineage graph looks like

1470
00:37:06,270 --> 00:37:06,280
is what the lineage graph looks like
 

1471
00:37:06,280 --> 00:37:09,690
is what the lineage graph looks like
it's you know this is all that the

1472
00:37:09,690 --> 00:37:09,700
it's you know this is all that the
 

1473
00:37:09,700 --> 00:37:11,730
it's you know this is all that the
program is producing it's just this

1474
00:37:11,730 --> 00:37:11,740
program is producing it's just this
 

1475
00:37:11,740 --> 00:37:14,640
program is producing it's just this
graph until the final collect and you

1476
00:37:14,640 --> 00:37:14,650
graph until the final collect and you
 

1477
00:37:14,650 --> 00:37:16,200
graph until the final collect and you
can see that it's a sequence of these

1478
00:37:16,200 --> 00:37:16,210
can see that it's a sequence of these
 

1479
00:37:16,210 --> 00:37:20,340
can see that it's a sequence of these
processing stage where we read the file

1480
00:37:20,340 --> 00:37:20,350
processing stage where we read the file
 

1481
00:37:20,350 --> 00:37:21,570
processing stage where we read the file
to produce links and then completely

1482
00:37:21,570 --> 00:37:21,580
to produce links and then completely
 

1483
00:37:21,580 --> 00:37:23,040
to produce links and then completely
separately we produce these initial

1484
00:37:23,040 --> 00:37:23,050
separately we produce these initial
 

1485
00:37:23,050 --> 00:37:26,670
separately we produce these initial
ranks and then there's repeated joins

1486
00:37:26,670 --> 00:37:26,680
ranks and then there's repeated joins
 

1487
00:37:26,680 --> 00:37:34,290
ranks and then there's repeated joins
and reduced by key pairs each loop

1488
00:37:34,290 --> 00:37:34,300
and reduced by key pairs each loop
 

1489
00:37:34,300 --> 00:37:41,160
and reduced by key pairs each loop
iteration produces a join and a each of

1490
00:37:41,160 --> 00:37:41,170
iteration produces a join and a each of
 

1491
00:37:41,170 --> 00:37:42,690
iteration produces a join and a each of
these pairs is one loop iteration and

1492
00:37:42,690 --> 00:37:42,700
these pairs is one loop iteration and
 

1493
00:37:42,700 --> 00:37:44,220
these pairs is one loop iteration and
you can see again that the loop is

1494
00:37:44,220 --> 00:37:44,230
you can see again that the loop is
 

1495
00:37:44,230 --> 00:37:46,500
you can see again that the loop is
appended more and more nodes to the

1496
00:37:46,500 --> 00:37:46,510
appended more and more nodes to the
 

1497
00:37:46,510 --> 00:37:49,410
appended more and more nodes to the
graph rather than what it is not doing

1498
00:37:49,410 --> 00:37:49,420
graph rather than what it is not doing
 

1499
00:37:49,420 --> 00:37:51,950
graph rather than what it is not doing
in particular it is not producing a

1500
00:37:51,950 --> 00:37:51,960
in particular it is not producing a
 

1501
00:37:51,960 --> 00:37:56,790
in particular it is not producing a
cyclic graph the loop is producing all

1502
00:37:56,790 --> 00:37:56,800
cyclic graph the loop is producing all
 

1503
00:37:56,800 --> 00:37:59,460
cyclic graph the loop is producing all
these graphs are a cyclic another thing

1504
00:37:59,460 --> 00:37:59,470
these graphs are a cyclic another thing
 

1505
00:37:59,470 --> 00:38:00,990
these graphs are a cyclic another thing
to notice that you wouldn't have seen a

1506
00:38:00,990 --> 00:38:01,000
to notice that you wouldn't have seen a
 

1507
00:38:01,000 --> 00:38:03,690
to notice that you wouldn't have seen a
MapReduce is that this data here which

1508
00:38:03,690 --> 00:38:03,700
MapReduce is that this data here which
 

1509
00:38:03,700 --> 00:38:05,190
MapReduce is that this data here which
was the data that we cashed that we

1510
00:38:05,190 --> 00:38:05,200
was the data that we cashed that we
 

1511
00:38:05,200 --> 00:38:07,320
was the data that we cashed that we
persisted is used over and over again

1512
00:38:07,320 --> 00:38:07,330
persisted is used over and over again
 

1513
00:38:07,330 --> 00:38:09,480
persisted is used over and over again
and every loop iteration and so it

1514
00:38:09,480 --> 00:38:09,490
and every loop iteration and so it
 

1515
00:38:09,490 --> 00:38:12,350
and every loop iteration and so it
sparks going to keep this in memory and

1516
00:38:12,350 --> 00:38:12,360
sparks going to keep this in memory and
 

1517
00:38:12,360 --> 00:38:20,060
sparks going to keep this in memory and
it's going to consult it multiple times

1518
00:38:20,060 --> 00:38:20,070

 

1519
00:38:20,070 --> 00:38:26,370

alright so it actually happens during

1520
00:38:26,370 --> 00:38:26,380
alright so it actually happens during
 

1521
00:38:26,380 --> 00:38:28,560
alright so it actually happens during
execution what is the execution look

1522
00:38:28,560 --> 00:38:28,570
execution what is the execution look
 

1523
00:38:28,570 --> 00:38:36,180
execution what is the execution look
like so again the the assumption is that

1524
00:38:36,180 --> 00:38:36,190
like so again the the assumption is that
 

1525
00:38:36,190 --> 00:38:39,030
like so again the the assumption is that
the data the input data starts out kind

1526
00:38:39,030 --> 00:38:39,040
the data the input data starts out kind
 

1527
00:38:39,040 --> 00:38:45,530
the data the input data starts out kind
of pre partitioned by over in HDFS

1528
00:38:45,530 --> 00:38:45,540
of pre partitioned by over in HDFS
 

1529
00:38:45,540 --> 00:38:48,450
of pre partitioned by over in HDFS
we assume our one file it's our input

1530
00:38:48,450 --> 00:38:48,460
we assume our one file it's our input
 

1531
00:38:48,460 --> 00:38:51,090
we assume our one file it's our input
files already split up into lots of you

1532
00:38:51,090 --> 00:38:51,100
files already split up into lots of you
 

1533
00:38:51,100 --> 00:38:53,460
files already split up into lots of you
know 64 megabyte or whatever it may

1534
00:38:53,460 --> 00:38:53,470
know 64 megabyte or whatever it may
 

1535
00:38:53,470 --> 00:38:58,550
know 64 megabyte or whatever it may
happen pieces in HDFS spark knows that

1536
00:38:58,550 --> 00:38:58,560
happen pieces in HDFS spark knows that
 

1537
00:38:58,560 --> 00:39:01,020
happen pieces in HDFS spark knows that
when you started you actually call

1538
00:39:01,020 --> 00:39:01,030
when you started you actually call
 

1539
00:39:01,030 --> 00:39:02,520
when you started you actually call
collect the start of computation spark

1540
00:39:02,520 --> 00:39:02,530
collect the start of computation spark
 

1541
00:39:02,530 --> 00:39:03,750
collect the start of computation spark
knows that the input data is already

1542
00:39:03,750 --> 00:39:03,760
knows that the input data is already
 

1543
00:39:03,760 --> 00:39:08,060
knows that the input data is already
partitioned HDFS and it's gonna try to

1544
00:39:08,060 --> 00:39:08,070
partitioned HDFS and it's gonna try to
 

1545
00:39:08,070 --> 00:39:11,430
partitioned HDFS and it's gonna try to
split up the work the workers in a

1546
00:39:11,430 --> 00:39:11,440
split up the work the workers in a
 

1547
00:39:11,440 --> 00:39:13,830
split up the work the workers in a
corresponding way so if it knows that

1548
00:39:13,830 --> 00:39:13,840
corresponding way so if it knows that
 

1549
00:39:13,840 --> 00:39:15,930
corresponding way so if it knows that
there's I actually don't know what the

1550
00:39:15,930 --> 00:39:15,940
there's I actually don't know what the
 

1551
00:39:15,940 --> 00:39:19,980
there's I actually don't know what the
details are a bit it might actually try

1552
00:39:19,980 --> 00:39:19,990
details are a bit it might actually try
 

1553
00:39:19,990 --> 00:39:21,330
details are a bit it might actually try
to run the computation on the same

1554
00:39:21,330 --> 00:39:21,340
to run the computation on the same
 

1555
00:39:21,340 --> 00:39:25,410
to run the computation on the same
machines that store the HDFS data or it

1556
00:39:25,410 --> 00:39:25,420
machines that store the HDFS data or it
 

1557
00:39:25,420 --> 00:39:31,640
machines that store the HDFS data or it
may just set up a bunch of workers to

1558
00:39:31,640 --> 00:39:31,650
may just set up a bunch of workers to
 

1559
00:39:31,650 --> 00:39:35,130
may just set up a bunch of workers to
read each of the HDFS partitions and

1560
00:39:35,130 --> 00:39:35,140
read each of the HDFS partitions and
 

1561
00:39:35,140 --> 00:39:37,320
read each of the HDFS partitions and
again there's likely to be more than one

1562
00:39:37,320 --> 00:39:37,330
again there's likely to be more than one
 

1563
00:39:37,330 --> 00:39:41,730
again there's likely to be more than one
partition per per worker so we have the

1564
00:39:41,730 --> 00:39:41,740
partition per per worker so we have the
 

1565
00:39:41,740 --> 00:39:45,570
partition per per worker so we have the
input file and the very first thing is

1566
00:39:45,570 --> 00:39:45,580
input file and the very first thing is
 

1567
00:39:45,580 --> 00:39:50,640
input file and the very first thing is
that each worker reads as part of the

1568
00:39:50,640 --> 00:39:50,650
that each worker reads as part of the
 

1569
00:39:50,650 --> 00:39:53,250
that each worker reads as part of the
input file so this is the read their

1570
00:39:53,250 --> 00:39:53,260
input file so this is the read their
 

1571
00:39:53,260 --> 00:39:55,950
input file so this is the read their
file read if you remember the next step

1572
00:39:55,950 --> 00:39:55,960
file read if you remember the next step
 

1573
00:39:55,960 --> 00:39:57,750
file read if you remember the next step
is a map where the each worker supposed

1574
00:39:57,750 --> 00:39:57,760
is a map where the each worker supposed
 

1575
00:39:57,760 --> 00:39:59,970
is a map where the each worker supposed
to map a little function that splits up

1576
00:39:59,970 --> 00:39:59,980
to map a little function that splits up
 

1577
00:39:59,980 --> 00:40:02,760
to map a little function that splits up
each line of input into a from two

1578
00:40:02,760 --> 00:40:02,770
each line of input into a from two
 

1579
00:40:02,770 --> 00:40:06,030
each line of input into a from two
linked tupple um but this is a purely

1580
00:40:06,030 --> 00:40:06,040
linked tupple um but this is a purely
 

1581
00:40:06,040 --> 00:40:08,400
linked tupple um but this is a purely
local operation and so it can go on in

1582
00:40:08,400 --> 00:40:08,410
local operation and so it can go on in
 

1583
00:40:08,410 --> 00:40:10,650
local operation and so it can go on in
the same worker so we imagine that we

1584
00:40:10,650 --> 00:40:10,660
the same worker so we imagine that we
 

1585
00:40:10,660 --> 00:40:13,440
the same worker so we imagine that we
read the data and then in the very same

1586
00:40:13,440 --> 00:40:13,450
read the data and then in the very same
 

1587
00:40:13,450 --> 00:40:16,590
read the data and then in the very same
worker spark is gonna do that initial

1588
00:40:16,590 --> 00:40:16,600
worker spark is gonna do that initial
 

1589
00:40:16,600 --> 00:40:19,740
worker spark is gonna do that initial
map so you know I'm drawing an arrow

1590
00:40:19,740 --> 00:40:19,750
map so you know I'm drawing an arrow
 

1591
00:40:19,750 --> 00:40:21,660
map so you know I'm drawing an arrow
here's really an arrow from each worker

1592
00:40:21,660 --> 00:40:21,670
here's really an arrow from each worker
 

1593
00:40:21,670 --> 00:40:22,950
here's really an arrow from each worker
to itself so there's no network

1594
00:40:22,950 --> 00:40:22,960
to itself so there's no network
 

1595
00:40:22,960 --> 00:40:24,720
to itself so there's no network
communication involved indeed it's just

1596
00:40:24,720 --> 00:40:24,730
communication involved indeed it's just
 

1597
00:40:24,730 --> 00:40:28,530
communication involved indeed it's just
you know we run the first read and the

1598
00:40:28,530 --> 00:40:28,540
you know we run the first read and the
 

1599
00:40:28,540 --> 00:40:30,180
you know we run the first read and the
output can be directly fed to that

1600
00:40:30,180 --> 00:40:30,190
output can be directly fed to that
 

1601
00:40:30,190 --> 00:40:33,690
output can be directly fed to that
little map function and in fact this is

1602
00:40:33,690 --> 00:40:33,700
little map function and in fact this is
 

1603
00:40:33,700 --> 00:40:39,200
little map function and in fact this is
that that initial map in fact spark

1604
00:40:39,200 --> 00:40:39,210
that that initial map in fact spark
 

1605
00:40:39,210 --> 00:40:40,970
that that initial map in fact spark
certainly streams the data record by

1606
00:40:40,970 --> 00:40:40,980
certainly streams the data record by
 

1607
00:40:40,980 --> 00:40:43,010
certainly streams the data record by
record through these transformations so

1608
00:40:43,010 --> 00:40:43,020
record through these transformations so
 

1609
00:40:43,020 --> 00:40:45,250
record through these transformations so
instead of reading the entire input

1610
00:40:45,250 --> 00:40:45,260
instead of reading the entire input
 

1611
00:40:45,260 --> 00:40:47,599
instead of reading the entire input
partition and then running the map on

1612
00:40:47,599 --> 00:40:47,609
partition and then running the map on
 

1613
00:40:47,609 --> 00:40:52,010
partition and then running the map on
the entire input partition SPARC reads

1614
00:40:52,010 --> 00:40:52,020
the entire input partition SPARC reads
 

1615
00:40:52,020 --> 00:40:53,690
the entire input partition SPARC reads
the first record or maybe the first just

1616
00:40:53,690 --> 00:40:53,700
the first record or maybe the first just
 

1617
00:40:53,700 --> 00:40:56,300
the first record or maybe the first just
couple of records and then runs the map

1618
00:40:56,300 --> 00:40:56,310
couple of records and then runs the map
 

1619
00:40:56,310 --> 00:40:58,640
couple of records and then runs the map
on just sort of all I'm each record in

1620
00:40:58,640 --> 00:40:58,650
on just sort of all I'm each record in
 

1621
00:40:58,650 --> 00:41:02,000
on just sort of all I'm each record in
fact runs each record of E if it was

1622
00:41:02,000 --> 00:41:02,010
fact runs each record of E if it was
 

1623
00:41:02,010 --> 00:41:05,300
fact runs each record of E if it was
many transformations as it can before

1624
00:41:05,300 --> 00:41:05,310
many transformations as it can before
 

1625
00:41:05,310 --> 00:41:06,650
many transformations as it can before
going on and reading the next little bit

1626
00:41:06,650 --> 00:41:06,660
going on and reading the next little bit
 

1627
00:41:06,660 --> 00:41:08,180
going on and reading the next little bit
from the file and that's so that it

1628
00:41:08,180 --> 00:41:08,190
from the file and that's so that it
 

1629
00:41:08,190 --> 00:41:10,040
from the file and that's so that it
doesn't have to store yes these files

1630
00:41:10,040 --> 00:41:10,050
doesn't have to store yes these files
 

1631
00:41:10,050 --> 00:41:13,099
doesn't have to store yes these files
could be very large it isn't one half so

1632
00:41:13,099 --> 00:41:13,109
could be very large it isn't one half so
 

1633
00:41:13,109 --> 00:41:14,839
could be very large it isn't one half so
like store the entire input file it's

1634
00:41:14,839 --> 00:41:14,849
like store the entire input file it's
 

1635
00:41:14,849 --> 00:41:16,940
like store the entire input file it's
much more efficient just to process it

1636
00:41:16,940 --> 00:41:16,950
much more efficient just to process it
 

1637
00:41:16,950 --> 00:41:18,980
much more efficient just to process it
record by record okay so there's a

1638
00:41:18,980 --> 00:41:18,990
record by record okay so there's a
 

1639
00:41:18,990 --> 00:41:22,099
record by record okay so there's a
question so the first node in each chain

1640
00:41:22,099 --> 00:41:22,109
question so the first node in each chain
 

1641
00:41:22,109 --> 00:41:24,410
question so the first node in each chain
is the worker holding the HDFS chunks

1642
00:41:24,410 --> 00:41:24,420
is the worker holding the HDFS chunks
 

1643
00:41:24,420 --> 00:41:26,570
is the worker holding the HDFS chunks
and the remaining nodes in the chain are

1644
00:41:26,570 --> 00:41:26,580
and the remaining nodes in the chain are
 

1645
00:41:26,580 --> 00:41:28,339
and the remaining nodes in the chain are
the nodes in the lineage oh yeah I'm

1646
00:41:28,339 --> 00:41:28,349
the nodes in the lineage oh yeah I'm
 

1647
00:41:28,349 --> 00:41:29,630
the nodes in the lineage oh yeah I'm
afraid I've been a little bit confusing

1648
00:41:29,630 --> 00:41:29,640
afraid I've been a little bit confusing
 

1649
00:41:29,640 --> 00:41:32,210
afraid I've been a little bit confusing
here I think the way to think of this is

1650
00:41:32,210 --> 00:41:32,220
here I think the way to think of this is
 

1651
00:41:32,220 --> 00:41:35,060
here I think the way to think of this is
that so far all this happen is happening

1652
00:41:35,060 --> 00:41:35,070
that so far all this happen is happening
 

1653
00:41:35,070 --> 00:41:37,790
that so far all this happen is happening
on it on individual workers so this is

1654
00:41:37,790 --> 00:41:37,800
on it on individual workers so this is
 

1655
00:41:37,800 --> 00:41:40,450
on it on individual workers so this is
worker one maybe this is another worker

1656
00:41:40,450 --> 00:41:40,460
worker one maybe this is another worker
 

1657
00:41:40,460 --> 00:41:45,880
worker one maybe this is another worker
and

1658
00:41:45,880 --> 00:41:45,890

 

1659
00:41:45,890 --> 00:41:48,550

each worker is sort of proceeding

1660
00:41:48,550 --> 00:41:48,560
each worker is sort of proceeding
 

1661
00:41:48,560 --> 00:41:50,200
each worker is sort of proceeding
independently and I'm imagining that

1662
00:41:50,200 --> 00:41:50,210
independently and I'm imagining that
 

1663
00:41:50,210 --> 00:41:53,020
independently and I'm imagining that
they're all running on the same machines

1664
00:41:53,020 --> 00:41:53,030
they're all running on the same machines
 

1665
00:41:53,030 --> 00:41:55,210
they're all running on the same machines
that stored the different partitions of

1666
00:41:55,210 --> 00:41:55,220
that stored the different partitions of
 

1667
00:41:55,220 --> 00:41:57,160
that stored the different partitions of
the HTTPS fob but there could be Network

1668
00:41:57,160 --> 00:41:57,170
the HTTPS fob but there could be Network
 

1669
00:41:57,170 --> 00:41:59,620
the HTTPS fob but there could be Network
communication here to get from HDFS to

1670
00:41:59,620 --> 00:41:59,630
communication here to get from HDFS to
 

1671
00:41:59,630 --> 00:42:02,350
communication here to get from HDFS to
the to the responsible worker but after

1672
00:42:02,350 --> 00:42:02,360
the to the responsible worker but after
 

1673
00:42:02,360 --> 00:42:04,980
the to the responsible worker but after
that it's very fast kind of local

1674
00:42:04,980 --> 00:42:04,990
that it's very fast kind of local
 

1675
00:42:04,990 --> 00:42:17,440
that it's very fast kind of local
operations all right and so this is what

1676
00:42:17,440 --> 00:42:17,450
operations all right and so this is what
 

1677
00:42:17,450 --> 00:42:20,080
operations all right and so this is what
happens for the with the people called

1678
00:42:20,080 --> 00:42:20,090
happens for the with the people called
 

1679
00:42:20,090 --> 00:42:23,200
happens for the with the people called
the narrow

1680
00:42:23,200 --> 00:42:23,210

 

1681
00:42:23,210 --> 00:42:25,609

dependencies that is transformations

1682
00:42:25,609 --> 00:42:25,619
dependencies that is transformations
 

1683
00:42:25,619 --> 00:42:28,519
dependencies that is transformations
that just look consider each record of

1684
00:42:28,519 --> 00:42:28,529
that just look consider each record of
 

1685
00:42:28,529 --> 00:42:30,799
that just look consider each record of
data independently without ever having

1686
00:42:30,799 --> 00:42:30,809
data independently without ever having
 

1687
00:42:30,809 --> 00:42:33,739
data independently without ever having
to worry about the relationship to other

1688
00:42:33,739 --> 00:42:33,749
to worry about the relationship to other
 

1689
00:42:33,749 --> 00:42:37,309
to worry about the relationship to other
records so by the way this is already

1690
00:42:37,309 --> 00:42:37,319
records so by the way this is already
 

1691
00:42:37,319 --> 00:42:39,229
records so by the way this is already
potentially more efficient than

1692
00:42:39,229 --> 00:42:39,239
potentially more efficient than
 

1693
00:42:39,239 --> 00:42:43,390
potentially more efficient than
MapReduce and that's because if we have

1694
00:42:43,390 --> 00:42:43,400
MapReduce and that's because if we have
 

1695
00:42:43,400 --> 00:42:46,759
MapReduce and that's because if we have
what amount to multiple map phases here

1696
00:42:46,759 --> 00:42:46,769
what amount to multiple map phases here
 

1697
00:42:46,769 --> 00:42:48,410
what amount to multiple map phases here
they just string together in memory

1698
00:42:48,410 --> 00:42:48,420
they just string together in memory
 

1699
00:42:48,420 --> 00:42:50,989
they just string together in memory
whereas MapReduce if you're not super

1700
00:42:50,989 --> 00:42:50,999
whereas MapReduce if you're not super
 

1701
00:42:50,999 --> 00:42:51,620
whereas MapReduce if you're not super
clever

1702
00:42:51,620 --> 00:42:51,630
clever
 

1703
00:42:51,630 --> 00:42:54,620
clever
if you run multiple MapReduce is even if

1704
00:42:54,620 --> 00:42:54,630
if you run multiple MapReduce is even if
 

1705
00:42:54,630 --> 00:42:56,469
if you run multiple MapReduce is even if
they're sort of degenerate map only

1706
00:42:56,469 --> 00:42:56,479
they're sort of degenerate map only
 

1707
00:42:56,479 --> 00:42:59,630
they're sort of degenerate map only
MapReduce applications each stage would

1708
00:42:59,630 --> 00:42:59,640
MapReduce applications each stage would
 

1709
00:42:59,640 --> 00:43:02,479
MapReduce applications each stage would
reduce input from G of s compute and

1710
00:43:02,479 --> 00:43:02,489
reduce input from G of s compute and
 

1711
00:43:02,489 --> 00:43:04,609
reduce input from G of s compute and
write its output back to GFS then the

1712
00:43:04,609 --> 00:43:04,619
write its output back to GFS then the
 

1713
00:43:04,619 --> 00:43:07,069
write its output back to GFS then the
next stage would be compute right so

1714
00:43:07,069 --> 00:43:07,079
next stage would be compute right so
 

1715
00:43:07,079 --> 00:43:08,299
next stage would be compute right so
here we've eliminated the reading

1716
00:43:08,299 --> 00:43:08,309
here we've eliminated the reading
 

1717
00:43:08,309 --> 00:43:10,670
here we've eliminated the reading
writing in it you know it's not a very

1718
00:43:10,670 --> 00:43:10,680
writing in it you know it's not a very
 

1719
00:43:10,680 --> 00:43:14,299
writing in it you know it's not a very
deep advantage but it sure helps

1720
00:43:14,299 --> 00:43:14,309
deep advantage but it sure helps
 

1721
00:43:14,309 --> 00:43:20,620
deep advantage but it sure helps
enormous Li for efficiency okay however

1722
00:43:20,620 --> 00:43:20,630
enormous Li for efficiency okay however
 

1723
00:43:20,630 --> 00:43:23,120
enormous Li for efficiency okay however
not all the transformations are narrow

1724
00:43:23,120 --> 00:43:23,130
not all the transformations are narrow
 

1725
00:43:23,130 --> 00:43:26,499
not all the transformations are narrow
not all just sort of read their input

1726
00:43:26,499 --> 00:43:26,509
not all just sort of read their input
 

1727
00:43:26,509 --> 00:43:28,519
not all just sort of read their input
record by record kind of with every

1728
00:43:28,519 --> 00:43:28,529
record by record kind of with every
 

1729
00:43:28,529 --> 00:43:30,349
record by record kind of with every
record independent from other records

1730
00:43:30,349 --> 00:43:30,359
record independent from other records
 

1731
00:43:30,359 --> 00:43:32,479
record independent from other records
and so what I'm worried about is the

1732
00:43:32,479 --> 00:43:32,489
and so what I'm worried about is the
 

1733
00:43:32,489 --> 00:43:34,700
and so what I'm worried about is the
distinct call which needed to know all

1734
00:43:34,700 --> 00:43:34,710
distinct call which needed to know all
 

1735
00:43:34,710 --> 00:43:37,549
distinct call which needed to know all
instances all records that had a

1736
00:43:37,549 --> 00:43:37,559
instances all records that had a
 

1737
00:43:37,559 --> 00:43:39,829
instances all records that had a
particular key similarly group by key

1738
00:43:39,829 --> 00:43:39,839
particular key similarly group by key
 

1739
00:43:39,839 --> 00:43:42,739
particular key similarly group by key
needs to know about all instances that

1740
00:43:42,739 --> 00:43:42,749
needs to know about all instances that
 

1741
00:43:42,749 --> 00:43:45,769
needs to know about all instances that
have a key join also it's gotta move

1742
00:43:45,769 --> 00:43:45,779
have a key join also it's gotta move
 

1743
00:43:45,779 --> 00:43:50,200
have a key join also it's gotta move
things around so that takes two inputs

1744
00:43:50,200 --> 00:43:50,210
things around so that takes two inputs
 

1745
00:43:50,210 --> 00:43:53,390
things around so that takes two inputs
needs to join together all keys from

1746
00:43:53,390 --> 00:43:53,400
needs to join together all keys from
 

1747
00:43:53,400 --> 00:43:54,950
needs to join together all keys from
both inputs so that this all records

1748
00:43:54,950 --> 00:43:54,960
both inputs so that this all records
 

1749
00:43:54,960 --> 00:43:56,269
both inputs so that this all records
from both inputs that are the same key

1750
00:43:56,269 --> 00:43:56,279
from both inputs that are the same key
 

1751
00:43:56,279 --> 00:43:58,640
from both inputs that are the same key
so there's a bunch of these non-local

1752
00:43:58,640 --> 00:43:58,650
so there's a bunch of these non-local
 

1753
00:43:58,650 --> 00:44:01,430
so there's a bunch of these non-local
transformations which the paper calls

1754
00:44:01,430 --> 00:44:01,440
transformations which the paper calls
 

1755
00:44:01,440 --> 00:44:04,459
transformations which the paper calls
wide transformations because they

1756
00:44:04,459 --> 00:44:04,469
wide transformations because they
 

1757
00:44:04,469 --> 00:44:05,979
wide transformations because they
potentially have to look at all

1758
00:44:05,979 --> 00:44:05,989
potentially have to look at all
 

1759
00:44:05,989 --> 00:44:08,569
potentially have to look at all
partitions of the input that's a lot

1760
00:44:08,569 --> 00:44:08,579
partitions of the input that's a lot
 

1761
00:44:08,579 --> 00:44:12,259
partitions of the input that's a lot
like reduce in MapReduce serve example

1762
00:44:12,259 --> 00:44:12,269
like reduce in MapReduce serve example
 

1763
00:44:12,269 --> 00:44:14,420
like reduce in MapReduce serve example
distinct exposing we're talking about

1764
00:44:14,420 --> 00:44:14,430
distinct exposing we're talking about
 

1765
00:44:14,430 --> 00:44:18,829
distinct exposing we're talking about
the distinct stage you know the distinct

1766
00:44:18,829 --> 00:44:18,839
the distinct stage you know the distinct
 

1767
00:44:18,839 --> 00:44:20,479
the distinct stage you know the distinct
is going to be run on multiple workers

1768
00:44:20,479 --> 00:44:20,489
is going to be run on multiple workers
 

1769
00:44:20,489 --> 00:44:24,650
is going to be run on multiple workers
also and no distinct works on each key

1770
00:44:24,650 --> 00:44:24,660
also and no distinct works on each key
 

1771
00:44:24,660 --> 00:44:27,289
also and no distinct works on each key
independently and so we can partition

1772
00:44:27,289 --> 00:44:27,299
independently and so we can partition
 

1773
00:44:27,299 --> 00:44:31,579
independently and so we can partition
the computation by key but the

1774
00:44:31,579 --> 00:44:31,589
the computation by key but the
 

1775
00:44:31,589 --> 00:44:33,229
the computation by key but the
data currently is not partitioned by key

1776
00:44:33,229 --> 00:44:33,239
data currently is not partitioned by key
 

1777
00:44:33,239 --> 00:44:34,519
data currently is not partitioned by key
at all actually isn't really partitioned

1778
00:44:34,519 --> 00:44:34,529
at all actually isn't really partitioned
 

1779
00:44:34,529 --> 00:44:36,559
at all actually isn't really partitioned
by anything but just sort of however

1780
00:44:36,559 --> 00:44:36,569
by anything but just sort of however
 

1781
00:44:36,569 --> 00:44:41,319
by anything but just sort of however
HDFS have my distorted so four distinct

1782
00:44:41,319 --> 00:44:41,329
HDFS have my distorted so four distinct
 

1783
00:44:41,329 --> 00:44:44,469
HDFS have my distorted so four distinct
we're gonna run distinct on all the word

1784
00:44:44,469 --> 00:44:44,479
we're gonna run distinct on all the word
 

1785
00:44:44,479 --> 00:44:46,009
we're gonna run distinct on all the word
partition and all the workers

1786
00:44:46,009 --> 00:44:46,019
partition and all the workers
 

1787
00:44:46,019 --> 00:44:49,999
partition and all the workers
partitioned by key but you know any one

1788
00:44:49,999 --> 00:44:50,009
partitioned by key but you know any one
 

1789
00:44:50,009 --> 00:44:52,789
partitioned by key but you know any one
worker needs to see all of the input

1790
00:44:52,789 --> 00:44:52,799
worker needs to see all of the input
 

1791
00:44:52,799 --> 00:44:54,920
worker needs to see all of the input
records with a given key which may be

1792
00:44:54,920 --> 00:44:54,930
records with a given key which may be
 

1793
00:44:54,930 --> 00:45:00,670
records with a given key which may be
spread out over all of the preceding

1794
00:45:00,670 --> 00:45:00,680

 

1795
00:45:00,680 --> 00:45:04,180

workers for the preceding transformation

1796
00:45:04,180 --> 00:45:04,190
workers for the preceding transformation
 

1797
00:45:04,190 --> 00:45:07,489
workers for the preceding transformation
and all of all of the you know they're

1798
00:45:07,489 --> 00:45:07,499
and all of all of the you know they're
 

1799
00:45:07,499 --> 00:45:09,229
and all of all of the you know they're
all for the workers are responsible for

1800
00:45:09,229 --> 00:45:09,239
all for the workers are responsible for
 

1801
00:45:09,239 --> 00:45:10,459
all for the workers are responsible for
different keys but the keys may be

1802
00:45:10,459 --> 00:45:10,469
different keys but the keys may be
 

1803
00:45:10,469 --> 00:45:16,839
different keys but the keys may be
spread out over

1804
00:45:16,839 --> 00:45:16,849

 

1805
00:45:16,849 --> 00:45:19,559

workers for the preceeding

1806
00:45:19,559 --> 00:45:19,569
workers for the preceeding
 

1807
00:45:19,569 --> 00:45:21,309
workers for the preceeding
transformation now in fact the workers

1808
00:45:21,309 --> 00:45:21,319
transformation now in fact the workers
 

1809
00:45:21,319 --> 00:45:23,349
transformation now in fact the workers
are the same typically it's gonna be the

1810
00:45:23,349 --> 00:45:23,359
are the same typically it's gonna be the
 

1811
00:45:23,359 --> 00:45:25,749
are the same typically it's gonna be the
same workers running the map is running

1812
00:45:25,749 --> 00:45:25,759
same workers running the map is running
 

1813
00:45:25,759 --> 00:45:27,549
same workers running the map is running
running the distinct but the data needs

1814
00:45:27,549 --> 00:45:27,559
running the distinct but the data needs
 

1815
00:45:27,559 --> 00:45:28,930
running the distinct but the data needs
to be moved between the two

1816
00:45:28,930 --> 00:45:28,940
to be moved between the two
 

1817
00:45:28,940 --> 00:45:30,910
to be moved between the two
transformations to bring all the keys

1818
00:45:30,910 --> 00:45:30,920
transformations to bring all the keys
 

1819
00:45:30,920 --> 00:45:33,039
transformations to bring all the keys
together and so what sparks actually

1820
00:45:33,039 --> 00:45:33,049
together and so what sparks actually
 

1821
00:45:33,049 --> 00:45:34,479
together and so what sparks actually
gonna do it's gonna take the output of

1822
00:45:34,479 --> 00:45:34,489
gonna do it's gonna take the output of
 

1823
00:45:34,489 --> 00:45:38,259
gonna do it's gonna take the output of
this map hash the each record by its key

1824
00:45:38,259 --> 00:45:38,269
this map hash the each record by its key
 

1825
00:45:38,269 --> 00:45:40,509
this map hash the each record by its key
and use that you know mod the number of

1826
00:45:40,509 --> 00:45:40,519
and use that you know mod the number of
 

1827
00:45:40,519 --> 00:45:42,759
and use that you know mod the number of
workers to select which workers should

1828
00:45:42,759 --> 00:45:42,769
workers to select which workers should
 

1829
00:45:42,769 --> 00:45:46,210
workers to select which workers should
see it and in fact the implementation is

1830
00:45:46,210 --> 00:45:46,220
see it and in fact the implementation is
 

1831
00:45:46,220 --> 00:45:48,339
see it and in fact the implementation is
a lot like your implementation of

1832
00:45:48,339 --> 00:45:48,349
a lot like your implementation of
 

1833
00:45:48,349 --> 00:45:51,009
a lot like your implementation of
MapReduce the very last thing that

1834
00:45:51,009 --> 00:45:51,019
MapReduce the very last thing that
 

1835
00:45:51,019 --> 00:45:55,539
MapReduce the very last thing that
happens in in the last of the narrow

1836
00:45:55,539 --> 00:45:55,549
happens in in the last of the narrow
 

1837
00:45:55,549 --> 00:45:59,019
happens in in the last of the narrow
stages is that the output is going to be

1838
00:45:59,019 --> 00:45:59,029
stages is that the output is going to be
 

1839
00:45:59,029 --> 00:46:01,690
stages is that the output is going to be
chopped up into buckets corresponding to

1840
00:46:01,690 --> 00:46:01,700
chopped up into buckets corresponding to
 

1841
00:46:01,700 --> 00:46:05,680
chopped up into buckets corresponding to
the different workers for the next

1842
00:46:05,680 --> 00:46:05,690
the different workers for the next
 

1843
00:46:05,690 --> 00:46:06,880
the different workers for the next
transformation where it's going to be

1844
00:46:06,880 --> 00:46:06,890
transformation where it's going to be
 

1845
00:46:06,890 --> 00:46:10,630
transformation where it's going to be
left waiting for them to fetch I saw the

1846
00:46:10,630 --> 00:46:10,640
left waiting for them to fetch I saw the
 

1847
00:46:10,640 --> 00:46:13,569
left waiting for them to fetch I saw the
scoop is that each of the workers run

1848
00:46:13,569 --> 00:46:13,579
scoop is that each of the workers run
 

1849
00:46:13,579 --> 00:46:15,489
scoop is that each of the workers run
the sort of as many stages all the

1850
00:46:15,489 --> 00:46:15,499
the sort of as many stages all the
 

1851
00:46:15,499 --> 00:46:16,930
the sort of as many stages all the
narrows stages they can through the

1852
00:46:16,930 --> 00:46:16,940
narrows stages they can through the
 

1853
00:46:16,940 --> 00:46:19,839
narrows stages they can through the
completion and store the output split up

1854
00:46:19,839 --> 00:46:19,849
completion and store the output split up
 

1855
00:46:19,849 --> 00:46:21,279
completion and store the output split up
into buckets when all of these are

1856
00:46:21,279 --> 00:46:21,289
into buckets when all of these are
 

1857
00:46:21,289 --> 00:46:24,959
into buckets when all of these are
finished then we can start running the

1858
00:46:24,959 --> 00:46:24,969
finished then we can start running the
 

1859
00:46:24,969 --> 00:46:27,849
finished then we can start running the
workers for the distinct transformation

1860
00:46:27,849 --> 00:46:27,859
workers for the distinct transformation
 

1861
00:46:27,859 --> 00:46:30,339
workers for the distinct transformation
whose first step is go and fetch from

1862
00:46:30,339 --> 00:46:30,349
whose first step is go and fetch from
 

1863
00:46:30,349 --> 00:46:32,650
whose first step is go and fetch from
every other worker the relevant bucket

1864
00:46:32,650 --> 00:46:32,660
every other worker the relevant bucket
 

1865
00:46:32,660 --> 00:46:35,170
every other worker the relevant bucket
of the output of the last narrow stage

1866
00:46:35,170 --> 00:46:35,180
of the output of the last narrow stage
 

1867
00:46:35,180 --> 00:46:38,229
of the output of the last narrow stage
and then we can run the distinct because

1868
00:46:38,229 --> 00:46:38,239
and then we can run the distinct because
 

1869
00:46:38,239 --> 00:46:40,150
and then we can run the distinct because
all the given keys are on the same

1870
00:46:40,150 --> 00:46:40,160
all the given keys are on the same
 

1871
00:46:40,160 --> 00:46:42,249
all the given keys are on the same
worker and they can all start producing

1872
00:46:42,249 --> 00:46:42,259
worker and they can all start producing
 

1873
00:46:42,259 --> 00:46:48,189
worker and they can all start producing
output themselves

1874
00:46:48,189 --> 00:46:48,199

 

1875
00:46:48,199 --> 00:46:50,659

all right now of course these Y

1876
00:46:50,659 --> 00:46:50,669
all right now of course these Y
 

1877
00:46:50,669 --> 00:46:52,699
all right now of course these Y
transformations are quite expensive the

1878
00:46:52,699 --> 00:46:52,709
transformations are quite expensive the
 

1879
00:46:52,709 --> 00:46:54,229
transformations are quite expensive the
now transformations are super efficient

1880
00:46:54,229 --> 00:46:54,239
now transformations are super efficient
 

1881
00:46:54,239 --> 00:46:56,449
now transformations are super efficient
because we're just sort of taking each

1882
00:46:56,449 --> 00:46:56,459
because we're just sort of taking each
 

1883
00:46:56,459 --> 00:46:58,279
because we're just sort of taking each
record and running a bunch of functions

1884
00:46:58,279 --> 00:46:58,289
record and running a bunch of functions
 

1885
00:46:58,289 --> 00:47:00,919
record and running a bunch of functions
on it totally locally the Y

1886
00:47:00,919 --> 00:47:00,929
on it totally locally the Y
 

1887
00:47:00,929 --> 00:47:02,749
on it totally locally the Y
transformations require pushing a lot of

1888
00:47:02,749 --> 00:47:02,759
transformations require pushing a lot of
 

1889
00:47:02,759 --> 00:47:04,339
transformations require pushing a lot of
data impact essentially all of the data

1890
00:47:04,339 --> 00:47:04,349
data impact essentially all of the data
 

1891
00:47:04,349 --> 00:47:06,229
data impact essentially all of the data
in for PageRank you know you get

1892
00:47:06,229 --> 00:47:06,239
in for PageRank you know you get
 

1893
00:47:06,239 --> 00:47:08,870
in for PageRank you know you get
terabytes of input data that means that

1894
00:47:08,870 --> 00:47:08,880
terabytes of input data that means that
 

1895
00:47:08,880 --> 00:47:10,639
terabytes of input data that means that
you know it's still the same data at

1896
00:47:10,639 --> 00:47:10,649
you know it's still the same data at
 

1897
00:47:10,649 --> 00:47:12,349
you know it's still the same data at
this stage because it's all the links

1898
00:47:12,349 --> 00:47:12,359
this stage because it's all the links
 

1899
00:47:12,359 --> 00:47:15,049
this stage because it's all the links
and then in the web so now we're pushing

1900
00:47:15,049 --> 00:47:15,059
and then in the web so now we're pushing
 

1901
00:47:15,059 --> 00:47:17,299
and then in the web so now we're pushing
terabytes and terabytes of data over the

1902
00:47:17,299 --> 00:47:17,309
terabytes and terabytes of data over the
 

1903
00:47:17,309 --> 00:47:19,309
terabytes and terabytes of data over the
network to implement this shuffle from

1904
00:47:19,309 --> 00:47:19,319
network to implement this shuffle from
 

1905
00:47:19,319 --> 00:47:22,999
network to implement this shuffle from
the output of the map functions to the

1906
00:47:22,999 --> 00:47:23,009
the output of the map functions to the
 

1907
00:47:23,009 --> 00:47:24,649
the output of the map functions to the
input of the distinct functions so these

1908
00:47:24,649 --> 00:47:24,659
input of the distinct functions so these
 

1909
00:47:24,659 --> 00:47:28,219
input of the distinct functions so these
wide transformations are pretty

1910
00:47:28,219 --> 00:47:28,229
wide transformations are pretty
 

1911
00:47:28,229 --> 00:47:28,699
wide transformations are pretty
heavyweight

1912
00:47:28,699 --> 00:47:28,709
heavyweight
 

1913
00:47:28,709 --> 00:47:31,519
heavyweight
a lot of communication and they're also

1914
00:47:31,519 --> 00:47:31,529
a lot of communication and they're also
 

1915
00:47:31,529 --> 00:47:33,469
a lot of communication and they're also
kind of computation barrier because we

1916
00:47:33,469 --> 00:47:33,479
kind of computation barrier because we
 

1917
00:47:33,479 --> 00:47:35,659
kind of computation barrier because we
have to wait all for all the narrow

1918
00:47:35,659 --> 00:47:35,669
have to wait all for all the narrow
 

1919
00:47:35,669 --> 00:47:37,579
have to wait all for all the narrow
processing to finish before we can go on

1920
00:47:37,579 --> 00:47:37,589
processing to finish before we can go on
 

1921
00:47:37,589 --> 00:47:45,969
processing to finish before we can go on
to the so there's wide transformation

1922
00:47:45,969 --> 00:47:45,979

 

1923
00:47:45,979 --> 00:47:54,279

all right that said the there are some

1924
00:47:54,279 --> 00:47:54,289

 

1925
00:47:54,289 --> 00:47:57,459

optimizations that are possible because

1926
00:47:57,459 --> 00:47:57,469
optimizations that are possible because
 

1927
00:47:57,469 --> 00:47:59,809
optimizations that are possible because
SPARC has a view SPARC creates the

1928
00:47:59,809 --> 00:47:59,819
SPARC has a view SPARC creates the
 

1929
00:47:59,819 --> 00:48:04,939
SPARC has a view SPARC creates the
entire lineage graph before it starts

1930
00:48:04,939 --> 00:48:04,949
entire lineage graph before it starts
 

1931
00:48:04,949 --> 00:48:06,649
entire lineage graph before it starts
any of the data processing so smart can

1932
00:48:06,649 --> 00:48:06,659
any of the data processing so smart can
 

1933
00:48:06,659 --> 00:48:08,419
any of the data processing so smart can
inspect the lineage graph and look for

1934
00:48:08,419 --> 00:48:08,429
inspect the lineage graph and look for
 

1935
00:48:08,429 --> 00:48:10,129
inspect the lineage graph and look for
opportunities for optimization and

1936
00:48:10,129 --> 00:48:10,139
opportunities for optimization and
 

1937
00:48:10,139 --> 00:48:13,489
opportunities for optimization and
certainly running all of if there's a

1938
00:48:13,489 --> 00:48:13,499
certainly running all of if there's a
 

1939
00:48:13,499 --> 00:48:15,589
certainly running all of if there's a
sequence of narrow stages running them

1940
00:48:15,589 --> 00:48:15,599
sequence of narrow stages running them
 

1941
00:48:15,599 --> 00:48:17,089
sequence of narrow stages running them
all in the same machine by basically

1942
00:48:17,089 --> 00:48:17,099
all in the same machine by basically
 

1943
00:48:17,099 --> 00:48:19,609
all in the same machine by basically
sequential function calls on each input

1944
00:48:19,609 --> 00:48:19,619
sequential function calls on each input
 

1945
00:48:19,619 --> 00:48:21,469
sequential function calls on each input
record that's definitely an optimization

1946
00:48:21,469 --> 00:48:21,479
record that's definitely an optimization
 

1947
00:48:21,479 --> 00:48:24,169
record that's definitely an optimization
that you can only notice if you sort of

1948
00:48:24,169 --> 00:48:24,179
that you can only notice if you sort of
 

1949
00:48:24,179 --> 00:48:26,919
that you can only notice if you sort of
see the entire lineage graph all at once

1950
00:48:26,919 --> 00:48:26,929
see the entire lineage graph all at once
 

1951
00:48:26,929 --> 00:48:34,460
see the entire lineage graph all at once
another optimization that

1952
00:48:34,460 --> 00:48:34,470

 

1953
00:48:34,470 --> 00:48:37,220

spark does is noticing when the data has

1954
00:48:37,220 --> 00:48:37,230
spark does is noticing when the data has
 

1955
00:48:37,230 --> 00:48:40,130
spark does is noticing when the data has
all has has already been partitioned due

1956
00:48:40,130 --> 00:48:40,140
all has has already been partitioned due
 

1957
00:48:40,140 --> 00:48:42,320
all has has already been partitioned due
to a wide shuffle that the data is

1958
00:48:42,320 --> 00:48:42,330
to a wide shuffle that the data is
 

1959
00:48:42,330 --> 00:48:44,300
to a wide shuffle that the data is
already partitioned in the way that it's

1960
00:48:44,300 --> 00:48:44,310
already partitioned in the way that it's
 

1961
00:48:44,310 --> 00:48:47,440
already partitioned in the way that it's
going to be needed for the next wide

1962
00:48:47,440 --> 00:48:47,450
going to be needed for the next wide
 

1963
00:48:47,450 --> 00:48:51,230
going to be needed for the next wide
transformation so in the in our original

1964
00:48:51,230 --> 00:48:51,240
transformation so in the in our original
 

1965
00:48:51,240 --> 00:48:57,980
transformation so in the in our original
program let's see I think we have two

1966
00:48:57,980 --> 00:48:57,990
program let's see I think we have two
 

1967
00:48:57,990 --> 00:49:00,020
program let's see I think we have two
wide transformations in a row distinct

1968
00:49:00,020 --> 00:49:00,030
wide transformations in a row distinct
 

1969
00:49:00,030 --> 00:49:02,930
wide transformations in a row distinct
requires a shuffle but group by key also

1970
00:49:02,930 --> 00:49:02,940
requires a shuffle but group by key also
 

1971
00:49:02,940 --> 00:49:05,839
requires a shuffle but group by key also
it's gonna bring together all the

1972
00:49:05,839 --> 00:49:05,849
it's gonna bring together all the
 

1973
00:49:05,849 --> 00:49:08,180
it's gonna bring together all the
records with a given key and replace

1974
00:49:08,180 --> 00:49:08,190
records with a given key and replace
 

1975
00:49:08,190 --> 00:49:11,599
records with a given key and replace
them with a list of for every key the

1976
00:49:11,599 --> 00:49:11,609
them with a list of for every key the
 

1977
00:49:11,609 --> 00:49:14,660
them with a list of for every key the
list of links you know starting at that

1978
00:49:14,660 --> 00:49:14,670
list of links you know starting at that
 

1979
00:49:14,670 --> 00:49:16,849
list of links you know starting at that
URL these are both wide operators they

1980
00:49:16,849 --> 00:49:16,859
URL these are both wide operators they
 

1981
00:49:16,859 --> 00:49:19,700
URL these are both wide operators they
both are grouping by key and so maybe we

1982
00:49:19,700 --> 00:49:19,710
both are grouping by key and so maybe we
 

1983
00:49:19,710 --> 00:49:21,140
both are grouping by key and so maybe we
have to do a shuffle for the distinct

1984
00:49:21,140 --> 00:49:21,150
have to do a shuffle for the distinct
 

1985
00:49:21,150 --> 00:49:24,140
have to do a shuffle for the distinct
but spark can cleverly recognize a high

1986
00:49:24,140 --> 00:49:24,150
but spark can cleverly recognize a high
 

1987
00:49:24,150 --> 00:49:25,550
but spark can cleverly recognize a high
you know that is already shuffled in a

1988
00:49:25,550 --> 00:49:25,560
you know that is already shuffled in a
 

1989
00:49:25,560 --> 00:49:26,870
you know that is already shuffled in a
way that's appropriate for a group by

1990
00:49:26,870 --> 00:49:26,880
way that's appropriate for a group by
 

1991
00:49:26,880 --> 00:49:28,910
way that's appropriate for a group by
key we don't have to do in other shuffle

1992
00:49:28,910 --> 00:49:28,920
key we don't have to do in other shuffle
 

1993
00:49:28,920 --> 00:49:30,440
key we don't have to do in other shuffle
so even though group by key is in

1994
00:49:30,440 --> 00:49:30,450
so even though group by key is in
 

1995
00:49:30,450 --> 00:49:32,720
so even though group by key is in
principle it could be a wide

1996
00:49:32,720 --> 00:49:32,730
principle it could be a wide
 

1997
00:49:32,730 --> 00:49:36,290
principle it could be a wide
transformation in fact I suspect spark

1998
00:49:36,290 --> 00:49:36,300
transformation in fact I suspect spark
 

1999
00:49:36,300 --> 00:49:38,120
transformation in fact I suspect spark
implements it without communication

2000
00:49:38,120 --> 00:49:38,130
implements it without communication
 

2001
00:49:38,130 --> 00:49:39,890
implements it without communication
because the data is already partitioned

2002
00:49:39,890 --> 00:49:39,900
because the data is already partitioned
 

2003
00:49:39,900 --> 00:49:45,559
because the data is already partitioned
by key so maybe the group by key

2004
00:49:45,559 --> 00:49:45,569

 

2005
00:49:45,569 --> 00:49:48,390

can be done in this particular case

2006
00:49:48,390 --> 00:49:48,400
can be done in this particular case
 

2007
00:49:48,400 --> 00:49:53,989
can be done in this particular case
without shuffling data without expense

2008
00:49:53,989 --> 00:49:53,999

 

2009
00:49:53,999 --> 00:49:56,339

of course it you know can only do this

2010
00:49:56,339 --> 00:49:56,349
of course it you know can only do this
 

2011
00:49:56,349 --> 00:49:58,319
of course it you know can only do this
because it produced the entire lineage

2012
00:49:58,319 --> 00:49:58,329
because it produced the entire lineage
 

2013
00:49:58,329 --> 00:50:00,569
because it produced the entire lineage
graph first and only then ran the

2014
00:50:00,569 --> 00:50:00,579
graph first and only then ran the
 

2015
00:50:00,579 --> 00:50:02,549
graph first and only then ran the
computation so this part gets a chance

2016
00:50:02,549 --> 00:50:02,559
computation so this part gets a chance
 

2017
00:50:02,559 --> 00:50:07,319
computation so this part gets a chance
to sort of examine and optimize and

2018
00:50:07,319 --> 00:50:07,329
to sort of examine and optimize and
 

2019
00:50:07,329 --> 00:50:13,760
to sort of examine and optimize and
maybe transform the graph

2020
00:50:13,760 --> 00:50:13,770

 

2021
00:50:13,770 --> 00:50:16,880

so that looks topic actually any any

2022
00:50:16,880 --> 00:50:16,890
so that looks topic actually any any
 

2023
00:50:16,890 --> 00:50:20,720
so that looks topic actually any any
questions about lineage graphs or how

2024
00:50:20,720 --> 00:50:20,730
questions about lineage graphs or how
 

2025
00:50:20,730 --> 00:50:21,830
questions about lineage graphs or how
things are executed

2026
00:50:21,830 --> 00:50:21,840
things are executed
 

2027
00:50:21,840 --> 00:50:28,370
things are executed
I feel free to interact the next thing I

2028
00:50:28,370 --> 00:50:28,380
I feel free to interact the next thing I
 

2029
00:50:28,380 --> 00:50:33,010
I feel free to interact the next thing I
want to talk about is fault tolerance

2030
00:50:33,010 --> 00:50:33,020
want to talk about is fault tolerance
 

2031
00:50:33,020 --> 00:50:40,070
want to talk about is fault tolerance
and here the you know these kind of

2032
00:50:40,070 --> 00:50:40,080
and here the you know these kind of
 

2033
00:50:40,080 --> 00:50:41,720
and here the you know these kind of
computations they're not the fault

2034
00:50:41,720 --> 00:50:41,730
computations they're not the fault
 

2035
00:50:41,730 --> 00:50:42,980
computations they're not the fault
tolerance are looking for is not the

2036
00:50:42,980 --> 00:50:42,990
tolerance are looking for is not the
 

2037
00:50:42,990 --> 00:50:45,110
tolerance are looking for is not the
sort of absolute fault tolerance you

2038
00:50:45,110 --> 00:50:45,120
sort of absolute fault tolerance you
 

2039
00:50:45,120 --> 00:50:46,610
sort of absolute fault tolerance you
would want with the database what you

2040
00:50:46,610 --> 00:50:46,620
would want with the database what you
 

2041
00:50:46,620 --> 00:50:48,590
would want with the database what you
really just cannot ever afford to lose

2042
00:50:48,590 --> 00:50:48,600
really just cannot ever afford to lose
 

2043
00:50:48,600 --> 00:50:49,940
really just cannot ever afford to lose
anything what you really want is a

2044
00:50:49,940 --> 00:50:49,950
anything what you really want is a
 

2045
00:50:49,950 --> 00:50:53,390
anything what you really want is a
database that never loses data here the

2046
00:50:53,390 --> 00:50:53,400
database that never loses data here the
 

2047
00:50:53,400 --> 00:50:54,890
database that never loses data here the
fault tolerance we're looking for is

2048
00:50:54,890 --> 00:50:54,900
fault tolerance we're looking for is
 

2049
00:50:54,900 --> 00:50:58,730
fault tolerance we're looking for is
more like well it's expensive if we have

2050
00:50:58,730 --> 00:50:58,740
more like well it's expensive if we have
 

2051
00:50:58,740 --> 00:51:00,530
more like well it's expensive if we have
to repeat the computation we can totally

2052
00:51:00,530 --> 00:51:00,540
to repeat the computation we can totally
 

2053
00:51:00,540 --> 00:51:02,300
to repeat the computation we can totally
repeat this computation if we have to

2054
00:51:02,300 --> 00:51:02,310
repeat this computation if we have to
 

2055
00:51:02,310 --> 00:51:04,580
repeat this computation if we have to
but you know it would take us a couple

2056
00:51:04,580 --> 00:51:04,590
but you know it would take us a couple
 

2057
00:51:04,590 --> 00:51:06,860
but you know it would take us a couple
of hours and that's irritating but not

2058
00:51:06,860 --> 00:51:06,870
of hours and that's irritating but not
 

2059
00:51:06,870 --> 00:51:09,500
of hours and that's irritating but not
the end of the world so we're looking to

2060
00:51:09,500 --> 00:51:09,510
the end of the world so we're looking to
 

2061
00:51:09,510 --> 00:51:12,620
the end of the world so we're looking to
you know tolerate common errors but we

2062
00:51:12,620 --> 00:51:12,630
you know tolerate common errors but we
 

2063
00:51:12,630 --> 00:51:15,130
you know tolerate common errors but we
don't have to certainly don't have to

2064
00:51:15,130 --> 00:51:15,140
don't have to certainly don't have to
 

2065
00:51:15,140 --> 00:51:19,570
don't have to certainly don't have to
having bulletproof ability to tolerate

2066
00:51:19,570 --> 00:51:19,580
having bulletproof ability to tolerate
 

2067
00:51:19,580 --> 00:51:26,690
having bulletproof ability to tolerate
any possible error so for example spark

2068
00:51:26,690 --> 00:51:26,700
any possible error so for example spark
 

2069
00:51:26,700 --> 00:51:29,510
any possible error so for example spark
doesn't replicate that driver machine if

2070
00:51:29,510 --> 00:51:29,520
doesn't replicate that driver machine if
 

2071
00:51:29,520 --> 00:51:31,880
doesn't replicate that driver machine if
the driver which was sort of controlling

2072
00:51:31,880 --> 00:51:31,890
the driver which was sort of controlling
 

2073
00:51:31,890 --> 00:51:33,170
the driver which was sort of controlling
the computation and knew about the

2074
00:51:33,170 --> 00:51:33,180
the computation and knew about the
 

2075
00:51:33,180 --> 00:51:34,850
the computation and knew about the
lineage graph of the driver crashes I

2076
00:51:34,850 --> 00:51:34,860
lineage graph of the driver crashes I
 

2077
00:51:34,860 --> 00:51:36,110
lineage graph of the driver crashes I
think you have to rerun the whole thing

2078
00:51:36,110 --> 00:51:36,120
think you have to rerun the whole thing
 

2079
00:51:36,120 --> 00:51:38,270
think you have to rerun the whole thing
but you know any only any one machine

2080
00:51:38,270 --> 00:51:38,280
but you know any only any one machine
 

2081
00:51:38,280 --> 00:51:40,460
but you know any only any one machine
only crashes maybe every few months so

2082
00:51:40,460 --> 00:51:40,470
only crashes maybe every few months so
 

2083
00:51:40,470 --> 00:51:41,420
only crashes maybe every few months so
that's no big deal

2084
00:51:41,420 --> 00:51:41,430
that's no big deal
 

2085
00:51:41,430 --> 00:51:45,890
that's no big deal
another thing to notice is that HDFS is

2086
00:51:45,890 --> 00:51:45,900
another thing to notice is that HDFS is
 

2087
00:51:45,900 --> 00:51:48,080
another thing to notice is that HDFS is
sort of a separate thing SPARC is just

2088
00:51:48,080 --> 00:51:48,090
sort of a separate thing SPARC is just
 

2089
00:51:48,090 --> 00:51:52,040
sort of a separate thing SPARC is just
assuming that the input is replicated in

2090
00:51:52,040 --> 00:51:52,050
assuming that the input is replicated in
 

2091
00:51:52,050 --> 00:51:55,280
assuming that the input is replicated in
a fault-tolerant way on HDFS and indeed

2092
00:51:55,280 --> 00:51:55,290
a fault-tolerant way on HDFS and indeed
 

2093
00:51:55,290 --> 00:51:58,280
a fault-tolerant way on HDFS and indeed
just just like GFS HDFS does indeed keep

2094
00:51:58,280 --> 00:51:58,290
just just like GFS HDFS does indeed keep
 

2095
00:51:58,290 --> 00:51:59,750
just just like GFS HDFS does indeed keep
multiple copies of the data on multiple

2096
00:51:59,750 --> 00:51:59,760
multiple copies of the data on multiple
 

2097
00:51:59,760 --> 00:52:02,300
multiple copies of the data on multiple
servers if one of them crashes can

2098
00:52:02,300 --> 00:52:02,310
servers if one of them crashes can
 

2099
00:52:02,310 --> 00:52:05,210
servers if one of them crashes can
soldier on with the other copy so the

2100
00:52:05,210 --> 00:52:05,220
soldier on with the other copy so the
 

2101
00:52:05,220 --> 00:52:09,620
soldier on with the other copy so the
input data is assumed to be to be

2102
00:52:09,620 --> 00:52:09,630
input data is assumed to be to be
 

2103
00:52:09,630 --> 00:52:11,330
input data is assumed to be to be
relatively fault tolerant and

2104
00:52:11,330 --> 00:52:11,340
relatively fault tolerant and
 

2105
00:52:11,340 --> 00:52:12,770
relatively fault tolerant and
what that means that at the highest

2106
00:52:12,770 --> 00:52:12,780
what that means that at the highest
 

2107
00:52:12,780 --> 00:52:17,300
what that means that at the highest
level is that spark strategy if one of

2108
00:52:17,300 --> 00:52:17,310
level is that spark strategy if one of
 

2109
00:52:17,310 --> 00:52:20,630
level is that spark strategy if one of
the workers fail is just to recompute

2110
00:52:20,630 --> 00:52:20,640
the workers fail is just to recompute
 

2111
00:52:20,640 --> 00:52:23,720
the workers fail is just to recompute
the whatever that worker was responsible

2112
00:52:23,720 --> 00:52:23,730
the whatever that worker was responsible
 

2113
00:52:23,730 --> 00:52:26,560
the whatever that worker was responsible
for to just repeat those computations

2114
00:52:26,560 --> 00:52:26,570
for to just repeat those computations
 

2115
00:52:26,570 --> 00:52:29,330
for to just repeat those computations
they were lost with the worker on some

2116
00:52:29,330 --> 00:52:29,340
they were lost with the worker on some
 

2117
00:52:29,340 --> 00:52:32,410
they were lost with the worker on some
other worker and on some other machine

2118
00:52:32,410 --> 00:52:32,420
other worker and on some other machine
 

2119
00:52:32,420 --> 00:52:37,120
other worker and on some other machine
so that's basically what's going on and

2120
00:52:37,120 --> 00:52:37,130
so that's basically what's going on and
 

2121
00:52:37,130 --> 00:52:40,100
so that's basically what's going on and
it you know it might take a while if you

2122
00:52:40,100 --> 00:52:40,110
it you know it might take a while if you
 

2123
00:52:40,110 --> 00:52:42,140
it you know it might take a while if you
have a long lineage like you would

2124
00:52:42,140 --> 00:52:42,150
have a long lineage like you would
 

2125
00:52:42,150 --> 00:52:44,330
have a long lineage like you would
actually get with PageRank because you

2126
00:52:44,330 --> 00:52:44,340
actually get with PageRank because you
 

2127
00:52:44,340 --> 00:52:45,260
actually get with PageRank because you
know PageRank with many iterations

2128
00:52:45,260 --> 00:52:45,270
know PageRank with many iterations
 

2129
00:52:45,270 --> 00:52:50,300
know PageRank with many iterations
produces a very long lineage graph one

2130
00:52:50,300 --> 00:52:50,310
produces a very long lineage graph one
 

2131
00:52:50,310 --> 00:52:53,780
produces a very long lineage graph one
way that spark makes it not so bad that

2132
00:52:53,780 --> 00:52:53,790
way that spark makes it not so bad that
 

2133
00:52:53,790 --> 00:52:55,490
way that spark makes it not so bad that
it has to be may have to be computer

2134
00:52:55,490 --> 00:52:55,500
it has to be may have to be computer
 

2135
00:52:55,500 --> 00:52:56,870
it has to be may have to be computer
everything from scratch if a worker

2136
00:52:56,870 --> 00:52:56,880
everything from scratch if a worker
 

2137
00:52:56,880 --> 00:53:00,770
everything from scratch if a worker
fails is that each workers actually

2138
00:53:00,770 --> 00:53:00,780
fails is that each workers actually
 

2139
00:53:00,780 --> 00:53:02,780
fails is that each workers actually
responsible for multiple partitions at

2140
00:53:02,780 --> 00:53:02,790
responsible for multiple partitions at
 

2141
00:53:02,790 --> 00:53:06,140
responsible for multiple partitions at
the input so spark can move those parts

2142
00:53:06,140 --> 00:53:06,150
the input so spark can move those parts
 

2143
00:53:06,150 --> 00:53:08,900
the input so spark can move those parts
move give each remaining worker just one

2144
00:53:08,900 --> 00:53:08,910
move give each remaining worker just one
 

2145
00:53:08,910 --> 00:53:10,400
move give each remaining worker just one
of the partitions and they'll be able to

2146
00:53:10,400 --> 00:53:10,410
of the partitions and they'll be able to
 

2147
00:53:10,410 --> 00:53:13,840
of the partitions and they'll be able to
basically paralyzed the recomputation

2148
00:53:13,840 --> 00:53:13,850
basically paralyzed the recomputation
 

2149
00:53:13,850 --> 00:53:17,300
basically paralyzed the recomputation
that was lost with the failed worker by

2150
00:53:17,300 --> 00:53:17,310
that was lost with the failed worker by
 

2151
00:53:17,310 --> 00:53:19,130
that was lost with the failed worker by
running each of its partitions on a on a

2152
00:53:19,130 --> 00:53:19,140
running each of its partitions on a on a
 

2153
00:53:19,140 --> 00:53:22,130
running each of its partitions on a on a
different worker in parallel so if all

2154
00:53:22,130 --> 00:53:22,140
different worker in parallel so if all
 

2155
00:53:22,140 --> 00:53:24,860
different worker in parallel so if all
else fails spark just goes back to the

2156
00:53:24,860 --> 00:53:24,870
else fails spark just goes back to the
 

2157
00:53:24,870 --> 00:53:27,830
else fails spark just goes back to the
beginning from being input and just

2158
00:53:27,830 --> 00:53:27,840
beginning from being input and just
 

2159
00:53:27,840 --> 00:53:29,300
beginning from being input and just
recomputes everything that was running

2160
00:53:29,300 --> 00:53:29,310
recomputes everything that was running
 

2161
00:53:29,310 --> 00:53:36,980
recomputes everything that was running
on that machine however and for now our

2162
00:53:36,980 --> 00:53:36,990
on that machine however and for now our
 

2163
00:53:36,990 --> 00:53:38,870
on that machine however and for now our
dependencies that's pretty much the end

2164
00:53:38,870 --> 00:53:38,880
dependencies that's pretty much the end
 

2165
00:53:38,880 --> 00:53:39,680
dependencies that's pretty much the end
of the story

2166
00:53:39,680 --> 00:53:39,690
of the story
 

2167
00:53:39,690 --> 00:53:42,020
of the story
however there actually is a problem with

2168
00:53:42,020 --> 00:53:42,030
however there actually is a problem with
 

2169
00:53:42,030 --> 00:53:43,790
however there actually is a problem with
the wide dependencies that makes that

2170
00:53:43,790 --> 00:53:43,800
the wide dependencies that makes that
 

2171
00:53:43,800 --> 00:53:48,200
the wide dependencies that makes that
story not as attractive as you might

2172
00:53:48,200 --> 00:53:48,210
story not as attractive as you might
 

2173
00:53:48,210 --> 00:53:53,570
story not as attractive as you might
hope so this is a topic here is failure

2174
00:53:53,570 --> 00:53:53,580
hope so this is a topic here is failure
 

2175
00:53:53,580 --> 00:54:00,910
hope so this is a topic here is failure
one failed node 1 failed worker

2176
00:54:00,910 --> 00:54:00,920

 

2177
00:54:00,920 --> 00:54:05,150

in a lineage graph that has wide

2178
00:54:05,150 --> 00:54:05,160
in a lineage graph that has wide
 

2179
00:54:05,160 --> 00:54:13,190
in a lineage graph that has wide
dependencies so the a reasonable or a

2180
00:54:13,190 --> 00:54:13,200
dependencies so the a reasonable or a
 

2181
00:54:13,200 --> 00:54:14,720
dependencies so the a reasonable or a
sort of sample graph you might have is

2182
00:54:14,720 --> 00:54:14,730
sort of sample graph you might have is
 

2183
00:54:14,730 --> 00:54:16,789
sort of sample graph you might have is
you know maybe you have a dependency

2184
00:54:16,789 --> 00:54:16,799
you know maybe you have a dependency
 

2185
00:54:16,799 --> 00:54:19,519
you know maybe you have a dependency
graph that's you know starts with some

2186
00:54:19,519 --> 00:54:19,529
graph that's you know starts with some
 

2187
00:54:19,529 --> 00:54:26,539
graph that's you know starts with some
power dependencies but then after a

2188
00:54:26,539 --> 00:54:26,549
power dependencies but then after a
 

2189
00:54:26,549 --> 00:54:29,509
power dependencies but then after a
while you have a wide dependency so you

2190
00:54:29,509 --> 00:54:29,519
while you have a wide dependency so you
 

2191
00:54:29,519 --> 00:54:37,730
while you have a wide dependency so you
got transformations that depend on all

2192
00:54:37,730 --> 00:54:37,740
got transformations that depend on all
 

2193
00:54:37,740 --> 00:54:39,289
got transformations that depend on all
the preceding transformations and then

2194
00:54:39,289 --> 00:54:39,299
the preceding transformations and then
 

2195
00:54:39,299 --> 00:54:44,180
the preceding transformations and then
some small narrow ones all right and you

2196
00:54:44,180 --> 00:54:44,190
some small narrow ones all right and you
 

2197
00:54:44,190 --> 00:54:45,950
some small narrow ones all right and you
know the game is that a single workers

2198
00:54:45,950 --> 00:54:45,960
know the game is that a single workers
 

2199
00:54:45,960 --> 00:54:48,109
know the game is that a single workers
fail and we need to reconstruct the

2200
00:54:48,109 --> 00:54:48,119
fail and we need to reconstruct the
 

2201
00:54:48,119 --> 00:54:50,569
fail and we need to reconstruct the
Maeby's field before we've gone to the

2202
00:54:50,569 --> 00:54:50,579
Maeby's field before we've gone to the
 

2203
00:54:50,579 --> 00:54:55,190
Maeby's field before we've gone to the
final action and produce the output so

2204
00:54:55,190 --> 00:54:55,200
final action and produce the output so
 

2205
00:54:55,200 --> 00:54:57,440
final action and produce the output so
we need to kind of reconstruct recompute

2206
00:54:57,440 --> 00:54:57,450
we need to kind of reconstruct recompute
 

2207
00:54:57,450 --> 00:55:02,210
we need to kind of reconstruct recompute
what was on this field work the the

2208
00:55:02,210 --> 00:55:02,220
what was on this field work the the
 

2209
00:55:02,220 --> 00:55:04,999
what was on this field work the the
damaging thing here is that ordinarily

2210
00:55:04,999 --> 00:55:05,009
damaging thing here is that ordinarily
 

2211
00:55:05,009 --> 00:55:09,349
damaging thing here is that ordinarily
as spark is executing along it you know

2212
00:55:09,349 --> 00:55:09,359
as spark is executing along it you know
 

2213
00:55:09,359 --> 00:55:12,910
as spark is executing along it you know
it executes each of the transformations

2214
00:55:12,910 --> 00:55:12,920
it executes each of the transformations
 

2215
00:55:12,920 --> 00:55:14,690
it executes each of the transformations
gives us output to the next

2216
00:55:14,690 --> 00:55:14,700
gives us output to the next
 

2217
00:55:14,700 --> 00:55:16,370
gives us output to the next
transformation but doesn't hold on to

2218
00:55:16,370 --> 00:55:16,380
transformation but doesn't hold on to
 

2219
00:55:16,380 --> 00:55:18,589
transformation but doesn't hold on to
the original output unless you unless

2220
00:55:18,589 --> 00:55:18,599
the original output unless you unless
 

2221
00:55:18,599 --> 00:55:20,920
the original output unless you unless
you happen to tell it to like the links

2222
00:55:20,920 --> 00:55:20,930
you happen to tell it to like the links
 

2223
00:55:20,930 --> 00:55:24,499
you happen to tell it to like the links
data is persisted with that cache call

2224
00:55:24,499 --> 00:55:24,509
data is persisted with that cache call
 

2225
00:55:24,509 --> 00:55:27,499
data is persisted with that cache call
but in general that data is not held on

2226
00:55:27,499 --> 00:55:27,509
but in general that data is not held on
 

2227
00:55:27,509 --> 00:55:31,059
but in general that data is not held on
to because now if you have a like the

2228
00:55:31,059 --> 00:55:31,069
to because now if you have a like the
 

2229
00:55:31,069 --> 00:55:34,339
to because now if you have a like the
PageRank lineage graph maybe dozens or

2230
00:55:34,339 --> 00:55:34,349
PageRank lineage graph maybe dozens or
 

2231
00:55:34,349 --> 00:55:35,930
PageRank lineage graph maybe dozens or
hundreds of steps long you don't want to

2232
00:55:35,930 --> 00:55:35,940
hundreds of steps long you don't want to
 

2233
00:55:35,940 --> 00:55:37,729
hundreds of steps long you don't want to
hold on to all that data it's way way

2234
00:55:37,729 --> 00:55:37,739
hold on to all that data it's way way
 

2235
00:55:37,739 --> 00:55:40,700
hold on to all that data it's way way
too much to fit in memory so as the

2236
00:55:40,700 --> 00:55:40,710
too much to fit in memory so as the
 

2237
00:55:40,710 --> 00:55:42,170
too much to fit in memory so as the
SPARC sort of moves through these

2238
00:55:42,170 --> 00:55:42,180
SPARC sort of moves through these
 

2239
00:55:42,180 --> 00:55:45,549
SPARC sort of moves through these
transformations it discards all the data

2240
00:55:45,549 --> 00:55:45,559
transformations it discards all the data
 

2241
00:55:45,559 --> 00:55:48,499
transformations it discards all the data
associated with earlier transformations

2242
00:55:48,499 --> 00:55:48,509
associated with earlier transformations
 

2243
00:55:48,509 --> 00:55:50,269
associated with earlier transformations
that means when we get here and if this

2244
00:55:50,269 --> 00:55:50,279
that means when we get here and if this
 

2245
00:55:50,279 --> 00:55:53,930
that means when we get here and if this
worker fails we need to we need to

2246
00:55:53,930 --> 00:55:53,940
worker fails we need to we need to
 

2247
00:55:53,940 --> 00:55:56,630
worker fails we need to we need to
restart its computation on a different

2248
00:55:56,630 --> 00:55:56,640
restart its computation on a different
 

2249
00:55:56,640 --> 00:55:58,789
restart its computation on a different
worker now so we can be the input and

2250
00:55:58,789 --> 00:55:58,799
worker now so we can be the input and
 

2251
00:55:58,799 --> 00:56:01,660
worker now so we can be the input and
maybe do the original narrow

2252
00:56:01,660 --> 00:56:01,670
maybe do the original narrow
 

2253
00:56:01,670 --> 00:56:04,240
maybe do the original narrow
transformations

2254
00:56:04,240 --> 00:56:04,250
transformations
 

2255
00:56:04,250 --> 00:56:05,680
transformations
they just depend on the input which we

2256
00:56:05,680 --> 00:56:05,690
they just depend on the input which we
 

2257
00:56:05,690 --> 00:56:07,240
they just depend on the input which we
have to reread but then if we get to

2258
00:56:07,240 --> 00:56:07,250
have to reread but then if we get to
 

2259
00:56:07,250 --> 00:56:08,650
have to reread but then if we get to
this y transformation we have this

2260
00:56:08,650 --> 00:56:08,660
this y transformation we have this
 

2261
00:56:08,660 --> 00:56:11,470
this y transformation we have this
problem that it requires input not just

2262
00:56:11,470 --> 00:56:11,480
problem that it requires input not just
 

2263
00:56:11,480 --> 00:56:13,960
problem that it requires input not just
from the same partition on the same

2264
00:56:13,960 --> 00:56:13,970
from the same partition on the same
 

2265
00:56:13,970 --> 00:56:15,670
from the same partition on the same
worker but also from every other

2266
00:56:15,670 --> 00:56:15,680
worker but also from every other
 

2267
00:56:15,680 --> 00:56:18,430
worker but also from every other
partition and these workers so they're

2268
00:56:18,430 --> 00:56:18,440
partition and these workers so they're
 

2269
00:56:18,440 --> 00:56:20,829
partition and these workers so they're
still alive have in this example have

2270
00:56:20,829 --> 00:56:20,839
still alive have in this example have
 

2271
00:56:20,839 --> 00:56:23,440
still alive have in this example have
proceeded past this transformation and

2272
00:56:23,440 --> 00:56:23,450
proceeded past this transformation and
 

2273
00:56:23,450 --> 00:56:28,440
proceeded past this transformation and
therefore discarded the output of this

2274
00:56:28,440 --> 00:56:28,450
therefore discarded the output of this
 

2275
00:56:28,450 --> 00:56:31,300
therefore discarded the output of this
transformation since it may have been a

2276
00:56:31,300 --> 00:56:31,310
transformation since it may have been a
 

2277
00:56:31,310 --> 00:56:33,690
transformation since it may have been a
while ago and therefore the input did

2278
00:56:33,690 --> 00:56:33,700
while ago and therefore the input did
 

2279
00:56:33,700 --> 00:56:36,760
while ago and therefore the input did
our recomputation needs from all the

2280
00:56:36,760 --> 00:56:36,770
our recomputation needs from all the
 

2281
00:56:36,770 --> 00:56:39,250
our recomputation needs from all the
other partitions doesn't exist anymore

2282
00:56:39,250 --> 00:56:39,260
other partitions doesn't exist anymore
 

2283
00:56:39,260 --> 00:56:41,829
other partitions doesn't exist anymore
and so if we're not careful that means

2284
00:56:41,829 --> 00:56:41,839
and so if we're not careful that means
 

2285
00:56:41,839 --> 00:56:44,680
and so if we're not careful that means
that in order to rebuild this the

2286
00:56:44,680 --> 00:56:44,690
that in order to rebuild this the
 

2287
00:56:44,690 --> 00:56:46,809
that in order to rebuild this the
computation on this field worker we may

2288
00:56:46,809 --> 00:56:46,819
computation on this field worker we may
 

2289
00:56:46,819 --> 00:56:51,670
computation on this field worker we may
in fact have to re execute this part of

2290
00:56:51,670 --> 00:56:51,680
in fact have to re execute this part of
 

2291
00:56:51,680 --> 00:56:55,030
in fact have to re execute this part of
every other worker as well as well as

2292
00:56:55,030 --> 00:56:55,040
every other worker as well as well as
 

2293
00:56:55,040 --> 00:56:58,240
every other worker as well as well as
the entire lineage graph on the failed

2294
00:56:58,240 --> 00:56:58,250
the entire lineage graph on the failed
 

2295
00:56:58,250 --> 00:57:01,240
the entire lineage graph on the failed
worker and so this could be very

2296
00:57:01,240 --> 00:57:01,250
worker and so this could be very
 

2297
00:57:01,250 --> 00:57:02,829
worker and so this could be very
damaging right if we're talking about oh

2298
00:57:02,829 --> 00:57:02,839
damaging right if we're talking about oh
 

2299
00:57:02,839 --> 00:57:05,079
damaging right if we're talking about oh
I mean I've been running this giant

2300
00:57:05,079 --> 00:57:05,089
I mean I've been running this giant
 

2301
00:57:05,089 --> 00:57:07,480
I mean I've been running this giant
spark job for a day and then one of a

2302
00:57:07,480 --> 00:57:07,490
spark job for a day and then one of a
 

2303
00:57:07,490 --> 00:57:10,359
spark job for a day and then one of a
thousand machines fails that may mean we

2304
00:57:10,359 --> 00:57:10,369
thousand machines fails that may mean we
 

2305
00:57:10,369 --> 00:57:12,430
thousand machines fails that may mean we
have to we know anything more clever

2306
00:57:12,430 --> 00:57:12,440
have to we know anything more clever
 

2307
00:57:12,440 --> 00:57:13,960
have to we know anything more clever
than this that we have to go back to the

2308
00:57:13,960 --> 00:57:13,970
than this that we have to go back to the
 

2309
00:57:13,970 --> 00:57:15,160
than this that we have to go back to the
very beginning on every one of the

2310
00:57:15,160 --> 00:57:15,170
very beginning on every one of the
 

2311
00:57:15,170 --> 00:57:18,670
very beginning on every one of the
workers and recompute the whole thing

2312
00:57:18,670 --> 00:57:18,680
workers and recompute the whole thing
 

2313
00:57:18,680 --> 00:57:21,520
workers and recompute the whole thing
from scratch no it's gonna be the same

2314
00:57:21,520 --> 00:57:21,530
from scratch no it's gonna be the same
 

2315
00:57:21,530 --> 00:57:22,720
from scratch no it's gonna be the same
amount of work is going to take the same

2316
00:57:22,720 --> 00:57:22,730
amount of work is going to take the same
 

2317
00:57:22,730 --> 00:57:27,099
amount of work is going to take the same
day to recompute a day's computation so

2318
00:57:27,099 --> 00:57:27,109
day to recompute a day's computation so
 

2319
00:57:27,109 --> 00:57:30,160
day to recompute a day's computation so
this would be unacceptable we'd really

2320
00:57:30,160 --> 00:57:30,170
this would be unacceptable we'd really
 

2321
00:57:30,170 --> 00:57:32,109
this would be unacceptable we'd really
like it so that if if one worker out of

2322
00:57:32,109 --> 00:57:32,119
like it so that if if one worker out of
 

2323
00:57:32,119 --> 00:57:33,670
like it so that if if one worker out of
a thousand crashes that we have to do

2324
00:57:33,670 --> 00:57:33,680
a thousand crashes that we have to do
 

2325
00:57:33,680 --> 00:57:36,790
a thousand crashes that we have to do
relatively little work to recover from

2326
00:57:36,790 --> 00:57:36,800
relatively little work to recover from
 

2327
00:57:36,800 --> 00:57:42,550
relatively little work to recover from
that and because of that spark allows

2328
00:57:42,550 --> 00:57:42,560
that and because of that spark allows
 

2329
00:57:42,560 --> 00:57:46,120
that and because of that spark allows
you to check point to make periodic

2330
00:57:46,120 --> 00:57:46,130
you to check point to make periodic
 

2331
00:57:46,130 --> 00:57:48,550
you to check point to make periodic
check points of specific transformation

2332
00:57:48,550 --> 00:57:48,560
check points of specific transformation
 

2333
00:57:48,560 --> 00:57:52,630
check points of specific transformation
so um so in this graph what we would do

2334
00:57:52,630 --> 00:57:52,640
so um so in this graph what we would do
 

2335
00:57:52,640 --> 00:57:57,250
so um so in this graph what we would do
is in the scallop program we would call

2336
00:57:57,250 --> 00:57:57,260
is in the scallop program we would call
 

2337
00:57:57,260 --> 00:57:59,589
is in the scallop program we would call
I think it's the persist call actually

2338
00:57:59,589 --> 00:57:59,599
I think it's the persist call actually
 

2339
00:57:59,599 --> 00:58:00,910
I think it's the persist call actually
we call the persist call with a special

2340
00:58:00,910 --> 00:58:00,920
we call the persist call with a special
 

2341
00:58:00,920 --> 00:58:04,720
we call the persist call with a special
argument that says look after you

2342
00:58:04,720 --> 00:58:04,730
argument that says look after you
 

2343
00:58:04,730 --> 00:58:06,510
argument that says look after you
compute the output of this

2344
00:58:06,510 --> 00:58:06,520
compute the output of this
 

2345
00:58:06,520 --> 00:58:09,760
compute the output of this
transformation please save the output to

2346
00:58:09,760 --> 00:58:09,770
transformation please save the output to
 

2347
00:58:09,770 --> 00:58:11,760
transformation please save the output to
HDFS

2348
00:58:11,760 --> 00:58:11,770
HDFS
 

2349
00:58:11,770 --> 00:58:14,740
HDFS
and so everything and then if something

2350
00:58:14,740 --> 00:58:14,750
and so everything and then if something
 

2351
00:58:14,750 --> 00:58:18,550
and so everything and then if something
fails the spark will know that aha the

2352
00:58:18,550 --> 00:58:18,560
fails the spark will know that aha the
 

2353
00:58:18,560 --> 00:58:21,400
fails the spark will know that aha the
output of the proceeding transformation

2354
00:58:21,400 --> 00:58:21,410
output of the proceeding transformation
 

2355
00:58:21,410 --> 00:58:24,670
output of the proceeding transformation
was safe th th d fs and so we just have

2356
00:58:24,670 --> 00:58:24,680
was safe th th d fs and so we just have
 

2357
00:58:24,680 --> 00:58:28,500
was safe th th d fs and so we just have
to read it from each DFS instead of

2358
00:58:28,500 --> 00:58:28,510
to read it from each DFS instead of
 

2359
00:58:28,510 --> 00:58:30,760
to read it from each DFS instead of
recomputing it on all for all partitions

2360
00:58:30,760 --> 00:58:30,770
recomputing it on all for all partitions
 

2361
00:58:30,770 --> 00:58:34,270
recomputing it on all for all partitions
back to the beginning of time um and

2362
00:58:34,270 --> 00:58:34,280
back to the beginning of time um and
 

2363
00:58:34,280 --> 00:58:36,250
back to the beginning of time um and
because HDFS is a separate storage

2364
00:58:36,250 --> 00:58:36,260
because HDFS is a separate storage
 

2365
00:58:36,260 --> 00:58:38,560
because HDFS is a separate storage
system which is itself replicated in

2366
00:58:38,560 --> 00:58:38,570
system which is itself replicated in
 

2367
00:58:38,570 --> 00:58:40,240
system which is itself replicated in
fault-tolerant the fact that one worker

2368
00:58:40,240 --> 00:58:40,250
fault-tolerant the fact that one worker
 

2369
00:58:40,250 --> 00:58:43,180
fault-tolerant the fact that one worker
fails you know the HDFS is still going

2370
00:58:43,180 --> 00:58:43,190
fails you know the HDFS is still going
 

2371
00:58:43,190 --> 00:58:49,680
fails you know the HDFS is still going
to be available even if a worker fails

2372
00:58:49,680 --> 00:58:49,690

 

2373
00:58:49,690 --> 00:58:55,380

so I think so for our example PageRank I

2374
00:58:55,380 --> 00:58:55,390
so I think so for our example PageRank I
 

2375
00:58:55,390 --> 00:58:59,470
so I think so for our example PageRank I
think what would be traditional would be

2376
00:58:59,470 --> 00:58:59,480
think what would be traditional would be
 

2377
00:58:59,480 --> 00:59:02,340
think what would be traditional would be
to tell

2378
00:59:02,340 --> 00:59:02,350
to tell
 

2379
00:59:02,350 --> 00:59:06,400
to tell
spark to check point the output to check

2380
00:59:06,400 --> 00:59:06,410
spark to check point the output to check
 

2381
00:59:06,410 --> 00:59:08,710
spark to check point the output to check
put ranks and you wouldn't even know you

2382
00:59:08,710 --> 00:59:08,720
put ranks and you wouldn't even know you
 

2383
00:59:08,720 --> 00:59:10,330
put ranks and you wouldn't even know you
can tell it to only check point

2384
00:59:10,330 --> 00:59:10,340
can tell it to only check point
 

2385
00:59:10,340 --> 00:59:12,490
can tell it to only check point
periodically so you know if you're gonna

2386
00:59:12,490 --> 00:59:12,500
periodically so you know if you're gonna
 

2387
00:59:12,500 --> 00:59:16,000
periodically so you know if you're gonna
run this thing for 100 iterations it

2388
00:59:16,000 --> 00:59:16,010
run this thing for 100 iterations it
 

2389
00:59:16,010 --> 00:59:18,400
run this thing for 100 iterations it
actually takes a fair amount of time to

2390
00:59:18,400 --> 00:59:18,410
actually takes a fair amount of time to
 

2391
00:59:18,410 --> 00:59:24,010
actually takes a fair amount of time to
save the entire ranks to HDFS because

2392
00:59:24,010 --> 00:59:24,020
save the entire ranks to HDFS because
 

2393
00:59:24,020 --> 00:59:25,600
save the entire ranks to HDFS because
again we're talking about terabytes of

2394
00:59:25,600 --> 00:59:25,610
again we're talking about terabytes of
 

2395
00:59:25,610 --> 00:59:28,240
again we're talking about terabytes of
data in total so maybe we would we can

2396
00:59:28,240 --> 00:59:28,250
data in total so maybe we would we can
 

2397
00:59:28,250 --> 00:59:31,720
data in total so maybe we would we can
tell SPARC look only check point ranks

2398
00:59:31,720 --> 00:59:31,730
tell SPARC look only check point ranks
 

2399
00:59:31,730 --> 00:59:38,140
tell SPARC look only check point ranks
to HDFS every every 10th iteration or

2400
00:59:38,140 --> 00:59:38,150
to HDFS every every 10th iteration or
 

2401
00:59:38,150 --> 00:59:40,630
to HDFS every every 10th iteration or
something to limit the expanse although

2402
00:59:40,630 --> 00:59:40,640
something to limit the expanse although
 

2403
00:59:40,640 --> 00:59:42,520
something to limit the expanse although
you know it's a trade-off between the

2404
00:59:42,520 --> 00:59:42,530
you know it's a trade-off between the
 

2405
00:59:42,530 --> 00:59:44,860
you know it's a trade-off between the
expensive repeatedly saving stuff to

2406
00:59:44,860 --> 00:59:44,870
expensive repeatedly saving stuff to
 

2407
00:59:44,870 --> 00:59:48,280
expensive repeatedly saving stuff to
disk and how much of a cost if a worker

2408
00:59:48,280 --> 00:59:48,290
disk and how much of a cost if a worker
 

2409
00:59:48,290 --> 00:59:55,620
disk and how much of a cost if a worker
failed you had to go back and redo it

2410
00:59:55,620 --> 00:59:55,630

 

2411
00:59:55,630 --> 00:59:59,900

Bertha's a question when we call

2412
00:59:59,900 --> 00:59:59,910
Bertha's a question when we call
 

2413
00:59:59,910 --> 01:00:02,210
Bertha's a question when we call
that does act as a checkpoint you know

2414
01:00:02,210 --> 01:00:02,220
that does act as a checkpoint you know
 

2415
01:00:02,220 --> 01:00:03,800
that does act as a checkpoint you know
okay so this is a very good question

2416
01:00:03,800 --> 01:00:03,810
okay so this is a very good question
 

2417
01:00:03,810 --> 01:00:05,480
okay so this is a very good question
which I don't know the answer to the

2418
01:00:05,480 --> 01:00:05,490
which I don't know the answer to the
 

2419
01:00:05,490 --> 01:00:08,090
which I don't know the answer to the
observation is that we could call cash

2420
01:00:08,090 --> 01:00:08,100
observation is that we could call cash
 

2421
01:00:08,100 --> 01:00:10,880
observation is that we could call cash
here and we do call cashier and we could

2422
01:00:10,880 --> 01:00:10,890
here and we do call cashier and we could
 

2423
01:00:10,890 --> 01:00:14,000
here and we do call cashier and we could
call cashier and the usual use of cash

2424
01:00:14,000 --> 01:00:14,010
call cashier and the usual use of cash
 

2425
01:00:14,010 --> 01:00:18,950
call cashier and the usual use of cash
is just to save data in memory with the

2426
01:00:18,950 --> 01:00:18,960
is just to save data in memory with the
 

2427
01:00:18,960 --> 01:00:21,530
is just to save data in memory with the
intent to reuse it that's certainly why

2428
01:00:21,530 --> 01:00:21,540
intent to reuse it that's certainly why
 

2429
01:00:21,540 --> 01:00:22,730
intent to reuse it that's certainly why
it's being called here because we're

2430
01:00:22,730 --> 01:00:22,740
it's being called here because we're
 

2431
01:00:22,740 --> 01:00:26,480
it's being called here because we're
using links for but in my example it

2432
01:00:26,480 --> 01:00:26,490
using links for but in my example it
 

2433
01:00:26,490 --> 01:00:30,680
using links for but in my example it
would also have the effect of making the

2434
01:00:30,680 --> 01:00:30,690
would also have the effect of making the
 

2435
01:00:30,690 --> 01:00:32,390
would also have the effect of making the
output of this stage available in memory

2436
01:00:32,390 --> 01:00:32,400
output of this stage available in memory
 

2437
01:00:32,400 --> 01:00:34,970
output of this stage available in memory
although not on not an HDFS but in the

2438
01:00:34,970 --> 01:00:34,980
although not on not an HDFS but in the
 

2439
01:00:34,980 --> 01:00:39,020
although not on not an HDFS but in the
memory of these workers and the paper

2440
01:00:39,020 --> 01:00:39,030
memory of these workers and the paper
 

2441
01:00:39,030 --> 01:00:45,350
memory of these workers and the paper
never talks about this possibility and

2442
01:00:45,350 --> 01:00:45,360
never talks about this possibility and
 

2443
01:00:45,360 --> 01:00:46,910
never talks about this possibility and
I'm not really sure what's going on

2444
01:00:46,910 --> 01:00:46,920
I'm not really sure what's going on
 

2445
01:00:46,920 --> 01:00:49,880
I'm not really sure what's going on
maybe that would work or maybe the fact

2446
01:00:49,880 --> 01:00:49,890
maybe that would work or maybe the fact
 

2447
01:00:49,890 --> 01:00:52,970
maybe that would work or maybe the fact
that the cash requests are merely

2448
01:00:52,970 --> 01:00:52,980
that the cash requests are merely
 

2449
01:00:52,980 --> 01:00:56,450
that the cash requests are merely
advisory and maybe evicted if the

2450
01:00:56,450 --> 01:00:56,460
advisory and maybe evicted if the
 

2451
01:00:56,460 --> 01:00:59,470
advisory and maybe evicted if the
workers run out of space means that

2452
01:00:59,470 --> 01:00:59,480
workers run out of space means that
 

2453
01:00:59,480 --> 01:01:01,730
workers run out of space means that
calling cash doesn't give you it isn't

2454
01:01:01,730 --> 01:01:01,740
calling cash doesn't give you it isn't
 

2455
01:01:01,740 --> 01:01:04,850
calling cash doesn't give you it isn't
like a reliable directed to make sure

2456
01:01:04,850 --> 01:01:04,860
like a reliable directed to make sure
 

2457
01:01:04,860 --> 01:01:06,650
like a reliable directed to make sure
the data really is available it's just

2458
01:01:06,650 --> 01:01:06,660
the data really is available it's just
 

2459
01:01:06,660 --> 01:01:08,930
the data really is available it's just
well it'll probably be available on most

2460
01:01:08,930 --> 01:01:08,940
well it'll probably be available on most
 

2461
01:01:08,940 --> 01:01:10,430
well it'll probably be available on most
nodes but not all nodes because remember

2462
01:01:10,430 --> 01:01:10,440
nodes but not all nodes because remember
 

2463
01:01:10,440 --> 01:01:16,670
nodes but not all nodes because remember
even a single node loses its data and

2464
01:01:16,670 --> 01:01:16,680
even a single node loses its data and
 

2465
01:01:16,680 --> 01:01:17,750
even a single node loses its data and
we're gonna have to do a bunch of

2466
01:01:17,750 --> 01:01:17,760
we're gonna have to do a bunch of
 

2467
01:01:17,760 --> 01:01:22,270
we're gonna have to do a bunch of
recomputation so III I'm guessing that

2468
01:01:22,270 --> 01:01:22,280
recomputation so III I'm guessing that
 

2469
01:01:22,280 --> 01:01:25,910
recomputation so III I'm guessing that
persists with replication is a firm

2470
01:01:25,910 --> 01:01:25,920
persists with replication is a firm
 

2471
01:01:25,920 --> 01:01:28,220
persists with replication is a firm
directive to guarantee that the data

2472
01:01:28,220 --> 01:01:28,230
directive to guarantee that the data
 

2473
01:01:28,230 --> 01:01:29,840
directive to guarantee that the data
will be available even if there's a

2474
01:01:29,840 --> 01:01:29,850
will be available even if there's a
 

2475
01:01:29,850 --> 01:01:30,200
will be available even if there's a
failure

2476
01:01:30,200 --> 01:01:30,210
failure
 

2477
01:01:30,210 --> 01:01:40,100
failure
I don't really know it's a good question

2478
01:01:40,100 --> 01:01:40,110

 

2479
01:01:40,110 --> 01:01:46,640

alright okay so that's the programming

2480
01:01:46,640 --> 01:01:46,650
alright okay so that's the programming
 

2481
01:01:46,650 --> 01:01:48,620
alright okay so that's the programming
model and the execution model and the

2482
01:01:48,620 --> 01:01:48,630
model and the execution model and the
 

2483
01:01:48,630 --> 01:01:52,420
model and the execution model and the
failure strategy and by the way just a

2484
01:01:52,420 --> 01:01:52,430
failure strategy and by the way just a
 

2485
01:01:52,430 --> 01:01:54,590
failure strategy and by the way just a
beat on the failure strategy a little

2486
01:01:54,590 --> 01:01:54,600
beat on the failure strategy a little
 

2487
01:01:54,600 --> 01:01:56,810
beat on the failure strategy a little
bit more the way these systems do

2488
01:01:56,810 --> 01:01:56,820
bit more the way these systems do
 

2489
01:01:56,820 --> 01:02:00,440
bit more the way these systems do
failure recovery is it's not a minor

2490
01:02:00,440 --> 01:02:00,450
failure recovery is it's not a minor
 

2491
01:02:00,450 --> 01:02:04,430
failure recovery is it's not a minor
thing as as people build bigger and

2492
01:02:04,430 --> 01:02:04,440
thing as as people build bigger and
 

2493
01:02:04,440 --> 01:02:06,410
thing as as people build bigger and
bigger clusters with thousands and

2494
01:02:06,410 --> 01:02:06,420
bigger clusters with thousands and
 

2495
01:02:06,420 --> 01:02:08,420
bigger clusters with thousands and
thousands of machines you know the

2496
01:02:08,420 --> 01:02:08,430
thousands of machines you know the
 

2497
01:02:08,430 --> 01:02:10,010
thousands of machines you know the
probability that job will be interrupted

2498
01:02:10,010 --> 01:02:10,020
probability that job will be interrupted
 

2499
01:02:10,020 --> 01:02:13,190
probability that job will be interrupted
by at least one worker failure it really

2500
01:02:13,190 --> 01:02:13,200
by at least one worker failure it really
 

2501
01:02:13,200 --> 01:02:16,690
by at least one worker failure it really
does start to approach one and so the

2502
01:02:16,690 --> 01:02:16,700
does start to approach one and so the
 

2503
01:02:16,700 --> 01:02:20,030
does start to approach one and so the
the designs recent designs intended to

2504
01:02:20,030 --> 01:02:20,040
the designs recent designs intended to
 

2505
01:02:20,040 --> 01:02:22,730
the designs recent designs intended to
run on big clusters have really been to

2506
01:02:22,730 --> 01:02:22,740
run on big clusters have really been to
 

2507
01:02:22,740 --> 01:02:25,910
run on big clusters have really been to
a great extent dominated by the failure

2508
01:02:25,910 --> 01:02:25,920
a great extent dominated by the failure
 

2509
01:02:25,920 --> 01:02:28,160
a great extent dominated by the failure
recovery strategy and that's for example

2510
01:02:28,160 --> 01:02:28,170
recovery strategy and that's for example
 

2511
01:02:28,170 --> 01:02:31,670
recovery strategy and that's for example
a lot of the explanation for why SPARC

2512
01:02:31,670 --> 01:02:31,680
a lot of the explanation for why SPARC
 

2513
01:02:31,680 --> 01:02:35,230
a lot of the explanation for why SPARC
insists that the transformations be

2514
01:02:35,230 --> 01:02:35,240
insists that the transformations be
 

2515
01:02:35,240 --> 01:02:39,770
insists that the transformations be
deterministic and why the are these its

2516
01:02:39,770 --> 01:02:39,780
deterministic and why the are these its
 

2517
01:02:39,780 --> 01:02:44,330
deterministic and why the are these its
rdd's are immutable because you know

2518
01:02:44,330 --> 01:02:44,340
rdd's are immutable because you know
 

2519
01:02:44,340 --> 01:02:47,810
rdd's are immutable because you know
that's what allows it to recover from

2520
01:02:47,810 --> 01:02:47,820
that's what allows it to recover from
 

2521
01:02:47,820 --> 01:02:49,790
that's what allows it to recover from
failure by simply recomputing one

2522
01:02:49,790 --> 01:02:49,800
failure by simply recomputing one
 

2523
01:02:49,800 --> 01:02:51,560
failure by simply recomputing one
partition instead of having to start the

2524
01:02:51,560 --> 01:02:51,570
partition instead of having to start the
 

2525
01:02:51,570 --> 01:02:53,740
partition instead of having to start the
entire computation from scratch and

2526
01:02:53,740 --> 01:02:53,750
entire computation from scratch and
 

2527
01:02:53,750 --> 01:02:56,450
entire computation from scratch and
there have been in the past plenty of

2528
01:02:56,450 --> 01:02:56,460
there have been in the past plenty of
 

2529
01:02:56,460 --> 01:02:59,840
there have been in the past plenty of
proposed sort of cluster big data

2530
01:02:59,840 --> 01:02:59,850
proposed sort of cluster big data
 

2531
01:02:59,850 --> 01:03:02,480
proposed sort of cluster big data
execution models in which there really

2532
01:03:02,480 --> 01:03:02,490
execution models in which there really
 

2533
01:03:02,490 --> 01:03:03,920
execution models in which there really
was mutable data and in which

2534
01:03:03,920 --> 01:03:03,930
was mutable data and in which
 

2535
01:03:03,930 --> 01:03:06,320
was mutable data and in which
computations could be non-deterministic

2536
01:03:06,320 --> 01:03:06,330
computations could be non-deterministic
 

2537
01:03:06,330 --> 01:03:08,120
computations could be non-deterministic
make if you look up distributed shared

2538
01:03:08,120 --> 01:03:08,130
make if you look up distributed shared
 

2539
01:03:08,130 --> 01:03:10,850
make if you look up distributed shared
memory systems those all support mutable

2540
01:03:10,850 --> 01:03:10,860
memory systems those all support mutable
 

2541
01:03:10,860 --> 01:03:14,330
memory systems those all support mutable
data and they support non-deterministic

2542
01:03:14,330 --> 01:03:14,340
data and they support non-deterministic
 

2543
01:03:14,340 --> 01:03:18,080
data and they support non-deterministic
execution but because of that they tend

2544
01:03:18,080 --> 01:03:18,090
execution but because of that they tend
 

2545
01:03:18,090 --> 01:03:20,470
execution but because of that they tend
not to have a good failure strategy so

2546
01:03:20,470 --> 01:03:20,480
not to have a good failure strategy so
 

2547
01:03:20,480 --> 01:03:22,850
not to have a good failure strategy so
you know thirty years ago when a big

2548
01:03:22,850 --> 01:03:22,860
you know thirty years ago when a big
 

2549
01:03:22,860 --> 01:03:25,400
you know thirty years ago when a big
cluster was for computers none of this

2550
01:03:25,400 --> 01:03:25,410
cluster was for computers none of this
 

2551
01:03:25,410 --> 01:03:26,960
cluster was for computers none of this
mattered because the failure probability

2552
01:03:26,960 --> 01:03:26,970
mattered because the failure probability
 

2553
01:03:26,970 --> 01:03:29,240
mattered because the failure probability
was little very low and so many

2554
01:03:29,240 --> 01:03:29,250
was little very low and so many
 

2555
01:03:29,250 --> 01:03:32,980
was little very low and so many
different kinds of computation models

2556
01:03:32,980 --> 01:03:32,990
different kinds of computation models
 

2557
01:03:32,990 --> 01:03:36,230
different kinds of computation models
seemed reasonable then but as the

2558
01:03:36,230 --> 01:03:36,240
seemed reasonable then but as the
 

2559
01:03:36,240 --> 01:03:37,700
seemed reasonable then but as the
clusters have grown to be hundreds and

2560
01:03:37,700 --> 01:03:37,710
clusters have grown to be hundreds and
 

2561
01:03:37,710 --> 01:03:41,590
clusters have grown to be hundreds and
thousands of workers really the only

2562
01:03:41,590 --> 01:03:41,600
thousands of workers really the only
 

2563
01:03:41,600 --> 01:03:44,120
thousands of workers really the only
models that have survived are ones for

2564
01:03:44,120 --> 01:03:44,130
models that have survived are ones for
 

2565
01:03:44,130 --> 01:03:47,390
models that have survived are ones for
which you can devise a very efficient to

2566
01:03:47,390 --> 01:03:47,400
which you can devise a very efficient to
 

2567
01:03:47,400 --> 01:03:49,340
which you can devise a very efficient to
failure recovery strategy that does not

2568
01:03:49,340 --> 01:03:49,350
failure recovery strategy that does not
 

2569
01:03:49,350 --> 01:03:52,880
failure recovery strategy that does not
require backing all the way up to the

2570
01:03:52,880 --> 01:03:52,890
require backing all the way up to the
 

2571
01:03:52,890 --> 01:03:53,720
require backing all the way up to the
beginning

2572
01:03:53,720 --> 01:03:53,730
beginning
 

2573
01:03:53,730 --> 01:03:56,150
beginning
and restarting the paper talks about

2574
01:03:56,150 --> 01:03:56,160
and restarting the paper talks about
 

2575
01:03:56,160 --> 01:03:57,740
and restarting the paper talks about
this a little bit when it's criticizing

2576
01:03:57,740 --> 01:03:57,750
this a little bit when it's criticizing
 

2577
01:03:57,750 --> 01:04:01,130
this a little bit when it's criticizing
I'm distributed shared memory and it's a

2578
01:04:01,130 --> 01:04:01,140
I'm distributed shared memory and it's a
 

2579
01:04:01,140 --> 01:04:05,890
I'm distributed shared memory and it's a
very valid criticism I bet it's a big

2580
01:04:05,890 --> 01:04:05,900
very valid criticism I bet it's a big
 

2581
01:04:05,900 --> 01:04:14,020
very valid criticism I bet it's a big
design constraint okay so the sparks not

2582
01:04:14,020 --> 01:04:14,030
design constraint okay so the sparks not
 

2583
01:04:14,030 --> 01:04:17,210
design constraint okay so the sparks not
perfect for all kinds of processing it's

2584
01:04:17,210 --> 01:04:17,220
perfect for all kinds of processing it's
 

2585
01:04:17,220 --> 01:04:19,630
perfect for all kinds of processing it's
really geared up for batch processing of

2586
01:04:19,630 --> 01:04:19,640
really geared up for batch processing of
 

2587
01:04:19,640 --> 01:04:23,870
really geared up for batch processing of
giant amounts of data bulk bulk data

2588
01:04:23,870 --> 01:04:23,880
giant amounts of data bulk bulk data
 

2589
01:04:23,880 --> 01:04:25,430
giant amounts of data bulk bulk data
processing so if you have terabytes of

2590
01:04:25,430 --> 01:04:25,440
processing so if you have terabytes of
 

2591
01:04:25,440 --> 01:04:27,650
processing so if you have terabytes of
data and you want to you know chew away

2592
01:04:27,650 --> 01:04:27,660
data and you want to you know chew away
 

2593
01:04:27,660 --> 01:04:31,490
data and you want to you know chew away
on it for for a couple hours smart great

2594
01:04:31,490 --> 01:04:31,500
on it for for a couple hours smart great
 

2595
01:04:31,500 --> 01:04:34,280
on it for for a couple hours smart great
if you're running a bank and you need to

2596
01:04:34,280 --> 01:04:34,290
if you're running a bank and you need to
 

2597
01:04:34,290 --> 01:04:37,520
if you're running a bank and you need to
process bank transfers or people's

2598
01:04:37,520 --> 01:04:37,530
process bank transfers or people's
 

2599
01:04:37,530 --> 01:04:40,040
process bank transfers or people's
balance queries then SPARC is just not

2600
01:04:40,040 --> 01:04:40,050
balance queries then SPARC is just not
 

2601
01:04:40,050 --> 01:04:43,640
balance queries then SPARC is just not
relevant to that kind of processing

2602
01:04:43,640 --> 01:04:43,650
relevant to that kind of processing
 

2603
01:04:43,650 --> 01:04:45,590
relevant to that kind of processing
known or to sort of typical websites

2604
01:04:45,590 --> 01:04:45,600
known or to sort of typical websites
 

2605
01:04:45,600 --> 01:04:48,109
known or to sort of typical websites
where I log into you know I access

2606
01:04:48,109 --> 01:04:48,119
where I log into you know I access
 

2607
01:04:48,119 --> 01:04:52,250
where I log into you know I access
Amazon and I want to order some paper

2608
01:04:52,250 --> 01:04:52,260
Amazon and I want to order some paper
 

2609
01:04:52,260 --> 01:04:53,870
Amazon and I want to order some paper
towels and put them into my shopping

2610
01:04:53,870 --> 01:04:53,880
towels and put them into my shopping
 

2611
01:04:53,880 --> 01:04:55,660
towels and put them into my shopping
cart SPARC is not going to help you

2612
01:04:55,660 --> 01:04:55,670
cart SPARC is not going to help you
 

2613
01:04:55,670 --> 01:04:58,670
cart SPARC is not going to help you
maintain this part the shopping cart

2614
01:04:58,670 --> 01:04:58,680
maintain this part the shopping cart
 

2615
01:04:58,680 --> 01:05:00,470
maintain this part the shopping cart
SPARC may be useful for analyzing your

2616
01:05:00,470 --> 01:05:00,480
SPARC may be useful for analyzing your
 

2617
01:05:00,480 --> 01:05:03,790
SPARC may be useful for analyzing your
customers buying habits sort of offline

2618
01:05:03,790 --> 01:05:03,800
customers buying habits sort of offline
 

2619
01:05:03,800 --> 01:05:07,390
customers buying habits sort of offline
but not for sort of online processing

2620
01:05:07,390 --> 01:05:07,400
but not for sort of online processing
 

2621
01:05:07,400 --> 01:05:11,359
but not for sort of online processing
the other sort of kind of a little more

2622
01:05:11,359 --> 01:05:11,369
the other sort of kind of a little more
 

2623
01:05:11,369 --> 01:05:14,240
the other sort of kind of a little more
close to home situation that spark in

2624
01:05:14,240 --> 01:05:14,250
close to home situation that spark in
 

2625
01:05:14,250 --> 01:05:15,890
close to home situation that spark in
the papers not so great at is stream

2626
01:05:15,890 --> 01:05:15,900
the papers not so great at is stream
 

2627
01:05:15,900 --> 01:05:18,349
the papers not so great at is stream
processing i SPARC definitely assumes

2628
01:05:18,349 --> 01:05:18,359
processing i SPARC definitely assumes
 

2629
01:05:18,359 --> 01:05:19,780
processing i SPARC definitely assumes
that all the input is already available

2630
01:05:19,780 --> 01:05:19,790
that all the input is already available
 

2631
01:05:19,790 --> 01:05:22,580
that all the input is already available
but in many situations the input that

2632
01:05:22,580 --> 01:05:22,590
but in many situations the input that
 

2633
01:05:22,590 --> 01:05:26,000
but in many situations the input that
people have is really a stream of input

2634
01:05:26,000 --> 01:05:26,010
people have is really a stream of input
 

2635
01:05:26,010 --> 01:05:28,670
people have is really a stream of input
like they're logging all user clicks on

2636
01:05:28,670 --> 01:05:28,680
like they're logging all user clicks on
 

2637
01:05:28,680 --> 01:05:30,170
like they're logging all user clicks on
their web sites and they want to analyze

2638
01:05:30,170 --> 01:05:30,180
their web sites and they want to analyze
 

2639
01:05:30,180 --> 01:05:32,359
their web sites and they want to analyze
them to understand user behavior you

2640
01:05:32,359 --> 01:05:32,369
them to understand user behavior you
 

2641
01:05:32,369 --> 01:05:35,090
them to understand user behavior you
know it's not a kind of fixed amount of

2642
01:05:35,090 --> 01:05:35,100
know it's not a kind of fixed amount of
 

2643
01:05:35,100 --> 01:05:36,730
know it's not a kind of fixed amount of
data is really a stream of input data

2644
01:05:36,730 --> 01:05:36,740
data is really a stream of input data
 

2645
01:05:36,740 --> 01:05:40,460
data is really a stream of input data
and you know SPARC as in describing the

2646
01:05:40,460 --> 01:05:40,470
and you know SPARC as in describing the
 

2647
01:05:40,470 --> 01:05:42,500
and you know SPARC as in describing the
paper doesn't really have anything to

2648
01:05:42,500 --> 01:05:42,510
paper doesn't really have anything to
 

2649
01:05:42,510 --> 01:05:46,099
paper doesn't really have anything to
say about processing streams of data but

2650
01:05:46,099 --> 01:05:46,109
say about processing streams of data but
 

2651
01:05:46,109 --> 01:05:47,510
say about processing streams of data but
it turned out to be quite close to home

2652
01:05:47,510 --> 01:05:47,520
it turned out to be quite close to home
 

2653
01:05:47,520 --> 01:05:51,470
it turned out to be quite close to home
for people who like to use spark and and

2654
01:05:51,470 --> 01:05:51,480
for people who like to use spark and and
 

2655
01:05:51,480 --> 01:05:52,880
for people who like to use spark and and
now there's a variant of SPARC called

2656
01:05:52,880 --> 01:05:52,890
now there's a variant of SPARC called
 

2657
01:05:52,890 --> 01:05:54,800
now there's a variant of SPARC called
spark streaming that that is a little

2658
01:05:54,800 --> 01:05:54,810
spark streaming that that is a little
 

2659
01:05:54,810 --> 01:05:57,410
spark streaming that that is a little
more geared up to kind of processing

2660
01:05:57,410 --> 01:05:57,420
more geared up to kind of processing
 

2661
01:05:57,420 --> 01:05:59,540
more geared up to kind of processing
data as it arrives and you know sort of

2662
01:05:59,540 --> 01:05:59,550
data as it arrives and you know sort of
 

2663
01:05:59,550 --> 01:06:01,490
data as it arrives and you know sort of
breaks it up into smaller batches and

2664
01:06:01,490 --> 01:06:01,500
breaks it up into smaller batches and
 

2665
01:06:01,500 --> 01:06:05,380
breaks it up into smaller batches and
runs in a batch at a time to spark

2666
01:06:05,380 --> 01:06:05,390
runs in a batch at a time to spark
 

2667
01:06:05,390 --> 01:06:07,670
runs in a batch at a time to spark
so it's good for a lot of bad stuff but

2668
01:06:07,670 --> 01:06:07,680
so it's good for a lot of bad stuff but
 

2669
01:06:07,680 --> 01:06:10,910
so it's good for a lot of bad stuff but
that's certainly on to be thing right to

2670
01:06:10,910 --> 01:06:10,920
that's certainly on to be thing right to
 

2671
01:06:10,920 --> 01:06:13,610
that's certainly on to be thing right to
wrap up the UH you should view spark as

2672
01:06:13,610 --> 01:06:13,620
wrap up the UH you should view spark as
 

2673
01:06:13,620 --> 01:06:16,910
wrap up the UH you should view spark as
a kind of evolution after MapReduce and

2674
01:06:16,910 --> 01:06:16,920
a kind of evolution after MapReduce and
 

2675
01:06:16,920 --> 01:06:19,930
a kind of evolution after MapReduce and
I may fix some expressivity and

2676
01:06:19,930 --> 01:06:19,940
I may fix some expressivity and
 

2677
01:06:19,940 --> 01:06:25,070
I may fix some expressivity and
performance sort of problems or that

2678
01:06:25,070 --> 01:06:25,080
performance sort of problems or that
 

2679
01:06:25,080 --> 01:06:28,790
performance sort of problems or that
MapReduce has what a lot of what SPARC

2680
01:06:28,790 --> 01:06:28,800
MapReduce has what a lot of what SPARC
 

2681
01:06:28,800 --> 01:06:31,070
MapReduce has what a lot of what SPARC
is doing is making the data flow graph

2682
01:06:31,070 --> 01:06:31,080
is doing is making the data flow graph
 

2683
01:06:31,080 --> 01:06:34,400
is doing is making the data flow graph
explicit sort of he wants you to think

2684
01:06:34,400 --> 01:06:34,410
explicit sort of he wants you to think
 

2685
01:06:34,410 --> 01:06:36,350
explicit sort of he wants you to think
of computations in the style of figure

2686
01:06:36,350 --> 01:06:36,360
of computations in the style of figure
 

2687
01:06:36,360 --> 01:06:38,990
of computations in the style of figure
three of entire lineage graphs stages of

2688
01:06:38,990 --> 01:06:39,000
three of entire lineage graphs stages of
 

2689
01:06:39,000 --> 01:06:41,600
three of entire lineage graphs stages of
computation and the data moving between

2690
01:06:41,600 --> 01:06:41,610
computation and the data moving between
 

2691
01:06:41,610 --> 01:06:44,450
computation and the data moving between
these stages and it does optimizations

2692
01:06:44,450 --> 01:06:44,460
these stages and it does optimizations
 

2693
01:06:44,460 --> 01:06:47,300
these stages and it does optimizations
on this graph and failure recovery is

2694
01:06:47,300 --> 01:06:47,310
on this graph and failure recovery is
 

2695
01:06:47,310 --> 01:06:49,130
on this graph and failure recovery is
very much thinking about the lineage

2696
01:06:49,130 --> 01:06:49,140
very much thinking about the lineage
 

2697
01:06:49,140 --> 01:06:52,430
very much thinking about the lineage
graph as well so it's really part of a

2698
01:06:52,430 --> 01:06:52,440
graph as well so it's really part of a
 

2699
01:06:52,440 --> 01:06:54,110
graph as well so it's really part of a
larger move and big data processing

2700
01:06:54,110 --> 01:06:54,120
larger move and big data processing
 

2701
01:06:54,120 --> 01:06:57,590
larger move and big data processing
towards explicit thinking about the data

2702
01:06:57,590 --> 01:06:57,600
towards explicit thinking about the data
 

2703
01:06:57,600 --> 01:07:00,470
towards explicit thinking about the data
flow graphs as a way to describe

2704
01:07:00,470 --> 01:07:00,480
flow graphs as a way to describe
 

2705
01:07:00,480 --> 01:07:04,490
flow graphs as a way to describe
computations a lot of the specific win

2706
01:07:04,490 --> 01:07:04,500
computations a lot of the specific win
 

2707
01:07:04,500 --> 01:07:06,970
computations a lot of the specific win
and SPARC have to do with performance

2708
01:07:06,970 --> 01:07:06,980
and SPARC have to do with performance
 

2709
01:07:06,980 --> 01:07:09,620
and SPARC have to do with performance
part of the prepend these are

2710
01:07:09,620 --> 01:07:09,630
part of the prepend these are
 

2711
01:07:09,630 --> 01:07:11,030
part of the prepend these are
straightforward but nevertheless

2712
01:07:11,030 --> 01:07:11,040
straightforward but nevertheless
 

2713
01:07:11,040 --> 01:07:13,970
straightforward but nevertheless
important some of the performance comes

2714
01:07:13,970 --> 01:07:13,980
important some of the performance comes
 

2715
01:07:13,980 --> 01:07:15,770
important some of the performance comes
from leaving the data in memory between

2716
01:07:15,770 --> 01:07:15,780
from leaving the data in memory between
 

2717
01:07:15,780 --> 01:07:18,740
from leaving the data in memory between
transformations rather than you know

2718
01:07:18,740 --> 01:07:18,750
transformations rather than you know
 

2719
01:07:18,750 --> 01:07:20,450
transformations rather than you know
writing them to GFS and then reading

2720
01:07:20,450 --> 01:07:20,460
writing them to GFS and then reading
 

2721
01:07:20,460 --> 01:07:21,590
writing them to GFS and then reading
them back at the beginning of the next

2722
01:07:21,590 --> 01:07:21,600
them back at the beginning of the next
 

2723
01:07:21,600 --> 01:07:23,480
them back at the beginning of the next
transformation which you essentially

2724
01:07:23,480 --> 01:07:23,490
transformation which you essentially
 

2725
01:07:23,490 --> 01:07:25,940
transformation which you essentially
have to do with MapReduce and the other

2726
01:07:25,940 --> 01:07:25,950
have to do with MapReduce and the other
 

2727
01:07:25,950 --> 01:07:28,520
have to do with MapReduce and the other
is the ability to define these data sets

2728
01:07:28,520 --> 01:07:28,530
is the ability to define these data sets
 

2729
01:07:28,530 --> 01:07:32,750
is the ability to define these data sets
these are Dedes and tell SPARC to leave

2730
01:07:32,750 --> 01:07:32,760
these are Dedes and tell SPARC to leave
 

2731
01:07:32,760 --> 01:07:34,910
these are Dedes and tell SPARC to leave
this RDD in memory because I'm going to

2732
01:07:34,910 --> 01:07:34,920
this RDD in memory because I'm going to
 

2733
01:07:34,920 --> 01:07:37,910
this RDD in memory because I'm going to
reuse it again and subsequent stages and

2734
01:07:37,910 --> 01:07:37,920
reuse it again and subsequent stages and
 

2735
01:07:37,920 --> 01:07:39,740
reuse it again and subsequent stages and
it's cheaper to reuse it than it is to

2736
01:07:39,740 --> 01:07:39,750
it's cheaper to reuse it than it is to
 

2737
01:07:39,750 --> 01:07:41,750
it's cheaper to reuse it than it is to
recompute it and that sort of a thing

2738
01:07:41,750 --> 01:07:41,760
recompute it and that sort of a thing
 

2739
01:07:41,760 --> 01:07:45,590
recompute it and that sort of a thing
that's easy and SPARC and hard to get at

2740
01:07:45,590 --> 01:07:45,600
that's easy and SPARC and hard to get at
 

2741
01:07:45,600 --> 01:07:48,710
that's easy and SPARC and hard to get at
in MapReduce and the result is a system

2742
01:07:48,710 --> 01:07:48,720
in MapReduce and the result is a system
 

2743
01:07:48,720 --> 01:07:51,110
in MapReduce and the result is a system
that's extremely successful and

2744
01:07:51,110 --> 01:07:51,120
that's extremely successful and
 

2745
01:07:51,120 --> 01:07:55,550
that's extremely successful and
extremely widely used and if you deserve

2746
01:07:55,550 --> 01:07:55,560
extremely widely used and if you deserve
 

2747
01:07:55,560 --> 01:07:59,810
extremely widely used and if you deserve
real success okay that that's all I have

2748
01:07:59,810 --> 01:07:59,820
real success okay that that's all I have
 

2749
01:07:59,820 --> 01:08:01,970
real success okay that that's all I have
to say and I'm happy to take questions

2750
01:08:01,970 --> 01:08:01,980
to say and I'm happy to take questions
 

2751
01:08:01,980 --> 01:08:09,900
to say and I'm happy to take questions
if anyone has them

2752
01:08:09,900 --> 01:08:09,910

 

2753
01:08:09,910 --> 01:08:11,970

you

